{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8c2128-9045-4555-b8ae-58ab3b0bea2d",
   "metadata": {},
   "source": [
    "# Best Practices in Feature Engineering for Tabular Data With GPU Acceleration #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04bb3a-548f-4be6-920f-950c1c591fd3",
   "metadata": {},
   "source": [
    "# Install \n",
    "\n",
    "https://docs.rapids.ai/install/\n",
    "\n",
    "```\n",
    " docker run --gpus all --pull always --rm -it     --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864     -p 8888:8888 -p 8787:8787 -p 8786:8786  --volume /mnt/d/repos/nvidia:/home/rapids/notebooks/nvidia   nvcr.io/nvidia/rapidsai/notebooks:25.08-cuda12.9-py3.13 \n",
    "```\n",
    "\n",
    "## Part 1: Target Encoding ##\n",
    "Most models cannot accept categorical columns as is. A categorical column is typically a column of strings (or non ordered numbers) and we need to convert these into some numeric representation to input it into our model. Common techniques are OHE (one hot encoding) and LE (label encoding). Advanced techniques are TE (Target encoding) and CE (Count encoding). In this notebook, we will discuss TE.\n",
    "\n",
    "[1]: https://rapids.ai/cudf-pandas/\n",
    "[2]: https://docs.rapids.ai/install/\n",
    "\n",
    "In this lab, we will use the speed of GPUs to help us create new columns quickly. Specificially we will use [cuDF-Pandas][1] zero code change GPU acceleration. After adding cell magic `%load_ext cudf.pandas` all of our subsequent Pandas calls will use [RAPIDS cuDF][2] and thus utilize GPU instead of Pandas CPU!\n",
    "\n",
    "**Table of Contents**\n",
    "<br>\n",
    "This notebook shows how to perform target encoding. This notebook covers the below sections: \n",
    "\n",
    "1. [GPU Accelerating Pandas with Zero Code Change](#GPU-Accelerating-Pandas-with-Zero-Code-Change)\n",
    "    * [Load Data](#Load-Data)\n",
    "    * [Target Encoding Technique](#Target-Encoding-Technique)\n",
    "    * [Smoothing](#Smoothing)\n",
    "    * [Compare ACC (Accuracy) Errors](#Compare-ACC-(Accuracy)-Errors)\n",
    "    * [Improve Target Encoding with Nested Folds](#Improve-Target-Encoding-with-Nested-Folds)\n",
    "    * [Target Encoding Summary](#Target-Encoding-Summary)\n",
    "2. [CPU-GPU Comparison](#CPU-GPU-Comparison)\n",
    "    * [Sample Data](#Sample-Data)\n",
    "    * [Enlarge Data](#Enlarge-Data)\n",
    "3. [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9df6e2",
   "metadata": {
    "papermill": {
     "duration": 0.010665,
     "end_time": "2025-01-16T22:15:12.889699",
     "exception": false,
     "start_time": "2025-01-16T22:15:12.879034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## GPU Accelerating Pandas with Zero Code Change\n",
    "After adding cell magic `%load_ext cudf.pandas` all of our subsequent Pandas calls will use [RAPIDS cuDF][1] and thus utilize GPU instead of Pandas CPU! \n",
    "\n",
    "[1]: https://rapids.ai/cudf-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571389f7",
   "metadata": {
    "papermill": {
     "duration": 11.972769,
     "end_time": "2025-01-16T22:15:24.873389",
     "exception": false,
     "start_time": "2025-01-16T22:15:12.900620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext cudf.pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71dcb36e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.010814,
     "end_time": "2025-01-16T22:15:24.895663",
     "exception": false,
     "start_time": "2025-01-16T22:15:24.884849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load Data\n",
    " **Amazon product data dataset** : https://jmcauley.ucsd.edu/data/amazon/\n",
    "\n",
    "\n",
    "**Description**<br>\n",
    "This dataset contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014.\n",
    "\n",
    "This dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).\n",
    "\n",
    "**Citation**<br>\n",
    "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering\n",
    "R. He, J. McAuley\n",
    "WWW, 2016\n",
    "[pdf](http://cseweb.ucsd.edu/~jmcauley/pdfs/www16a.pdf)\n",
    "\n",
    "Image-based recommendations on styles and substitutes\n",
    "J. McAuley, C. Targett, J. Shi, A. van den Hengel\n",
    "SIGIR, 2015\n",
    "[pdf](http://cseweb.ucsd.edu/~jmcauley/pdfs/sigir15.pdf)\n",
    "\n",
    "First we load the data and fill nans in the categorical column `brand` with string `UNKNOWN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed5735",
   "metadata": {
    "papermill": {
     "duration": 1.611163,
     "end_time": "2025-01-16T22:15:26.517756",
     "exception": false,
     "start_time": "2025-01-16T22:15:24.906593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LOAD DATA\n",
    "PATH = \"./data/\"\n",
    "df_train = pd.read_parquet(f'{PATH}train.parquet') \n",
    "df_valid = pd.read_parquet(f'{PATH}valid.parquet')\n",
    "df_test = pd.read_parquet(f'{PATH}test.parquet')\n",
    "\n",
    "# FILL NAN\n",
    "df_train['brand'] = df_train['brand'].fillna('UNKNOWN')\n",
    "df_valid['brand'] = df_valid['brand'].fillna('UNKNOWN')\n",
    "df_test['brand'] = df_test['brand'].fillna('UNKNOWN')\n",
    "\n",
    "print(\"Train data shape:\",df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e3a14",
   "metadata": {
    "papermill": {
     "duration": 0.012119,
     "end_time": "2025-01-16T22:15:26.542449",
     "exception": false,
     "start_time": "2025-01-16T22:15:26.530330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Target Encoding Technique\n",
    "\n",
    "`Target Encoding` is a technique used to create new features, which can be used by the model for training. The advantage of `Target Encoding` is, that it process the categorical features and makes them more accessible to the model during training and validation.\n",
    "\n",
    "Tree-based models require to create a split for each categorical value (depending on the exact model). `Target Encoding` makes it easier for the model to locate important values without creating many splits. In particular, when applying `Target Encoding` to multiple columns, it reduces significantly the number of splits needed. The model can directly operate on the probablities/averages and create splits based on them.\n",
    "\n",
    "Another advantage is, that some boosted-tree libraries, such as XGBoost, only offer experimental categorical feature handling. The library may require a `One Hot Encoding`. Categorical features with large cardinality (e.g. >100) are inefficient to store as `One Hot`.\n",
    "\n",
    "Deep learning models often apply Embedding Layers to categorical features. Embedding layer can overfit quickly and categorical values with low frequencies have ony a few gradient descent updates and models will memorize the training data.\n",
    "\n",
    "#### Encode Single Categorical Column\n",
    "\n",
    "`Target Encoding (TE)` calculates the statistics from a target variable grouped by the unique values of one or more categorical features.\n",
    "\n",
    "For example in a binary classification problem, it calculates the probability that the target is true for each category value - a simple mean. See the example (in first code cell) below where we list all unique values from column `brand` together with their proportion of target equal true. In second code cell below, we merge `TE` onto the original dataframe creating a new `TE` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde4af98",
   "metadata": {
    "papermill": {
     "duration": 0.303397,
     "end_time": "2025-01-16T22:15:26.857246",
     "exception": false,
     "start_time": "2025-01-16T22:15:26.553849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat = 'brand'\n",
    "te = df_train[[cat, 'label']].groupby(cat).mean()\n",
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416000e",
   "metadata": {
    "papermill": {
     "duration": 0.251471,
     "end_time": "2025-01-16T22:15:27.120510",
     "exception": false,
     "start_time": "2025-01-16T22:15:26.869039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "te = te.reset_index()\n",
    "te.columns = [cat, 'TE_' + cat]\n",
    "df_train.merge(te, how='left', on=cat)[['userID', 'productID', cat, 'TE_' + cat]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874c196",
   "metadata": {
    "papermill": {
     "duration": 0.011508,
     "end_time": "2025-01-16T22:15:27.147173",
     "exception": false,
     "start_time": "2025-01-16T22:15:27.135665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Encode Group of Categorical Columns\n",
    "Similarly, we can apply `Target Encoding` to a group of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562c1db",
   "metadata": {
    "papermill": {
     "duration": 1.371336,
     "end_time": "2025-01-16T22:15:28.530026",
     "exception": false,
     "start_time": "2025-01-16T22:15:27.158690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "te = df_train[['brand', 'cat_2', 'label']].groupby(['brand', 'cat_2']).mean()\n",
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193120c",
   "metadata": {
    "papermill": {
     "duration": 0.186962,
     "end_time": "2025-01-16T22:15:28.730660",
     "exception": false,
     "start_time": "2025-01-16T22:15:28.543698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "te = te.reset_index()\n",
    "te.columns = ['brand', 'cat_2', 'TE_brand_2']\n",
    "df_train.merge(te, how='left', on=['brand', 'cat_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97884c6e",
   "metadata": {
    "papermill": {
     "duration": 0.012004,
     "end_time": "2025-01-16T22:15:28.779188",
     "exception": false,
     "start_time": "2025-01-16T22:15:28.767184",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Smoothing\n",
    "The introduced `Target Encoding` is a good first step, but it lacks ability to generalize well and it will tend to overfit too. Let's take a look on `Target Encoding` with the observation count. We notice how some `brands` only have a few rows in train data. See the table and histogram below. If a brand only has a few observations, can we be confident that the observed proportion of true targets will apply to new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634624b",
   "metadata": {
    "papermill": {
     "duration": 0.083943,
     "end_time": "2025-01-16T22:15:28.875672",
     "exception": false,
     "start_time": "2025-01-16T22:15:28.791729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dd = df_train[[cat, 'label']].groupby(cat).agg(['mean', 'count'])\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b04a09",
   "metadata": {
    "papermill": {
     "duration": 0.97659,
     "end_time": "2025-01-16T22:15:29.864582",
     "exception": false,
     "start_time": "2025-01-16T22:15:28.887992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.bar(dd['label']['count'].value_counts().index.to_numpy() , dd['label']['count'].value_counts().to_numpy() )\n",
    "plt.xlim(0,50)\n",
    "plt.title(\"Histogram of Brands and their Observation Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7eba10",
   "metadata": {
    "papermill": {
     "duration": 0.015341,
     "end_time": "2025-01-16T22:15:29.893343",
     "exception": false,
     "start_time": "2025-01-16T22:15:29.878002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can observe, that the observation count for some categories are 1. This means, that we have only one data point to calculate the average and `Target Encoding` overfits to these values. Therefore, we need to adjust the calculation:\n",
    "* if the number of observation is **high**, we want to use the **mean of this category value**\n",
    "* if the number of observation is **low**, we want to use the **global mean**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27082bf0",
   "metadata": {
    "papermill": {
     "duration": 0.012559,
     "end_time": "2025-01-16T22:15:29.944234",
     "exception": false,
     "start_time": "2025-01-16T22:15:29.931675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A simple way is to calculate a weighted average of the `category value mean` and the `global mean`.\n",
    "\n",
    "We add a smoothing weight `w`. A bigger `w` encourages the `Target Encoding` to be closer to the `global mean`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c4811",
   "metadata": {
    "papermill": {
     "duration": 0.012536,
     "end_time": "2025-01-16T22:15:29.969441",
     "exception": false,
     "start_time": "2025-01-16T22:15:29.956905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "* Use a smoothing factor of `w=20`\n",
    "* Target Encode the columns `feat = ['brand', 'cat_2']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8bc842-59d7-473e-993f-1d992ab6cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = ['brand', 'cat_2']\n",
    "w = 20\n",
    "mean_global = df_train.label.mean()\n",
    "te = df_train.groupby(feat)['label'].agg(['mean','count']).reset_index()\n",
    "te['TE_brand_cat_2'] = ((te['mean']*te['count'])+(mean_global*w))/(te['count']+w)\n",
    "\n",
    "df_train = df_train.merge(te, on=feat, how='left')\n",
    "df_valid = df_valid.merge( te, on=feat, how='left' )\n",
    "df_test = df_test.merge( te, on=feat, how='left' )\n",
    "df_valid['TE_brand_cat_2'] = df_valid['TE_brand_cat_2'].fillna(mean_global)\n",
    "df_test['TE_brand_cat_2'] = df_test['TE_brand_cat_2'].fillna(mean_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72de41d",
   "metadata": {
    "papermill": {
     "duration": 0.012646,
     "end_time": "2025-01-16T22:15:30.021297",
     "exception": false,
     "start_time": "2025-01-16T22:15:30.008651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Exploring the Effect of Smoothing\n",
    "\n",
    "A tree-based or deep learning based model cannot easily capture the idea of smoothing. We show the positive effect of smoothing on the target. Therefore, we compare `Target Encoding` with and without smoothing.\n",
    "\n",
    "#### TargetEncoding Without Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1ecb38",
   "metadata": {
    "papermill": {
     "duration": 0.230532,
     "end_time": "2025-01-16T22:15:30.264432",
     "exception": false,
     "start_time": "2025-01-16T22:15:30.033900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat = ['weekday', 'cat_2', 'brand']\n",
    "te = df_train.groupby(cat).label.agg(['mean', 'count']).reset_index()\n",
    "te.columns = cat + ['TE_mean', 'TE_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f75dfb",
   "metadata": {
    "papermill": {
     "duration": 0.101039,
     "end_time": "2025-01-16T22:15:30.379008",
     "exception": false,
     "start_time": "2025-01-16T22:15:30.277969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_valid = df_valid.merge(te, on=cat, how='left')\n",
    "df_valid['error'] = (df_valid['label'] - (df_valid['TE_mean']>=0.5)).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874d943",
   "metadata": {
    "papermill": {
     "duration": 0.154407,
     "end_time": "2025-01-16T22:15:30.546829",
     "exception": false,
     "start_time": "2025-01-16T22:15:30.392422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_global = df_train.label.mean()\n",
    "df_valid['TE_mean'] = df_valid['TE_mean'].fillna(mean_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1386c12",
   "metadata": {
    "papermill": {
     "duration": 0.012446,
     "end_time": "2025-01-16T22:15:30.572268",
     "exception": false,
     "start_time": "2025-01-16T22:15:30.559822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### TargetEncoding With Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d894116e",
   "metadata": {
    "papermill": {
     "duration": 0.072194,
     "end_time": "2025-01-16T22:15:30.657169",
     "exception": false,
     "start_time": "2025-01-16T22:15:30.584975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = 20\n",
    "df_valid['TE_mean_smoothed'] = ((df_valid['TE_mean']*df_valid['TE_count'])+(mean_global*w))/(df_valid['TE_count']+w)\n",
    "df_valid['TE_mean_smoothed'] = df_valid['TE_mean_smoothed'].fillna(mean_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b914dd8",
   "metadata": {
    "papermill": {
     "duration": 0.019896,
     "end_time": "2025-01-16T22:15:30.690085",
     "exception": false,
     "start_time": "2025-01-16T22:15:30.670189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_valid['error_smoothed'] = (df_valid['label'] - (df_valid['TE_mean_smoothed']>=0.5)).abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16542251",
   "metadata": {
    "papermill": {
     "duration": 0.012358,
     "end_time": "2025-01-16T22:15:30.714970",
     "exception": false,
     "start_time": "2025-01-16T22:15:30.702612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Compare ACC (Accuracy) Errors\n",
    "Let's look at the error based on the number of observations. We can see, that the categorical values with low observation count (1, 2, 3) have a lower error rate with smoothing than without smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcece5b",
   "metadata": {
    "papermill": {
     "duration": 0.067077,
     "end_time": "2025-01-16T22:15:30.794576",
     "exception": false,
     "start_time": "2025-01-16T22:15:30.727499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"ACC errors without smoothing:\")\n",
    "df_valid[['TE_count', 'error']].groupby('TE_count').error.mean().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6be34",
   "metadata": {
    "papermill": {
     "duration": 0.06442,
     "end_time": "2025-01-16T22:15:30.871869",
     "exception": false,
     "start_time": "2025-01-16T22:15:30.807449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"ACC errors with smoothing:\")\n",
    "df_valid[['TE_count', 'error_smoothed']].groupby('TE_count').error_smoothed.mean().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc91a6cc",
   "metadata": {
    "papermill": {
     "duration": 0.014008,
     "end_time": "2025-01-16T22:15:30.899324",
     "exception": false,
     "start_time": "2025-01-16T22:15:30.885316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Compare AUC (Area under ROC curve) Errors\n",
    "We can look at the roc_auc values as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc28e6",
   "metadata": {
    "papermill": {
     "duration": 1.568753,
     "end_time": "2025-01-16T22:15:32.481542",
     "exception": false,
     "start_time": "2025-01-16T22:15:30.912789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"AUC without smoothing:\")\n",
    "roc_auc_score(df_valid['label'].astype(int).values, \n",
    "              df_valid['TE_mean'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21c8500",
   "metadata": {
    "papermill": {
     "duration": 0.030743,
     "end_time": "2025-01-16T22:15:32.525801",
     "exception": false,
     "start_time": "2025-01-16T22:15:32.495058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"AUC with smoothing:\")\n",
    "roc_auc_score(df_valid['label'].astype(int).values, \n",
    "              df_valid['TE_mean_smoothed'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1028d06",
   "metadata": {
    "papermill": {
     "duration": 0.013633,
     "end_time": "2025-01-16T22:15:32.553864",
     "exception": false,
     "start_time": "2025-01-16T22:15:32.540231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Improve Target Encoding with Nested Folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ec027",
   "metadata": {
    "papermill": {
     "duration": 0.013185,
     "end_time": "2025-01-16T22:15:32.580149",
     "exception": false,
     "start_time": "2025-01-16T22:15:32.566964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can still improve our `Target Encoding` function. We can even make it more generalizable, if we apply an **out of fold calculation**. \n",
    "\n",
    "In our current definition, we use the full training dataset to `Target Encode` the training dataset and validation/test dataset. Therefore, we will likely overfit slightly on our training dataset, because we use the information from it to encode the categorical values. A better strategy is to use **out of fold**:\n",
    "* use the full training dataset to encode the validation/test dataset\n",
    "* split the training dataset in k-folds and encode the i-th fold by using all folds except of the i-th one\n",
    "\n",
    "The following figure visualize the strategy for k=5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb1416d",
   "metadata": {
    "papermill": {
     "duration": 0.012965,
     "end_time": "2025-01-16T22:15:32.632801",
     "exception": false,
     "start_time": "2025-01-16T22:15:32.619836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The k-fold can be generated by a random split or by a timestamp depending on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52196c6",
   "metadata": {
    "papermill": {
     "duration": 0.013086,
     "end_time": "2025-01-16T22:15:32.658956",
     "exception": false,
     "start_time": "2025-01-16T22:15:32.645870",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Target Encode with Nested Folds and Smoothing\n",
    "We now restart the session, load data, and perform target encoding with nested folds and smoothing using zero code change GPU acceleration with cuDF-Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c72f61",
   "metadata": {
    "papermill": {
     "duration": 0.017944,
     "end_time": "2025-01-16T22:15:32.690048",
     "exception": false,
     "start_time": "2025-01-16T22:15:32.672104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba0f6bb",
   "metadata": {
    "papermill": {
     "duration": 0.013161,
     "end_time": "2025-01-16T22:15:32.716463",
     "exception": false,
     "start_time": "2025-01-16T22:15:32.703302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Zero Code GPU Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa139503",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 0.019604,
     "end_time": "2025-01-16T22:15:32.749255",
     "exception": false,
     "start_time": "2025-01-16T22:15:32.729651",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext cudf.pandas\n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b260f5",
   "metadata": {
    "papermill": {
     "duration": 0.013223,
     "end_time": "2025-01-16T22:15:32.775883",
     "exception": false,
     "start_time": "2025-01-16T22:15:32.762660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e0509f",
   "metadata": {
    "papermill": {
     "duration": 0.424647,
     "end_time": "2025-01-16T22:15:33.213919",
     "exception": false,
     "start_time": "2025-01-16T22:15:32.789272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATH = \"./data/\"\n",
    "df_train = pd.read_parquet(f'{PATH}train.parquet') \n",
    "df_valid = pd.read_parquet(f'{PATH}valid.parquet')\n",
    "\n",
    "df_train['brand'] = df_train['brand'].fillna('UNKNOWN')\n",
    "df_valid['brand'] = df_valid['brand'].fillna('UNKNOWN')\n",
    "\n",
    "print(\"Original train data and valid data shape:\")\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17297de5",
   "metadata": {
    "papermill": {
     "duration": 0.013718,
     "end_time": "2025-01-16T22:15:33.241547",
     "exception": false,
     "start_time": "2025-01-16T22:15:33.227829",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Enlarge Data\n",
    "The training and validation datasets are small for real-world use cases. We artificially increase the dataset size by duplicating the datasets 10 times to make it more similar to a real-world dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08cdae",
   "metadata": {
    "papermill": {
     "duration": 0.135625,
     "end_time": "2025-01-16T22:15:33.390889",
     "exception": false,
     "start_time": "2025-01-16T22:15:33.255264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train]*10).reset_index(drop=True)\n",
    "df_valid = pd.concat([df_valid]*10).reset_index(drop=True)\n",
    "print(\"Enlarged train data and valid data shape:\")\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae8dbe",
   "metadata": {
    "papermill": {
     "duration": 0.024131,
     "end_time": "2025-01-16T22:15:33.428701",
     "exception": false,
     "start_time": "2025-01-16T22:15:33.404570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def target_encode(train, valid, col, target, kfold=5, smooth=20):\n",
    "    \"\"\"\n",
    "        train:  train dataset\n",
    "        valid:  validation dataset\n",
    "        col:   column which will be encoded (in the example RESOURCE)\n",
    "        target: target column which will be used to calculate the statistic\n",
    "    \"\"\"\n",
    "    \n",
    "    # We assume that the train dataset is shuffled\n",
    "    train['kfold'] = ((train.index) % kfold)\n",
    "    # We create the output column, we fill with 0\n",
    "    col_name = '_'.join(col)\n",
    "    train['TE_' + col_name] = 0.\n",
    "    for i in range(kfold):\n",
    "        ###################################\n",
    "        # filter for out of fold\n",
    "        # calculate the mean/counts per group category\n",
    "        # calculate the global mean for the oof\n",
    "        # calculate the smoothed TE\n",
    "        # merge it to the original dataframe\n",
    "        ###################################\n",
    "        \n",
    "        df_tmp = train[train['kfold']!=i]\n",
    "        mn = df_tmp[target].mean()\n",
    "        df_tmp = df_tmp[col + [target]].groupby(col).agg(['mean', 'count']).reset_index()\n",
    "        df_tmp.columns = col + ['mean', 'count']\n",
    "        df_tmp['TE_tmp'] = ((df_tmp['mean']*df_tmp['count'])+(mn*smooth)) / (df_tmp['count']+smooth)\n",
    "        df_tmp_m = train[col + ['kfold', 'TE_' + col_name]].merge(df_tmp, how='left', left_on=col, right_on=col)\n",
    "        df_tmp_m.loc[df_tmp_m['kfold']==i, 'TE_' + col_name] = df_tmp_m.loc[df_tmp_m['kfold']==i, 'TE_tmp']\n",
    "        train['TE_' + col_name] = df_tmp_m['TE_' + col_name].fillna(mn).values\n",
    "\n",
    "    \n",
    "    ###################################\n",
    "    # calculate the mean/counts per group for the full training dataset\n",
    "    # calculate the global mean\n",
    "    # calculate the smoothed TE\n",
    "    # merge it to the original dataframe\n",
    "    # drop all temp columns\n",
    "    ###################################    \n",
    "    \n",
    "    df_tmp = train[col + [target]].groupby(col).agg(['mean', 'count']).reset_index()\n",
    "    mn = train[target].mean()\n",
    "    df_tmp.columns = col + ['mean', 'count']\n",
    "    df_tmp['TE_tmp'] = ((df_tmp['mean']*df_tmp['count'])+(mn*smooth)) / (df_tmp['count']+smooth)\n",
    "    df_tmp_m = valid[col].merge(df_tmp, how='left', left_on=col, right_on=col)\n",
    "    valid['TE_' + col_name] = df_tmp_m['TE_tmp'].fillna(mn).values\n",
    "    \n",
    "    train = train.drop('kfold', axis=1)\n",
    "    return(train, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcd88f",
   "metadata": {
    "papermill": {
     "duration": 4.622947,
     "end_time": "2025-01-16T22:15:38.065303",
     "exception": false,
     "start_time": "2025-01-16T22:15:33.442356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train, df_valid = target_encode(df_train, df_valid, ['weekday', 'cat_2', 'brand'], 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc77dd1",
   "metadata": {
    "papermill": {
     "duration": 0.1092,
     "end_time": "2025-01-16T22:15:38.189003",
     "exception": false,
     "start_time": "2025-01-16T22:15:38.079803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0e1e8",
   "metadata": {
    "papermill": {
     "duration": 0.085637,
     "end_time": "2025-01-16T22:15:38.288751",
     "exception": false,
     "start_time": "2025-01-16T22:15:38.203114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26405649",
   "metadata": {
    "papermill": {
     "duration": 0.013796,
     "end_time": "2025-01-16T22:15:38.316659",
     "exception": false,
     "start_time": "2025-01-16T22:15:38.302863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Target Encoding Summary\n",
    "\n",
    "* `Target Encoding` calculates statistics of a target column given one or more categorical features\n",
    "* `Target Encoding` smooths the statistics as a weighted average of the category value and the global statistic\n",
    "* `Target Encoding` uses a out-of-fold strategy to prevent overfitting to the training dataset.\n",
    "    \n",
    "We can see the advantage of using `Target Encoding` as a feature engineering step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b64da8",
   "metadata": {
    "papermill": {
     "duration": 0.013664,
     "end_time": "2025-01-16T22:15:38.344339",
     "exception": false,
     "start_time": "2025-01-16T22:15:38.330675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CPU-GPU Comparison\n",
    "Let's compare the runtime between `CPU Pandas` and `GPU cuDF-Pandas`. All the code is written in Pandas, so we can execute it on both CPU and GPU by choosing to activate GPU acceleration or not.\n",
    "\n",
    "We restart the session, load data, and perform `target encoding` with nested folds and smoothing. This time we will not use the magic command `%load_ext cudf.pandas` and subsequently our code will run using CPU Pandas instead of GPU `cuDF-Pandas`. When running with GPU above, it took about `3 seconds` to add a new TE column. Let's see how long CPU takes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9710f9e",
   "metadata": {
    "papermill": {
     "duration": 0.018853,
     "end_time": "2025-01-16T22:15:38.377766",
     "exception": false,
     "start_time": "2025-01-16T22:15:38.358913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b3805a",
   "metadata": {
    "papermill": {
     "duration": 0.013646,
     "end_time": "2025-01-16T22:15:38.405249",
     "exception": false,
     "start_time": "2025-01-16T22:15:38.391603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdfdf3a",
   "metadata": {
    "papermill": {
     "duration": 0.250033,
     "end_time": "2025-01-16T22:15:38.669179",
     "exception": false,
     "start_time": "2025-01-16T22:15:38.419146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATH = \"./data/\"\n",
    "df_train = pd.read_parquet(f'{PATH}train.parquet') \n",
    "df_valid = pd.read_parquet(f'{PATH}valid.parquet')\n",
    "\n",
    "df_train['brand'] = df_train['brand'].fillna('UNKNOWN')\n",
    "df_valid['brand'] = df_valid['brand'].fillna('UNKNOWN')\n",
    "\n",
    "print(\"Original train data and valid data shape:\")\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955adfeb",
   "metadata": {
    "papermill": {
     "duration": 0.014816,
     "end_time": "2025-01-16T22:15:38.734556",
     "exception": false,
     "start_time": "2025-01-16T22:15:38.719740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Enlarge Data\n",
    "The training and validation datasets are small for real-world use cases. We artificially increase the dataset size by duplicating the datasets 10 times to make it more similar to a real-world dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcdafe7",
   "metadata": {
    "papermill": {
     "duration": 0.122366,
     "end_time": "2025-01-16T22:15:38.871039",
     "exception": false,
     "start_time": "2025-01-16T22:15:38.748673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train]*10).reset_index(drop=True)\n",
    "df_valid = pd.concat([df_valid]*10).reset_index(drop=True)\n",
    "print(\"Enlarged train data and valid data shape:\")\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e20ff5d",
   "metadata": {
    "papermill": {
     "duration": 0.024283,
     "end_time": "2025-01-16T22:15:38.912176",
     "exception": false,
     "start_time": "2025-01-16T22:15:38.887893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def target_encode(train, valid, col, target, kfold=5, smooth=20):\n",
    "    \"\"\"\n",
    "        train:  train dataset\n",
    "        valid:  validation dataset\n",
    "        col:   column which will be encoded (in the example RESOURCE)\n",
    "        target: target column which will be used to calculate the statistic\n",
    "    \"\"\"\n",
    "    \n",
    "    # We assume that the train dataset is shuffled\n",
    "    train['kfold'] = ((train.index) % kfold)\n",
    "    # We create the output column, we fill with 0\n",
    "    col_name = '_'.join(col)\n",
    "    train['TE_' + col_name] = 0.\n",
    "    for i in range(kfold):\n",
    "        ###################################\n",
    "        # filter for out of fold\n",
    "        # calculate the mean/counts per group category\n",
    "        # calculate the global mean for the oof\n",
    "        # calculate the smoothed TE\n",
    "        # merge it to the original dataframe\n",
    "        ###################################\n",
    "        \n",
    "        df_tmp = train[train['kfold']!=i]\n",
    "        mn = df_tmp[target].mean()\n",
    "        df_tmp = df_tmp[col + [target]].groupby(col).agg(['mean', 'count']).reset_index()\n",
    "        df_tmp.columns = col + ['mean', 'count']\n",
    "        df_tmp['TE_tmp'] = ((df_tmp['mean']*df_tmp['count'])+(mn*smooth)) / (df_tmp['count']+smooth)\n",
    "        df_tmp_m = train[col + ['kfold', 'TE_' + col_name]].merge(df_tmp, how='left', left_on=col, right_on=col)\n",
    "        df_tmp_m.loc[df_tmp_m['kfold']==i, 'TE_' + col_name] = df_tmp_m.loc[df_tmp_m['kfold']==i, 'TE_tmp']\n",
    "        train['TE_' + col_name] = df_tmp_m['TE_' + col_name].fillna(mn).values\n",
    "\n",
    "    \n",
    "    ###################################\n",
    "    # calculate the mean/counts per group for the full training dataset\n",
    "    # calculate the global mean\n",
    "    # calculate the smoothed TE\n",
    "    # merge it to the original dataframe\n",
    "    # drop all temp columns\n",
    "    ###################################    \n",
    "    \n",
    "    df_tmp = train[col + [target]].groupby(col).agg(['mean', 'count']).reset_index()\n",
    "    mn = train[target].mean()\n",
    "    df_tmp.columns = col + ['mean', 'count']\n",
    "    df_tmp['TE_tmp'] = ((df_tmp['mean']*df_tmp['count'])+(mn*smooth)) / (df_tmp['count']+smooth)\n",
    "    df_tmp_m = valid[col].merge(df_tmp, how='left', left_on=col, right_on=col)\n",
    "    valid['TE_' + col_name] = df_tmp_m['TE_tmp'].fillna(mn).values\n",
    "    \n",
    "    train = train.drop('kfold', axis=1)\n",
    "    return(train, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6b7ee",
   "metadata": {
    "papermill": {
     "duration": 4.549389,
     "end_time": "2025-01-16T22:15:43.475783",
     "exception": false,
     "start_time": "2025-01-16T22:15:38.926394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train, df_valid = target_encode(df_train, df_valid, ['weekday', 'cat_2', 'brand'], 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1997319",
   "metadata": {
    "papermill": {
     "duration": 0.015327,
     "end_time": "2025-01-16T22:15:43.506909",
     "exception": false,
     "start_time": "2025-01-16T22:15:43.491582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "In this notebook, the GPU accelerated code computed and added a new Target Encoded column in about 3 seconds and the CPU code took about 60 seconds. We observe a speed up of `20x using GPU versus CPU`, wow!\n",
    "\n",
    "Additionally, our implementation can be still improved. When the dataset gets larger, the speed up will increase more because GPUs like lots of data and doing lots of work at once. Furthermore, we can optimize our solution more based on `dask` and `dask_cudf` to use multiple GPUs. See our Recsys 2020 solution writeup for details!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca9c204-05b7-408e-963f-4360d0020727",
   "metadata": {},
   "source": [
    "Please execute the cell below to shut down the kernel when you are done. Also do not forget to stop the running instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a83d4",
   "metadata": {
    "papermill": {
     "duration": 0.019829,
     "end_time": "2025-01-16T22:15:43.542378",
     "exception": false,
     "start_time": "2025-01-16T22:15:43.522549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a027e-5019-4abe-87cd-1486bf23d585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6332640,
     "sourceId": 10240283,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 37.287817,
   "end_time": "2025-01-16T22:15:46.559417",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-16T22:15:09.271600",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
