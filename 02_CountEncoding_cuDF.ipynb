{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5af987-4934-4989-bf29-33ebac87669e",
   "metadata": {},
   "source": [
    "# Best Practices in Feature Engineering for Tabular Data With GPU Acceleration #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af857d70-4358-4331-825c-2a83f7851920",
   "metadata": {},
   "source": [
    "## Part 2: Count Encoding ##\n",
    "Most models cannot accept categorical columns as is. A categorical column is typically a column of strings (or non ordered numbers) and we need to convert these into some numeric representation to input it into our model. Common techniques are `OHE (one hot encoding)` and `LE (label encoding)`. Advanced techniques are `TE (Target encoding`) and `CE (Count encoding)`. In this notebook, we will discuss CE.\n",
    "\n",
    "[1]: https://rapids.ai/cudf-pandas/\n",
    "[2]: https://docs.rapids.ai/install/\n",
    "\n",
    "In this lab, we will use the speed of GPUs to help us create new columns (features) quickly. Specificially we will use [cuDF-Pandas][1] zero code change GPU acceleration. After adding cell magic `%load_ext cudf.pandas` all of our subsequent Pandas calls will use [RAPIDS cuDF][2] and thus utilize GPU instead of Pandas CPU!\n",
    "\n",
    "**Table of Contents**\n",
    "<br>\n",
    "This notebook shows how to perform count encoding. This notebook covers the below sections: \n",
    "\n",
    "1. [GPU Accelerating Pandas with Zero Code Change](#GPU-Accelerating-Pandas-with-Zero-Code-Change)\n",
    "    * [Load Data](#Load-Data)\n",
    "    * [Count Encoding Technique](#Count-Encoding-Technique)\n",
    "    * [Apply Count Encoding](#Apply-Count-Encoding)\n",
    "2. [CPU-GPU Comparison](#CPU-GPU-Comparison)\n",
    "    * [Sample Data](#Sample-Data)\n",
    "    * [Enlarge Data](#Enlarge-Data)\n",
    "3. [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf07947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T18:36:43.045161Z",
     "iopub.status.busy": "2025-01-10T18:36:43.044831Z",
     "iopub.status.idle": "2025-01-10T18:36:43.050667Z",
     "shell.execute_reply": "2025-01-10T18:36:43.049538Z",
     "shell.execute_reply.started": "2025-01-10T18:36:43.045130Z"
    },
    "papermill": {
     "duration": 0.005614,
     "end_time": "2025-01-16T22:14:46.990895",
     "exception": false,
     "start_time": "2025-01-16T22:14:46.985281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## GPU Accelerating Pandas with Zero Code Change\n",
    "After adding cell magic `%load_ext cudf.pandas` all of our subsequent Pandas calls will use [RAPIDS cuDF][1] and thus utilize GPU instead of Pandas CPU!\n",
    "\n",
    "[1]: https://rapids.ai/cudf-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730700b",
   "metadata": {
    "papermill": {
     "duration": 7.669971,
     "end_time": "2025-01-16T22:14:54.666771",
     "exception": false,
     "start_time": "2025-01-16T22:14:46.996800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext cudf.pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadaa7d9",
   "metadata": {
    "papermill": {
     "duration": 0.005797,
     "end_time": "2025-01-16T22:14:54.678944",
     "exception": false,
     "start_time": "2025-01-16T22:14:54.673147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load Data\n",
    " **Amazon product data dataset** : https://jmcauley.ucsd.edu/data/amazon/\n",
    "\n",
    "**Description**<br>\n",
    "This dataset contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014.\n",
    "\n",
    "This dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).\n",
    "\n",
    "**Citation**<br>\n",
    "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering\n",
    "R. He, J. McAuley\n",
    "WWW, 2016\n",
    "[pdf](http://cseweb.ucsd.edu/~jmcauley/pdfs/www16a.pdf)\n",
    "\n",
    "Image-based recommendations on styles and substitutes\n",
    "J. McAuley, C. Targett, J. Shi, A. van den Hengel\n",
    "SIGIR, 2015\n",
    "[pdf](http://cseweb.ucsd.edu/~jmcauley/pdfs/sigir15.pdf)\n",
    "\n",
    "First we load the data and fill `nans` in the categorical column `brand` with string `UNKNOWN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dac21a",
   "metadata": {
    "papermill": {
     "duration": 1.37602,
     "end_time": "2025-01-16T22:14:56.061137",
     "exception": false,
     "start_time": "2025-01-16T22:14:54.685117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LOAD DATA\n",
    "PATH = \"./data/\"\n",
    "df_train = pd.read_parquet(f'{PATH}train.parquet') \n",
    "df_valid = pd.read_parquet(f'{PATH}valid.parquet')\n",
    "df_test = pd.read_parquet(f'{PATH}test.parquet')\n",
    "\n",
    "# FILL NAN\n",
    "df_train['brand'] = df_train['brand'].fillna('UNKNOWN')\n",
    "df_valid['brand'] = df_valid['brand'].fillna('UNKNOWN')\n",
    "df_test['brand'] = df_test['brand'].fillna('UNKNOWN')\n",
    "\n",
    "print(\"Train data shape:\",df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebdbd48",
   "metadata": {
    "papermill": {
     "duration": 0.006371,
     "end_time": "2025-01-16T22:14:56.074474",
     "exception": false,
     "start_time": "2025-01-16T22:14:56.068103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Count Encoding Technique\n",
    "\n",
    "`Count Encoding` creates a new feature, which can be used by the model for training. It calculates frequency of categories and thus groups categorical values based on their frequency together.\n",
    "\n",
    "For example:\n",
    "* users, which have only 1 interaction in the datasets, are encoded with 1. Instead of having 1 datapoint per user, now, the model can learn a behavior pattern of these users at once.\n",
    "* products, which have many interactions in the datasets, are encoded with a high number. The model can learn to see them as top sellers and treat them, accordingly.\n",
    "\n",
    "The advantage of Count Encoding is that the category values are grouped together based on behavior. Particularly in cases with only a few observation, a decision tree is not able to create a split and neural networks have only a few gradient descent updates for these values.\n",
    "\n",
    "#### Note\n",
    "In competitions, we could count encode the categories for the datasets in different ways:\n",
    "* Count Encode the training dataset and apply it to the validation dataset<br>\n",
    "* Count Encode the training dataset and Count Encode the validataion dataset, separately<br>\n",
    "* Merge the training dataset and validation dataset, Count Encode the concatenated dataset and apply to both datasets.\n",
    "\n",
    "Our focus is on industry applications, therefore only the first process is a valid real-world solution. We maybe can collect statistics as a stream and update the characteristic of our dataset, but it is probably cleaner to increase the training frequency of our recommender models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad6b11-04c4-4997-a105-ccaefeeb7928",
   "metadata": {},
   "source": [
    "#### Encode Single Categorical Column\n",
    "`Count Encoding (CE)` calculates the frequency from one or more categorical features given the training dataset.\n",
    "\n",
    "For example we can consider `Count Encoding` as the popularity of an item or activity of an user. See the example (in first code cell) below where we list all unique values from column `productID` together with their frequency count. In second code cell below, we merge CE onto the original dataframe creating a new CE column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd5e525",
   "metadata": {
    "papermill": {
     "duration": 0.239173,
     "end_time": "2025-01-16T22:14:56.320116",
     "exception": false,
     "start_time": "2025-01-16T22:14:56.080943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat = 'productID'\n",
    "ce = df_train[[cat, 'label']].groupby(cat).count()\n",
    "ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682112e9",
   "metadata": {
    "papermill": {
     "duration": 0.145847,
     "end_time": "2025-01-16T22:14:56.472791",
     "exception": false,
     "start_time": "2025-01-16T22:14:56.326944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ce = ce.reset_index()\n",
    "ce.columns = [cat, 'CE_' + cat]\n",
    "df_train.merge(ce, how='left', on=cat)[['userID', 'productID', 'CE_productID']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eff8b6",
   "metadata": {
    "papermill": {
     "duration": 0.008164,
     "end_time": "2025-01-16T22:14:56.487880",
     "exception": false,
     "start_time": "2025-01-16T22:14:56.479716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Encode Group of Categorical Columns\n",
    "Similarly, we can apply `Count Encoding` to a group of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f20af3",
   "metadata": {
    "papermill": {
     "duration": 1.081516,
     "end_time": "2025-01-16T22:14:57.576169",
     "exception": false,
     "start_time": "2025-01-16T22:14:56.494653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ce = df_train[['cat_2', 'brand', 'label']].groupby(['cat_2', 'brand']).count()\n",
    "ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c423d9",
   "metadata": {
    "papermill": {
     "duration": 0.12292,
     "end_time": "2025-01-16T22:14:57.706223",
     "exception": false,
     "start_time": "2025-01-16T22:14:57.583303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ce = ce.reset_index()\n",
    "ce.columns = ['cat_2', 'brand', 'CE_cat_2_brand']\n",
    "df_train.merge(ce, how='left', on=['cat_2', 'brand'])[['productID', 'userID', 'brand', 'cat_2', 'CE_cat_2_brand']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f5a88",
   "metadata": {
    "papermill": {
     "duration": 0.00691,
     "end_time": "2025-01-16T22:14:57.720514",
     "exception": false,
     "start_time": "2025-01-16T22:14:57.713604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "* Count Encode the column `col = 'userID'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cce628e-c2f7-469a-ae64-c2271e727a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'userID'\n",
    "train_tmp = df_train[col].value_counts().reset_index()\n",
    "#train_tmp = df_train[[col,'label']].groupby(col).count().reset_index()\n",
    "train_tmp.columns = [col, 'CE_' + col]\n",
    "df_train = df_train.merge(train_tmp, how='left', on=col)\n",
    "df_train['CE_' + col] = df_train['CE_' + col].fillna(0).values\n",
    "df_valid = df_valid.merge(train_tmp, how='left', on=col)\n",
    "df_valid['CE_' + col] = df_valid['CE_' + col].fillna(0).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f69225",
   "metadata": {
    "papermill": {
     "duration": 0.006853,
     "end_time": "2025-01-16T22:14:57.748987",
     "exception": false,
     "start_time": "2025-01-16T22:14:57.742134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Apply Count Encoding\n",
    "We now restart the session, load data, and perform `Count Encoding` using zero code change GPU acceleration with cuDF-Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98efaa",
   "metadata": {
    "papermill": {
     "duration": 0.013248,
     "end_time": "2025-01-16T22:14:57.769229",
     "exception": false,
     "start_time": "2025-01-16T22:14:57.755981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd10a50",
   "metadata": {
    "papermill": {
     "duration": 0.006853,
     "end_time": "2025-01-16T22:14:57.783307",
     "exception": false,
     "start_time": "2025-01-16T22:14:57.776454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Zero Code GPU Acceleration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784220f7",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 0.0146,
     "end_time": "2025-01-16T22:14:57.805001",
     "exception": false,
     "start_time": "2025-01-16T22:14:57.790401",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext cudf.pandas\n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82c953",
   "metadata": {
    "papermill": {
     "duration": 0.006792,
     "end_time": "2025-01-16T22:14:57.819250",
     "exception": false,
     "start_time": "2025-01-16T22:14:57.812458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051b138",
   "metadata": {
    "papermill": {
     "duration": 0.421591,
     "end_time": "2025-01-16T22:14:58.247824",
     "exception": false,
     "start_time": "2025-01-16T22:14:57.826233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATH = \"./data/\"\n",
    "df_train = pd.read_parquet(f'{PATH}train.parquet')\n",
    "df_valid = pd.read_parquet(f'{PATH}valid.parquet')\n",
    "\n",
    "df_train['brand'] = df_train['brand'].fillna('UNKNOWN')\n",
    "df_valid['brand'] = df_valid['brand'].fillna('UNKNOWN')\n",
    "\n",
    "print(\"Original train data and valid data shape:\")\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb831402",
   "metadata": {
    "papermill": {
     "duration": 0.00717,
     "end_time": "2025-01-16T22:14:58.262672",
     "exception": false,
     "start_time": "2025-01-16T22:14:58.255502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Enlarge Data\n",
    "The training and validation datasets are small for real-world use cases. We artificially increase the dataset size by duplicating the datasets 10 times to make it more similar to a real-world dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f4f63",
   "metadata": {
    "papermill": {
     "duration": 0.152959,
     "end_time": "2025-01-16T22:14:58.423058",
     "exception": false,
     "start_time": "2025-01-16T22:14:58.270099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train = pd.concat([df_train]*10).reset_index(drop=True)\n",
    "df_valid = pd.concat([df_valid]*10).reset_index(drop=True)\n",
    "print(\"Enlarged train data and valid data shape:\")\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42c9a6",
   "metadata": {
    "papermill": {
     "duration": 0.015294,
     "end_time": "2025-01-16T22:14:58.446035",
     "exception": false,
     "start_time": "2025-01-16T22:14:58.430741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_encode(train, valid, col):\n",
    "    \"\"\"\n",
    "        train:  train dataset\n",
    "        valid:  validation dataset\n",
    "        col:    column which will be count encoded (in the example RESOURCE)\n",
    "    \"\"\"\n",
    "\n",
    "    train_tmp = train[col].value_counts().reset_index()\n",
    "    train_tmp.columns = [col,  'CE_' + col]\n",
    "    df_tmp = train[[col]].merge(train_tmp, how='left', left_on=col, right_on=col)\n",
    "    train['CE_' + col] = df_tmp['CE_' + col].fillna(0).values\n",
    "        \n",
    "    df_tmp = valid[[col]].merge(train_tmp, how='left', left_on=col, right_on=col)\n",
    "    valid['CE_' + col] = df_tmp['CE_' + col].fillna(0).values\n",
    "\n",
    "    return(train, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6026696b",
   "metadata": {
    "papermill": {
     "duration": 0.293669,
     "end_time": "2025-01-16T22:14:58.747491",
     "exception": false,
     "start_time": "2025-01-16T22:14:58.453822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train, df_valid = count_encode(df_train, df_valid, 'userID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827c7690",
   "metadata": {
    "papermill": {
     "duration": 0.122493,
     "end_time": "2025-01-16T22:14:58.878125",
     "exception": false,
     "start_time": "2025-01-16T22:14:58.755632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d751b",
   "metadata": {
    "papermill": {
     "duration": 0.099564,
     "end_time": "2025-01-16T22:14:58.985729",
     "exception": false,
     "start_time": "2025-01-16T22:14:58.886165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c7f4c",
   "metadata": {
    "papermill": {
     "duration": 0.007867,
     "end_time": "2025-01-16T22:14:59.018168",
     "exception": false,
     "start_time": "2025-01-16T22:14:59.010301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CPU-GPU Comparison\n",
    "Let's compare the runtime between `CPU Pandas` and `GPU cuDF-Pandas`. All the code is written in Pandas, so we can execute it on both CPU and GPU by choosing to activate GPU acceleration or not.\n",
    "\n",
    "We restart the session, load data, and perform `count encoding`. This time we will not use the magic command `%load_ext cudf.pandas` and subsequently our code will run using CPU Pandas instead of GPU cuDF-Pandas. When running with GPU above, it took about 0.5 seconds to add a new CE column. Let's see how long CPU takes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f91b69",
   "metadata": {
    "papermill": {
     "duration": 0.014065,
     "end_time": "2025-01-16T22:14:59.040334",
     "exception": false,
     "start_time": "2025-01-16T22:14:59.026269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9182a9",
   "metadata": {
    "papermill": {
     "duration": 0.007916,
     "end_time": "2025-01-16T22:14:59.056443",
     "exception": false,
     "start_time": "2025-01-16T22:14:59.048527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00513b61",
   "metadata": {
    "papermill": {
     "duration": 0.247124,
     "end_time": "2025-01-16T22:14:59.311835",
     "exception": false,
     "start_time": "2025-01-16T22:14:59.064711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATH = \"./data/\"\n",
    "df_train = pd.read_parquet(f'{PATH}train.parquet')\n",
    "df_valid = pd.read_parquet(f'{PATH}valid.parquet')\n",
    "\n",
    "df_train['brand'] = df_train['brand'].fillna('UNKNOWN')\n",
    "df_valid['brand'] = df_valid['brand'].fillna('UNKNOWN')\n",
    "\n",
    "print(\"Original train data and valid data shape:\")\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a267f59",
   "metadata": {
    "papermill": {
     "duration": 0.008158,
     "end_time": "2025-01-16T22:14:59.328760",
     "exception": false,
     "start_time": "2025-01-16T22:14:59.320602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Enlarge Data\n",
    "The training and validation datasets are small for real-world use cases. We artificially increase the dataset size by duplicating the datasets 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f84fa",
   "metadata": {
    "papermill": {
     "duration": 0.144699,
     "end_time": "2025-01-16T22:14:59.481782",
     "exception": false,
     "start_time": "2025-01-16T22:14:59.337083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train = pd.concat([df_train]*10).reset_index(drop=True)\n",
    "df_valid = pd.concat([df_valid]*10).reset_index(drop=True)\n",
    "print(\"Enlarged train data and valid data shape:\")\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d9a8c2",
   "metadata": {
    "papermill": {
     "duration": 0.018003,
     "end_time": "2025-01-16T22:14:59.508322",
     "exception": false,
     "start_time": "2025-01-16T22:14:59.490319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_encode(train, valid, col):\n",
    "    \"\"\"\n",
    "        train:  train dataset\n",
    "        valid:  validation dataset\n",
    "        col:    column which will be count encoded (in the example RESOURCE)\n",
    "    \"\"\"\n",
    "\n",
    "    train_tmp = train[col].value_counts().reset_index()\n",
    "    train_tmp.columns = [col,  'CE_' + col]\n",
    "    df_tmp = train[[col]].merge(train_tmp, how='left', left_on=col, right_on=col)\n",
    "    train['CE_' + col] = df_tmp['CE_' + col].fillna(0).values\n",
    "        \n",
    "    df_tmp = valid[[col]].merge(train_tmp, how='left', left_on=col, right_on=col)\n",
    "    valid['CE_' + col] = df_tmp['CE_' + col].fillna(0).values\n",
    "\n",
    "    return(train, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33289645",
   "metadata": {
    "papermill": {
     "duration": 0.284339,
     "end_time": "2025-01-16T22:14:59.801671",
     "exception": false,
     "start_time": "2025-01-16T22:14:59.517332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train, df_valid = count_encode(df_train, df_valid, 'userID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92024758",
   "metadata": {
    "papermill": {
     "duration": 0.009683,
     "end_time": "2025-01-16T22:14:59.820412",
     "exception": false,
     "start_time": "2025-01-16T22:14:59.810729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "In this notebook, the GPU accelerated code computed and added a new Count Encoded column in about `0.5 seconds` and the CPU code took about `7.5 seconds`. We observe a speed up of `15x using GPU versus CPU`, wow!\n",
    "\n",
    "Additionally, our implementation can be still improved. When the dataset gets larger, the speed up will increase more because GPUs like lots of data and doing lots of work at once. Furthermore, we can optimize our solution more based on `dask` and `dask_cudf` to use multiple GPUs. See our Recsys 2020 solution writeup for details!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfae277-bc07-4eea-ba5d-a8737f1ced45",
   "metadata": {},
   "source": [
    "Please execute the cell below to shut down the kernel when you are done. Also do not forget to stop the running instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f9fd29",
   "metadata": {
    "papermill": {
     "duration": 0.015245,
     "end_time": "2025-01-16T22:14:59.844077",
     "exception": false,
     "start_time": "2025-01-16T22:14:59.828832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6332640,
     "sourceId": 10240283,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.621442,
   "end_time": "2025-01-16T22:15:01.575788",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-16T22:14:43.954346",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
