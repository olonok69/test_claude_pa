# Azure ML Pipeline Configuration
# For Personal Agendas Data Processing Pipeline

pipeline:
  name: "personal_agendas_pipeline"
  description: "Multi-step pipeline for Personal Agendas data processing"
  version: "1.0.0"
  
# Azure ML Workspace Configuration
azure_ml:
  subscription_id: "${SUBSCRIPTION_ID}"
  resource_group: "${RESOURCE_GROUP}"
  workspace_name: "${AZUREML_WORKSPACE_NAME}"
  compute:
    cluster_name: "cpu-cluster"
    vm_size: "STANDARD_DS3_V2"
    min_nodes: 0
    max_nodes: 4

# Pipeline Steps Configuration
steps:
  # Step 1: Data Preparation (Registration, Scan, Session)
  data_preparation:
    enabled: true
    script: "azureml_step1_data_prep.py"
    compute: "cpu-cluster"
    environment: "personal_agendas_env"
    inputs:
      input_data:
        type: "uri_folder"
        path: "azureml://datastores/landing/paths/weekly_refresh_data"
      config_file:
        type: "uri_file"
        path: "./config/config_${EVENT_TYPE}.yaml"
    outputs:
      registration_output:
        type: "uri_folder"
        mode: "rw_mount"
      scan_output:
        type: "uri_folder"
        mode: "rw_mount"
      session_output:
        type: "uri_folder"
        mode: "rw_mount"
      metadata_output:
        type: "uri_folder"
        mode: "rw_mount"
    parameters:
      incremental: false
      event_type: "${EVENT_TYPE}"  # vet or ecomm

  # Step 2: Neo4j Preparation
  neo4j_preparation:
    enabled: true
    script: "azureml_step2_neo4j_prep.py"
    compute: "cpu-cluster"
    environment: "personal_agendas_env"
    inputs:
      registration_data:
        type: "uri_folder"
        source: "data_preparation.outputs.registration_output"
      scan_data:
        type: "uri_folder"
        source: "data_preparation.outputs.scan_output"
      session_data:
        type: "uri_folder"
        source: "data_preparation.outputs.session_output"
      config_file:
        type: "uri_file"
        path: "./config/config_${EVENT_TYPE}.yaml"
    outputs:
      neo4j_data:
        type: "uri_folder"
        mode: "rw_mount"
      relationships:
        type: "uri_folder"
        mode: "rw_mount"
    parameters:
      create_only_new: true

  # Step 3: Session Embeddings
  session_embeddings:
    enabled: true
    script: "azureml_step3_embeddings.py"
    compute: "gpu-cluster"  # Use GPU for embeddings
    environment: "personal_agendas_gpu_env"
    inputs:
      neo4j_data:
        type: "uri_folder"
        source: "neo4j_preparation.outputs.neo4j_data"
      session_data:
        type: "uri_folder"
        source: "data_preparation.outputs.session_output"
      config_file:
        type: "uri_file"
        path: "./config/config_${EVENT_TYPE}.yaml"
    outputs:
      embeddings:
        type: "uri_folder"
        mode: "rw_mount"
    parameters:
      model_type: "sentence-transformers"
      batch_size: 32

  # Step 4: Recommendations
  recommendations:
    enabled: true
    script: "azureml_step4_recommendations.py"
    compute: "cpu-cluster"
    environment: "personal_agendas_env"
    inputs:
      neo4j_data:
        type: "uri_folder"
        source: "neo4j_preparation.outputs.neo4j_data"
      embeddings:
        type: "uri_folder"
        source: "session_embeddings.outputs.embeddings"
      config_file:
        type: "uri_file"
        path: "./config/config_${EVENT_TYPE}.yaml"
    outputs:
      recommendations:
        type: "uri_folder"
        mode: "rw_mount"
      metrics:
        type: "uri_folder"
        mode: "rw_mount"
    parameters:
      top_k: 10
      similarity_threshold: 0.7

# Data Configuration
data:
  input_datastore: "landing"
  output_datastore: "processed"
  intermediate_datastore: "intermediate"
  
# Monitoring and Logging
monitoring:
  enable_mlflow: true
  log_level: "INFO"
  retention_days: 30

# Error Handling
error_handling:
  retry_count: 2
  retry_delay_seconds: 60
  continue_on_step_failure: false

# Scheduling (optional)
schedule:
  enabled: false
  cron_expression: "0 2 * * MON"  # Every Monday at 2 AM
  time_zone: "UTC"

# Tags for tracking
tags:
  project: "personal_agendas"
  team: "data_science"
  environment: "${ENVIRONMENT}"  # dev, staging, prod