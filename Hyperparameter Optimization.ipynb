{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56357206-721e-4198-92c4-b956a45ba70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083cfcc8-2c89-4487-bc6b-6a1ab4d75be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(4120)\n",
    "\n",
    "# Define date range\n",
    "start_date = datetime(2021, 1, 1)\n",
    "end_date = datetime(2025, 12, 31)\n",
    "\n",
    "# Generate date range with daily frequency\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Generate random sales data\n",
    "# Using a combination of base sales + trend + seasonality + random noise\n",
    "n_days = len(date_range)\n",
    "\n",
    "# Base sales around 1000 units\n",
    "base_sales = 1000\n",
    "\n",
    "# Add trend component (slight upward trend over time)\n",
    "trend = np.linspace(0, 200, n_days)\n",
    "\n",
    "# Add seasonal component (yearly cycle)\n",
    "seasonal = 150 * np.sin(2 * np.pi * np.arange(n_days) / 365.25)\n",
    "\n",
    "# Add weekly pattern (higher sales on weekends)\n",
    "day_of_week = pd.Series(date_range).dt.dayofweek\n",
    "weekly_pattern = np.where(day_of_week.isin([5, 6]), 100, 0)  # Weekend boost\n",
    "\n",
    "# Add random noise\n",
    "noise = np.random.normal(0, 50, n_days)\n",
    "\n",
    "# Combine all components\n",
    "sales = base_sales + trend + seasonal + weekly_pattern + noise\n",
    "\n",
    "# Ensure sales are positive\n",
    "sales = np.maximum(sales, 0)\n",
    "\n",
    "# Round to nearest integer\n",
    "sales = np.round(sales).astype(int)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date': date_range,\n",
    "    'sales': sales\n",
    "})\n",
    "\n",
    "# Display first and last few rows\n",
    "print(\"Dataset created successfully!\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nLast 10 rows:\")\n",
    "print(df.tail(10))\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "print(df['sales'].describe())\n",
    "\n",
    "# Save to CSV file\n",
    "csv_filename = 'sales_data_2023_2025_v3.csv'\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nDataset saved to '{csv_filename}'\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad83c2-0c05-40f8-b782-b4d01996224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Plot 1: Full time series\n",
    "    axes[0].plot(df['date'], df['sales'], linewidth=0.5, alpha=0.7)\n",
    "    axes[0].set_title('Daily Sales Volume (2021-2025)')\n",
    "    axes[0].set_xlabel('Date')\n",
    "    axes[0].set_ylabel('Sales Volume')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Monthly aggregated sales\n",
    "    df_monthly = df.set_index('date').resample('ME')['sales'].sum().reset_index()\n",
    "    axes[1].bar(df_monthly['date'], df_monthly['sales'], width=20, alpha=0.7)\n",
    "    axes[1].set_title('Monthly Sales Volume')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel('Total Monthly Sales')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sales_visualization_v3.png', dpi=100)\n",
    "    plt.show()\n",
    "    print(\"\\nVisualization saved to 'sales_visualization_v3.png'\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\nNote: Install matplotlib for visualization (pip install matplotlib)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd62dc-fb52-44de-b8cd-6cbd48dc277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.prophet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "experiment_name = \"/Users/j.huertas@closerstillmedia.com/prophet\"\n",
    "try:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "except:\n",
    "    print(\"experiment exists\")\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f32bc-9ddf-4d54-b146-4e60dc2c21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prophet_data(data, date_col, value_col, freq=\"D\"):\n",
    "    \"\"\"\n",
    "    Prepare data for Prophet training.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame with time series data\n",
    "        date_col: Name of date column\n",
    "        value_col: Name of value column\n",
    "        freq: Frequency of the time series\n",
    "    \"\"\"\n",
    "\n",
    "    # Prophet requires columns named 'ds' (datestamp) and 'y' (value)\n",
    "    prophet_df = data[[date_col, value_col]].copy()\n",
    "    prophet_df.columns = [\"ds\", \"y\"]\n",
    "\n",
    "    # Ensure ds is datetime\n",
    "    prophet_df[\"ds\"] = pd.to_datetime(prophet_df[\"ds\"])\n",
    "\n",
    "    # Sort by date\n",
    "    prophet_df = prophet_df.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "    # Handle missing dates if needed\n",
    "    if freq:\n",
    "        full_date_range = pd.date_range(\n",
    "            start=prophet_df[\"ds\"].min(), end=prophet_df[\"ds\"].max(), freq=freq\n",
    "        )\n",
    "\n",
    "        # Reindex to fill missing dates\n",
    "        prophet_df = prophet_df.set_index(\"ds\").reindex(full_date_range).reset_index()\n",
    "        prophet_df.columns = [\"ds\", \"y\"]\n",
    "\n",
    "        # Log data quality metrics\n",
    "        missing_dates = prophet_df[\"y\"].isna().sum()\n",
    "        print(f\"Missing dates filled: {missing_dates}\")\n",
    "\n",
    "    return prophet_df\n",
    "\n",
    "    \n",
    "df_prepared = prepare_prophet_data(df, 'date', 'sales', freq='D')\n",
    "df_prepared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f0af2-7c81-421a-a1a6-d00e9d0a944b",
   "metadata": {},
   "source": [
    "# Systematic Parameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9da6b-38c7-42f4-a562-4eeee2939f66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "def optimize_prophet_hyperparameters(df, param_grid=None):\n",
    "    \"\"\"Systematic hyperparameter optimization for Prophet.\"\"\"\n",
    "\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            \"changepoint_prior_scale\": [0.001, 0.01, 0.1, 0.5],\n",
    "            \"seasonality_prior_scale\": [0.01, 0.1, 1.0, 10.0],\n",
    "            \"holidays_prior_scale\": [0.01, 0.1, 1.0, 10.0],\n",
    "            \"seasonality_mode\": [\"additive\", \"multiplicative\"],\n",
    "        }\n",
    "\n",
    "    # Generate all parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    param_combinations = list(itertools.product(*param_values))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Prophet Hyperparameter Optimization\"):\n",
    "        mlflow.log_param(\"total_combinations\", len(param_combinations))\n",
    "\n",
    "        for i, param_combo in enumerate(param_combinations):\n",
    "            param_dict = dict(zip(param_names, param_combo))\n",
    "\n",
    "            with mlflow.start_run(run_name=f\"Config_{i+1}\", nested=True):\n",
    "                try:\n",
    "                    # Create model with current parameters\n",
    "                    model = Prophet(**param_dict)\n",
    "\n",
    "                    # Time series split for validation\n",
    "                    train_size = int(len(df) * 0.8)\n",
    "                    train_df = df.iloc[:train_size]\n",
    "                    test_df = df.iloc[train_size:]\n",
    "\n",
    "                    # Fit model\n",
    "                    model.fit(train_df)\n",
    "\n",
    "                    # Predict on test set\n",
    "                    future = model.make_future_dataframe(periods=len(test_df))\n",
    "                    if model.growth == \"logistic\":\n",
    "                        future[\"cap\"] = df[\"cap\"].iloc[-1]\n",
    "\n",
    "                    forecast = model.predict(future)\n",
    "                    test_forecast = forecast.iloc[-len(test_df) :]\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    mape = mean_absolute_percentage_error(\n",
    "                        test_df[\"y\"], test_forecast[\"yhat\"]\n",
    "                    )\n",
    "                    mae = np.mean(np.abs(test_df[\"y\"] - test_forecast[\"yhat\"]))\n",
    "                    rmse = np.sqrt(np.mean((test_df[\"y\"] - test_forecast[\"yhat\"]) ** 2))\n",
    "\n",
    "                    # Log parameters and metrics\n",
    "                    mlflow.log_params(param_dict)\n",
    "                    mlflow.log_metrics(\n",
    "                        {\"test_mape\": mape, \"test_mae\": mae, \"test_rmse\": rmse}\n",
    "                    )\n",
    "\n",
    "                    # Store results\n",
    "                    result = {**param_dict, \"mape\": mape, \"mae\": mae, \"rmse\": rmse}\n",
    "                    results.append(result)\n",
    "\n",
    "                    print(f\"Config {i+1}/{len(param_combinations)}: MAPE = {mape:.4f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in configuration {i+1}: {e}\")\n",
    "                    mlflow.log_param(\"error\", str(e))\n",
    "\n",
    "        # Find best configuration\n",
    "        best_result = min(results, key=lambda x: x[\"mape\"])\n",
    "\n",
    "        # Log best configuration\n",
    "        mlflow.log_params({f\"best_{k}\": v for k, v in best_result.items()})\n",
    "\n",
    "        # Train final model with best parameters\n",
    "        best_params = {\n",
    "            k: v for k, v in best_result.items() if k not in [\"mape\", \"mae\", \"rmse\"]\n",
    "        }\n",
    "        final_model = Prophet(**best_params)\n",
    "        final_model.fit(df)\n",
    "\n",
    "        # Log final model\n",
    "        mlflow.prophet.log_model(\n",
    "            pr_model=final_model,\n",
    "            name=\"best_model\",\n",
    "            input_example=df[[\"ds\"]].head(),\n",
    "        )\n",
    "\n",
    "        return final_model, best_result, results\n",
    "\n",
    "\n",
    "# Usage\n",
    "best_model, best_config, all_results = optimize_prophet_hyperparameters(df_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4d059-791f-4529-b28e-e2755ec33329",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e8c62-c59b-412e-beab-13630035ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb3ec8-0a71-42c1-9dc2-835fe0a4bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Log Best Model\"):\n",
    "    mlflow.prophet.log_model(\n",
    "            pr_model=best_model,\n",
    "            name=\"best_model\",\n",
    "            input_example=df_prepared[[\"ds\"]].head(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3e7b5-1158-43f4-b4b1-4b593e1e525f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6489bd-166d-47d8-9ebf-3ad958424f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9eaee-b68e-4c51-a19e-3bff4050bc45",
   "metadata": {},
   "source": [
    "# Cross-Validation Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519cea5-7f16-4fe5-aa58-df002bdac61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_model_validation(model, df):\n",
    "    \"\"\"Perform comprehensive Prophet model validation.\"\"\"\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Comprehensive Model Validation\"):\n",
    "        # Multiple cross-validation configurations\n",
    "        cv_configs = [\n",
    "            {\n",
    "                \"name\": \"short_horizon\",\n",
    "                \"initial\": \"365 days\",\n",
    "                \"period\": \"90 days\",\n",
    "                \"horizon\": \"90 days\",\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"medium_horizon\",\n",
    "                \"initial\": \"730 days\",\n",
    "                \"period\": \"180 days\",\n",
    "                \"horizon\": \"180 days\",\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"long_horizon\",\n",
    "                \"initial\": \"1095 days\",\n",
    "                \"period\": \"180 days\",\n",
    "                \"horizon\": \"365 days\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        all_metrics = {}\n",
    "\n",
    "        for config in cv_configs:\n",
    "            try:\n",
    "                # Perform cross-validation\n",
    "                cv_results = cross_validation(\n",
    "                    model,\n",
    "                    initial=config[\"initial\"],\n",
    "                    period=config[\"period\"],\n",
    "                    horizon=config[\"horizon\"],\n",
    "                    parallel=\"threads\",\n",
    "                )\n",
    "\n",
    "                # Calculate metrics\n",
    "                metrics = performance_metrics(cv_results)\n",
    "                avg_metrics = metrics[[\"mse\", \"rmse\", \"mae\", \"mape\", \"coverage\"]].mean()\n",
    "\n",
    "                # Store metrics with configuration prefix\n",
    "                for metric, value in avg_metrics.items():\n",
    "                    metric_name = f\"{config['name']}_{metric}\"\n",
    "                    all_metrics[metric_name] = value\n",
    "                    mlflow.log_metric(metric_name, value)\n",
    "\n",
    "                # Log additional statistics\n",
    "                mlflow.log_metrics(\n",
    "                    {\n",
    "                        f\"{config['name']}_cv_folds\": len(cv_results),\n",
    "                        f\"{config['name']}_mape_std\": metrics[\"mape\"].std(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Cross-validation failed for {config['name']}: {e}\")\n",
    "                mlflow.log_param(f\"{config['name']}_error\", str(e))\n",
    "\n",
    "        return all_metrics\n",
    "\n",
    "\n",
    "# Usage\n",
    "validation_metrics = comprehensive_model_validation(advanced_model, df_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e16f98-97af-476a-bfad-bdd376e2c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a0fce-b41c-47b6-9a39-eadc77fb76f2",
   "metadata": {},
   "source": [
    "# Forecast Quality Assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857695df-2123-4ab9-8e2b-5f51fc2408f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! uv pip install seaborn statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6250dfe-8135-458e-8400-1e831ef71f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def analyze_forecast_quality(model, df):\n",
    "    \"\"\"Analyze forecast quality with visualizations.\"\"\"\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Forecast Quality Analysis\"):\n",
    "        # Generate forecast\n",
    "        future = model.make_future_dataframe(periods=365)\n",
    "        if model.growth == \"logistic\":\n",
    "            future[\"cap\"] = df[\"cap\"].iloc[-1]  # Use last known capacity\n",
    "            future[\"floor\"] = df[\"floor\"].iloc[-1] if \"floor\" in df.columns else 0\n",
    "\n",
    "        forecast = model.predict(future)\n",
    "\n",
    "        # Component analysis\n",
    "        fig = model.plot_components(forecast, figsize=(12, 8))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"forecast_components.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        mlflow.log_artifact(\"forecast_components.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Forecast plot\n",
    "        fig = model.plot(forecast, figsize=(12, 6))\n",
    "        plt.title(\"Prophet Forecast\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"forecast_plot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        mlflow.log_artifact(\"forecast_plot.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Residual analysis\n",
    "        # Get historical predictions\n",
    "        historical_forecast = forecast[forecast[\"ds\"] <= df[\"ds\"].max()]\n",
    "        residuals = (\n",
    "            df.set_index(\"ds\")[\"y\"] - historical_forecast.set_index(\"ds\")[\"yhat\"]\n",
    "        )\n",
    "\n",
    "        # Plot residuals\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Residuals over time\n",
    "        axes[0, 0].plot(residuals.index, residuals.values)\n",
    "        axes[0, 0].set_title(\"Residuals Over Time\")\n",
    "        axes[0, 0].set_xlabel(\"Date\")\n",
    "        axes[0, 0].set_ylabel(\"Residual\")\n",
    "\n",
    "        # Residual distribution\n",
    "        axes[0, 1].hist(residuals.values, bins=30, alpha=0.7)\n",
    "        axes[0, 1].set_title(\"Residual Distribution\")\n",
    "        axes[0, 1].set_xlabel(\"Residual\")\n",
    "        axes[0, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "        # Q-Q plot\n",
    "        from scipy import stats\n",
    "        from statsmodels.stats import diagnostic as diag\n",
    "\n",
    "        stats.probplot(residuals.values, dist=\"norm\", plot=axes[1, 0])\n",
    "        axes[1, 0].set_title(\"Q-Q Plot\")\n",
    "\n",
    "        # Residuals vs fitted\n",
    "        axes[1, 1].scatter(historical_forecast[\"yhat\"], residuals.values, alpha=0.6)\n",
    "        axes[1, 1].set_title(\"Residuals vs Fitted\")\n",
    "        axes[1, 1].set_xlabel(\"Fitted Values\")\n",
    "        axes[1, 1].set_ylabel(\"Residuals\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"residual_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        mlflow.log_artifact(\"residual_analysis.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Calculate residual statistics\n",
    "        residual_stats = {\n",
    "            \"residual_mean\": residuals.mean(),\n",
    "            \"residual_std\": residuals.std(),\n",
    "            \"residual_skewness\": stats.skew(residuals.values),\n",
    "            \"residual_kurtosis\": stats.kurtosis(residuals.values),\n",
    "            \"ljung_box_pvalue\": diag.acorr_ljungbox(\n",
    "                residuals.values, lags=10, return_df=True\n",
    "            )[\"lb_pvalue\"].iloc[-1],\n",
    "        }\n",
    "\n",
    "        mlflow.log_metrics(residual_stats)\n",
    "\n",
    "        return forecast, residual_stats\n",
    "\n",
    "\n",
    "# Usage\n",
    "forecast_analysis, residual_stats = analyze_forecast_quality(advanced_model, df_prepared)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
