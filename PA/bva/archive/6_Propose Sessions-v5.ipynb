{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3293c1f-fe75-481a-b228-1203c77a2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "uri = \"bolt://127.0.0.1:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"\"  # Replace with your password.\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "# driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "# Load a good text embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# model = SentenceTransformer(\n",
    "#     \"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True,model_kwargs = {\"weights_only\":True}\n",
    "# )  # , device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d416b1-fe40-4367-b3d3-0383c511fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Urology in veterinary medicine focuses on the diagnosis, treatment, and management of diseases and conditions affecting the urinary tract and kidneys in animals. \n",
    "This category encompasses a wide range of topics including proteinuria interpretation, urinary tract infections, obstructive conditions like blocked bladder, and surgical\n",
    "interventions for urolithiasis. It also highlights the importance of diagnostic imaging techniques and the critical role of veterinary nurses in managing chronic kidney disease. \n",
    "Overall, urology integrates medical, surgical, and supportive care approaches to maintain and restore urinary health in various animal species.\"\"\"\n",
    "len(model.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ad952-1028-4c7b-a844-63e2525e2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values for properties\n",
    "default_properties = {\n",
    "    \"Days_since_registration\": \"119\",\n",
    "    \"Country\": \"UK\",\n",
    "    \"Source\": \"BVA Key Stakeholders\",\n",
    "    \"Email_domain\": \"effem.com\",\n",
    "    \"assist_year_before\": \"1\",\n",
    "    \"job_role\": \"NA\",\n",
    "    \"what_type_does_your_practice_specialise_in\": \"NA\",\n",
    "    \"organisation_type\": \"NA\",\n",
    "    \"JobTitle\": \"NA\",\n",
    "}\n",
    "\n",
    "\n",
    "def set_default_properties(tx, properties):\n",
    "    query = \"\"\"\n",
    "    MATCH (n:Visitor_this_year)\n",
    "    SET\n",
    "    \"\"\" + \",\\n    \".join(\n",
    "        [f\"n.{key} = COALESCE(n.{key}, $props.{key})\" for key in properties.keys()]\n",
    "    )\n",
    "\n",
    "    tx.run(query, props=properties)\n",
    "\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    with driver.session() as session:\n",
    "        session.execute_write(set_default_properties, default_properties)\n",
    "    driver.close()\n",
    "    print(\"Missing properties set to default values for all Visitor_this_year nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fddd99-3df7-4742-a4cf-129ad343d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42142ae-ee05-43ce-b21e-0755f59d3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import time\n",
    "\n",
    "\n",
    "def get_db_session():\n",
    "    \"\"\"Get a Neo4j session from the global driver.\"\"\"\n",
    "    global neo4j_driver\n",
    "    try:\n",
    "        # Verify connection is still valid\n",
    "        if neo4j_driver.verify_connectivity():\n",
    "            return neo4j_driver.session()\n",
    "        else:\n",
    "            # Reinitialize if connection is invalid\n",
    "            neo4j_driver = GraphDatabase.driver(\n",
    "                \"neo4j://localhost:7687\", auth=(username, password)\n",
    "            )\n",
    "            return neo4j_driver.session()\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting database session: {e}\")\n",
    "        # Reinitialize driver\n",
    "        neo4j_driver = GraphDatabase.driver(\n",
    "            \"neo4j://localhost:7687\", auth=(username, password)\n",
    "        )\n",
    "        return neo4j_driver.session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc493c1d-6786-42f6-b079-c4f924d60147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from neo4j import GraphDatabase\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display settings for better visualization\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "# 1. Connect to Neo4j database\n",
    "# ------------------------------\n",
    "# Replace with your actual connection details\n",
    "uri = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"\"\n",
    "\n",
    "# Create the driver\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "\n",
    "# Test connection\n",
    "def test_connection():\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"MATCH (n) RETURN count(n) as count\").single()\n",
    "        if result:\n",
    "            print(f\"✅ Successfully connected to Neo4j - Found {result['count']} nodes\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Failed to connect to Neo4j\")\n",
    "            return False\n",
    "\n",
    "\n",
    "test_connection()\n",
    "\n",
    "# 2. Import the optimized code\n",
    "# ----------------------------\n",
    "# Import the code from the Python file\n",
    "# You should first save the code from the previous artifact as a Python file\n",
    "# Note: For this notebook's sake, I'm pasting key functions directly in the next cell\n",
    "\n",
    "# Define job role categories\n",
    "VET_ROLES = [\n",
    "    \"Vet/Vet Surgeon\",\n",
    "    \"Assistant Vet\",\n",
    "    \"Vet/Owner\",\n",
    "    \"Clinical or other Director\",\n",
    "    \"Locum Vet\",\n",
    "    \"Academic\",\n",
    "]\n",
    "\n",
    "NURSE_ROLES = [\"Head Nurse/Senior Nurse\", \"Vet Nurse\", \"Locum RVN\"]\n",
    "\n",
    "BUSINESS = [\"Practice Manager\", \"Practice Partner/Owner\"]\n",
    "# Other roles can attend any session\n",
    "OTHER_ROLES = [\"Student\", \"Receptionist\", \"Other (please specify)\"]\n",
    "\n",
    "\n",
    "# Global cache for reuse of data\n",
    "class RecommendationCache:\n",
    "    def __init__(self):\n",
    "        self.session_embeddings = None\n",
    "        self.all_sessions_data = None\n",
    "        self.stream_descriptions = None\n",
    "        self.visitor_info_cache = {}\n",
    "        self.past_sessions_cache = {}\n",
    "        self.similar_visitors_cache = {}\n",
    "        self.filtered_sessions_cache = {}\n",
    "\n",
    "    def is_initialized(self):\n",
    "        return self.session_embeddings is not None\n",
    "\n",
    "\n",
    "cache = RecommendationCache()\n",
    "\n",
    "# 3. Load and initialize the sentence transformer model\n",
    "# -----------------------------------------------------\n",
    "print(\"Loading sentence transformer model...\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Using a smaller, faster model\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# 4. Load the visitor data\n",
    "# -----------------------\n",
    "# Replace with your actual file path\n",
    "csv_file_path = \"visitor_data.csv\"\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    print(f\"✅ Successfully loaded CSV with {len(data)} rows\")\n",
    "    # Display sample of data\n",
    "    display(data.head())\n",
    "    # Get unique badge IDs\n",
    "    list_badgeId_this = list(data[\"BadgeId\"].unique())\n",
    "    print(f\"Found {len(list_badgeId_this)} unique badge IDs\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ File not found: {csv_file_path}\")\n",
    "    # Create some dummy data for demonstration\n",
    "    print(\"Creating dummy data for demonstration\")\n",
    "    list_badgeId_this = [f\"BADGE{i}\" for i in range(1, 101)]\n",
    "    print(f\"Created {len(list_badgeId_this)} dummy badge IDs\")\n",
    "\n",
    "\n",
    "# 5. Precompute all session embeddings\n",
    "# ------------------------------------\n",
    "def precompute_all_data(tx):\n",
    "    \"\"\"Precompute all data needed for recommendations in a single database transaction.\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"Precomputing all session data...\")\n",
    "\n",
    "    # 1. Fetch all sessions in a single query\n",
    "    sessions_query = \"\"\"\n",
    "    MATCH (s)\n",
    "    WHERE s:Sessions_past_year_dva OR s:Sessions_past_year_lva OR s:Sessions_this_year OR s:Sessions_past_year\n",
    "    RETURN s.session_id as session_id, s.title as title, \n",
    "           s.stream as stream, s.synopsis_stripped as synopsis_stripped,\n",
    "           s.theatre__name as theatre__name, s.sponsored_by as sponsored_by,\n",
    "           s.sponsored_session as sponsored_session, labels(s)[0] as type,\n",
    "           CASE WHEN s.key_text IS NOT NULL THEN s.key_text ELSE '' END as key_text\n",
    "    \"\"\"\n",
    "    all_sessions = tx.run(sessions_query).data()\n",
    "    print(\n",
    "        f\"Fetched {len(all_sessions)} sessions from database in {time.time() - start_time:.2f}s\"\n",
    "    )\n",
    "\n",
    "    # 2. Fetch all stream descriptions at once\n",
    "    stream_query = \"\"\"\n",
    "    MATCH (s:Stream)\n",
    "    RETURN s.stream as stream, s.description as description\n",
    "    \"\"\"\n",
    "    stream_data = tx.run(stream_query).data()\n",
    "\n",
    "    # Create a dictionary of stream descriptions for quick lookup\n",
    "    stream_descriptions = {s[\"stream\"].lower(): s[\"description\"] for s in stream_data}\n",
    "    cache.stream_descriptions = stream_descriptions\n",
    "\n",
    "    # Process all sessions at once - this is CPU-bound, so we'll use batching\n",
    "    print(\"Computing embeddings for all sessions...\")\n",
    "    batch_size = 100  # Process in smaller batches to avoid memory issues\n",
    "    embeddings = {}\n",
    "\n",
    "    # Store raw session data for quick lookup later\n",
    "    cache.all_sessions_data = {s[\"session_id\"]: s for s in all_sessions}\n",
    "\n",
    "    for i in range(0, len(all_sessions), batch_size):\n",
    "        batch = all_sessions[i : i + batch_size]\n",
    "        batch_embeddings = {}\n",
    "\n",
    "        for s in batch:\n",
    "            # Process the stream field - split by semicolon and handle duplicates\n",
    "            session_streams = []\n",
    "            if s[\"stream\"]:\n",
    "                # Split the stream string and strip whitespace\n",
    "                stream_list = [\n",
    "                    stream.strip().lower() for stream in s[\"stream\"].split(\";\")\n",
    "                ]\n",
    "                # Remove duplicates by converting to set and back to list\n",
    "                stream_list = list(set(stream_list))\n",
    "\n",
    "                # Get the description for each stream\n",
    "                for stream in stream_list:\n",
    "                    if stream in stream_descriptions:\n",
    "                        session_streams.append(stream)\n",
    "\n",
    "            # Create session text for embedding\n",
    "            base_text = f\"{s['title']} {s['synopsis_stripped']} {s['theatre__name']}\"\n",
    "            if session_streams and len(session_streams) > 0:\n",
    "                stream_text = \" \".join(session_streams)\n",
    "                text = f\"{base_text} {stream_text}\"\n",
    "            else:\n",
    "                text = base_text\n",
    "\n",
    "            # Add to batch for embedding\n",
    "            batch_embeddings[s[\"session_id\"]] = {\n",
    "                \"text\": text,\n",
    "                \"type\": s[\"type\"],\n",
    "                \"theatre__name\": s[\"theatre__name\"],\n",
    "                \"stream\": s[\"stream\"],\n",
    "                \"title\": s[\"title\"],\n",
    "                \"sponsored_by\": (\n",
    "                    s[\"sponsored_by\"] if \"sponsored_by\" in s else \"Not Sponsored\"\n",
    "                ),\n",
    "                \"sponsored_session\": (\n",
    "                    s[\"sponsored_session\"] if \"sponsored_session\" in s else \"False\"\n",
    "                ),\n",
    "                \"key_text\": s[\"key_text\"] if \"key_text\" in s else \"\",\n",
    "            }\n",
    "\n",
    "        # Compute embeddings for entire batch at once\n",
    "        texts = [data[\"text\"] for _, data in batch_embeddings.items()]\n",
    "        batch_vectors = model.encode(texts, show_progress_bar=False)\n",
    "\n",
    "        # Store embeddings\n",
    "        for (session_id, data), vector in zip(batch_embeddings.items(), batch_vectors):\n",
    "            data[\"embedding\"] = vector\n",
    "            embeddings[session_id] = data\n",
    "\n",
    "        if (i + batch_size) % 500 == 0 or (i + batch_size) >= len(all_sessions):\n",
    "            print(\n",
    "                f\"Processed {min(i + batch_size, len(all_sessions))}/{len(all_sessions)} sessions\"\n",
    "            )\n",
    "\n",
    "    print(f\"Finished computing all embeddings in {time.time() - start_time:.2f}s\")\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Initialize the cache with precomputed data\n",
    "with driver.session() as session:\n",
    "    print(\"Starting data precomputation...\")\n",
    "    cache.session_embeddings = session.execute_read(precompute_all_data)\n",
    "    print(\"Data precomputation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea3063-f9cc-4e34-bb78-87cf1e9ed27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Define job role categories\n",
    "VET_ROLES = [\n",
    "    \"Vet/Vet Surgeon\",\n",
    "    \"Assistant Vet\",\n",
    "    \"Vet/Owner\",\n",
    "    \"Clinical or other Director\",\n",
    "    \"Locum Vet\",\n",
    "    \"Academic\",\n",
    "]\n",
    "\n",
    "NURSE_ROLES = [\"Head Nurse/Senior Nurse\", \"Vet Nurse\", \"Locum RVN\"]\n",
    "\n",
    "BUSINESS = [\"Practice Manager\", \"Practice Partner/Owner\"]\n",
    "# Other roles can attend any session\n",
    "OTHER_ROLES = [\"Student\", \"Receptionist\", \"Other (please specify)\"]\n",
    "\n",
    "\n",
    "# Define session embedding\n",
    "def create_session_embedding(session, stream_descriptions=None):\n",
    "    # Include title, synopsis_stripped, and theatre__name as required\n",
    "    base_text = (\n",
    "        f\"{session['title']} {session['synopsis_stripped']} {session['theatre__name']}\"\n",
    "    )\n",
    "\n",
    "    # Add stream descriptions if provided\n",
    "    if stream_descriptions and len(stream_descriptions) > 0:\n",
    "        stream_desc_text = \" \".join(stream_descriptions)\n",
    "        text = f\"{base_text} {stream_desc_text}\"\n",
    "    else:\n",
    "        text = base_text\n",
    "\n",
    "    return model.encode(text)\n",
    "\n",
    "\n",
    "# Embedding function for all sessions\n",
    "def embed_all_sessions(tx):\n",
    "    # Query all Sessions from past_year_dva, past_year_lva, Sessions_past_year (for compatibility) and this_year\n",
    "    # Include the new sponsored_by and sponsored_session attributes\n",
    "    query = \"\"\"\n",
    "    MATCH (s)\n",
    "    WHERE s:Sessions_past_year_dva OR s:Sessions_past_year_lva OR s:Sessions_this_year OR s:Sessions_past_year\n",
    "    RETURN s.session_id as session_id, s.title as title, \n",
    "           s.stream as stream, s.synopsis_stripped as synopsis_stripped,\n",
    "           s.theatre__name as theatre__name, s.sponsored_by as sponsored_by,\n",
    "           s.sponsored_session as sponsored_session, labels(s)[0] as type,\n",
    "           CASE WHEN s.key_text IS NOT NULL THEN s.key_text ELSE '' END as key_text\n",
    "    \"\"\"\n",
    "    sessions = tx.run(query).data()\n",
    "\n",
    "    # Fetch all stream descriptions once to avoid multiple queries\n",
    "    stream_query = \"\"\"\n",
    "    MATCH (s:Stream)\n",
    "    RETURN s.stream as stream, s.description as description\n",
    "    \"\"\"\n",
    "    stream_data = tx.run(stream_query).data()\n",
    "\n",
    "    # Create a dictionary of stream descriptions for quick lookup\n",
    "    stream_descriptions = {s[\"stream\"].lower(): s[\"description\"] for s in stream_data}\n",
    "\n",
    "    embeddings = {}\n",
    "    for s in sessions:\n",
    "        # Process the stream field - split by semicolon and handle duplicates\n",
    "        session_streams = []\n",
    "        if s[\"stream\"]:\n",
    "            # Split the stream string and strip whitespace\n",
    "            stream_list = [stream.strip().lower() for stream in s[\"stream\"].split(\";\")]\n",
    "            # Remove duplicates by converting to set and back to list\n",
    "            stream_list = list(set(stream_list))\n",
    "\n",
    "            # Get the description for each stream\n",
    "            for stream in stream_list:\n",
    "                if stream in stream_descriptions:\n",
    "                    # session_streams.append(stream_descriptions[stream])\n",
    "                    session_streams.append(stream)\n",
    "\n",
    "        # Create embedding with the session data and stream descriptions\n",
    "        embeddings[s[\"session_id\"]] = {\n",
    "            \"type\": s[\"type\"],\n",
    "            \"embedding\": create_session_embedding(s, session_streams),\n",
    "            \"theatre__name\": s[\"theatre__name\"],\n",
    "            \"stream\": s[\"stream\"],\n",
    "            \"title\": s[\"title\"],\n",
    "            \"sponsored_by\": (\n",
    "                s[\"sponsored_by\"] if \"sponsored_by\" in s else \"Not Sponsored\"\n",
    "            ),\n",
    "            \"sponsored_session\": (\n",
    "                s[\"sponsored_session\"] if \"sponsored_session\" in s else \"False\"\n",
    "            ),\n",
    "            \"key_text\": s[\"key_text\"] if \"key_text\" in s else \"\",\n",
    "        }\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Visitor-based similarity\n",
    "def visitor_similarity(v1, v2):\n",
    "    \"\"\"\n",
    "    Calculate similarity between two visitors using only specific attributes:\n",
    "    - what_type_does_your_practice_specialise_in\n",
    "    - job_role\n",
    "    - organisation_type\n",
    "\n",
    "    Only use attributes if they are not \"NA\", and apply defaults if all are NA.\n",
    "    \"\"\"\n",
    "    # Attributes to compare\n",
    "    attrs_to_compare = [\n",
    "        \"what_type_does_your_practice_specialise_in\",\n",
    "        \"job_role\",\n",
    "        \"organisation_type\",\n",
    "    ]\n",
    "\n",
    "    # Count how many attributes are not \"NA\" for both visitors\n",
    "    valid_attrs = []\n",
    "    for attr in attrs_to_compare:\n",
    "        # Check if attribute exists and is not \"NA\" for both visitors\n",
    "        if attr in v1 and v1[attr] != \"NA\" and attr in v2 and v2[attr] != \"NA\":\n",
    "            valid_attrs.append(attr)\n",
    "\n",
    "    # If all three attributes are NA, use defaults\n",
    "    # Apply defaults only to v1 (the current visitor we're finding recommendations for)\n",
    "    defaults_applied = False\n",
    "    if len(valid_attrs) == 0:\n",
    "        # Apply defaults to v1\n",
    "        v1_copy = dict(v1)  # Create a copy to avoid modifying the original\n",
    "        v1_copy[\"job_role\"] = \"Vet/Owner\"\n",
    "        v1_copy[\"what_type_does_your_practice_specialise_in\"] = \"Mixed\"\n",
    "\n",
    "        # Re-evaluate which attributes to use\n",
    "        valid_attrs = []\n",
    "        for attr in attrs_to_compare:\n",
    "            if (\n",
    "                attr in v1_copy\n",
    "                and v1_copy[attr] != \"NA\"\n",
    "                and attr in v2\n",
    "                and v2[attr] != \"NA\"\n",
    "            ):\n",
    "                valid_attrs.append(attr)\n",
    "\n",
    "        # Use the copy for comparison\n",
    "        v1 = v1_copy\n",
    "        defaults_applied = True\n",
    "\n",
    "    # Calculate similarity only on valid attributes\n",
    "    if len(valid_attrs) > 0:\n",
    "        matches = sum(1 for attr in valid_attrs if v1[attr] == v2[attr])\n",
    "        similarity = matches / len(valid_attrs)\n",
    "    else:\n",
    "        # If still no valid attributes (unlikely but possible)\n",
    "        similarity = 0.0\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# Modularized function to get this year's sessions filtered by practice type and role\n",
    "def get_filtered_sessions_for_visitor(tx, visitor_id, practice_type, job_role):\n",
    "    \"\"\"\n",
    "    Get sessions for this year that are appropriate for the visitor's practice type and role.\n",
    "    Uses direct Neo4j relationships for more accurate filtering.\n",
    "    \"\"\"\n",
    "    # First filter out President-related sessions\n",
    "    base_query = \"\"\"\n",
    "    MATCH (s:Sessions_this_year)\n",
    "    WHERE NOT s.title = \"BVA's President's Welcome\" AND NOT s.title CONTAINS \"President\"\n",
    "    \"\"\"\n",
    "\n",
    "    # If practice type is null, \"no_data\", or \"NA\", we can't apply practice-based rules\n",
    "    if not practice_type or practice_type.lower() in [\"no_data\", \"na\"]:\n",
    "        # Only apply role-based filtering\n",
    "        if job_role in NURSE_ROLES:\n",
    "            # Rule 3: Nurses can only get sessions with nursing, wellbeing, or welfare streams\n",
    "            query = (\n",
    "                base_query\n",
    "                + \"\"\"\n",
    "            MATCH (s)-[:HAS_STREAM]->(stream:Stream)\n",
    "            WHERE stream.stream IN [\"nursing\", \"wellbeing\", \"welfare\"]\n",
    "            RETURN DISTINCT s.session_id as session_id\n",
    "            \"\"\"\n",
    "            )\n",
    "        elif job_role in VET_ROLES:\n",
    "            # Rule 4: Vets cannot get nursing sessions\n",
    "            query = (\n",
    "                base_query\n",
    "                + \"\"\"\n",
    "            MATCH (s)\n",
    "            WHERE NOT EXISTS {\n",
    "                MATCH (s)-[:HAS_STREAM]->(stream:Stream)\n",
    "                WHERE stream.stream = \"nursing\"\n",
    "            }\n",
    "            RETURN DISTINCT s.session_id as session_id\n",
    "            \"\"\"\n",
    "            )\n",
    "        else:\n",
    "            # No specific role filtering\n",
    "            query = (\n",
    "                base_query\n",
    "                + \"\"\"\n",
    "            RETURN DISTINCT s.session_id as session_id\n",
    "            \"\"\"\n",
    "            )\n",
    "    else:\n",
    "        # Split practice type into tokens\n",
    "        practice_types = [pt.strip().lower() for pt in practice_type.split(\";\")]\n",
    "\n",
    "        if \"equine\" in practice_types or \"mixed\" in practice_types:\n",
    "            # Rule 1: Equine/Mixed practices cannot get exotics, feline, exotic animal, farm, small animal sessions\n",
    "            excluded_streams = [\n",
    "                \"exotics\",\n",
    "                \"feline\",\n",
    "                \"exotic animal\",\n",
    "                \"farm\",\n",
    "                \"small animal\",\n",
    "            ]\n",
    "\n",
    "            if job_role in NURSE_ROLES:\n",
    "                # Combine Rule 1 and Rule 3\n",
    "                query = (\n",
    "                    base_query\n",
    "                    + \"\"\"\n",
    "                MATCH (s)-[:HAS_STREAM]->(stream:Stream)\n",
    "                WHERE stream.stream IN [\"nursing\", \"wellbeing\", \"welfare\"]\n",
    "                AND NOT EXISTS {\n",
    "                    MATCH (s)-[:HAS_STREAM]->(excluded:Stream)\n",
    "                    WHERE toLower(excluded.stream) IN $excluded_streams\n",
    "                }\n",
    "                RETURN DISTINCT s.session_id as session_id\n",
    "                \"\"\"\n",
    "                )\n",
    "            elif job_role in VET_ROLES:\n",
    "                # Combine Rule 1 and Rule 4\n",
    "                query = (\n",
    "                    base_query\n",
    "                    + \"\"\"\n",
    "                MATCH (s)\n",
    "                WHERE NOT EXISTS {\n",
    "                    MATCH (s)-[:HAS_STREAM]->(stream:Stream)\n",
    "                    WHERE toLower(stream.stream) = \"nursing\" OR toLower(stream.stream) IN $excluded_streams\n",
    "                }\n",
    "                RETURN DISTINCT s.session_id as session_id\n",
    "                \"\"\"\n",
    "                )\n",
    "            else:\n",
    "                # Only Rule 1\n",
    "                query = (\n",
    "                    base_query\n",
    "                    + \"\"\"\n",
    "                MATCH (s)\n",
    "                WHERE NOT EXISTS {\n",
    "                    MATCH (s)-[:HAS_STREAM]->(stream:Stream)\n",
    "                    WHERE toLower(stream.stream) IN $excluded_streams\n",
    "                }\n",
    "                RETURN DISTINCT s.session_id as session_id\n",
    "                \"\"\"\n",
    "                )\n",
    "\n",
    "            return tx.run(query, excluded_streams=excluded_streams).data()\n",
    "\n",
    "        elif any(\"small animal\" in pt for pt in practice_types):\n",
    "            # Rule 2: Small Animal practices cannot get equine, farm animal, farm, large animal sessions\n",
    "            excluded_streams = [\"equine\", \"farm animal\", \"farm\", \"large animal\"]\n",
    "\n",
    "            if job_role in NURSE_ROLES:\n",
    "                # Combine Rule 2 and Rule 3\n",
    "                query = (\n",
    "                    base_query\n",
    "                    + \"\"\"\n",
    "                MATCH (s)-[:HAS_STREAM]->(stream:Stream)\n",
    "                WHERE stream.stream IN [\"nursing\", \"wellbeing\", \"welfare\"]\n",
    "                AND NOT EXISTS {\n",
    "                    MATCH (s)-[:HAS_STREAM]->(excluded:Stream)\n",
    "                    WHERE toLower(excluded.stream) IN $excluded_streams\n",
    "                }\n",
    "                RETURN DISTINCT s.session_id as session_id\n",
    "                \"\"\"\n",
    "                )\n",
    "            elif job_role in VET_ROLES:\n",
    "                # Combine Rule 2 and Rule 4\n",
    "                query = (\n",
    "                    base_query\n",
    "                    + \"\"\"\n",
    "                MATCH (s)\n",
    "                WHERE NOT EXISTS {\n",
    "                    MATCH (s)-[:HAS_STREAM]->(stream:Stream)\n",
    "                    WHERE toLower(stream.stream) = \"nursing\" OR toLower(stream.stream) IN $excluded_streams\n",
    "                }\n",
    "                RETURN DISTINCT s.session_id as session_id\n",
    "                \"\"\"\n",
    "                )\n",
    "            else:\n",
    "                # Only Rule 2\n",
    "                query = (\n",
    "                    base_query\n",
    "                    + \"\"\"\n",
    "                MATCH (s)\n",
    "                WHERE NOT EXISTS {\n",
    "                    MATCH (s)-[:HAS_STREAM]->(stream:Stream)\n",
    "                    WHERE toLower(stream.stream) IN $excluded_streams\n",
    "                }\n",
    "                RETURN DISTINCT s.session_id as session_id\n",
    "                \"\"\"\n",
    "                )\n",
    "\n",
    "            return tx.run(query, excluded_streams=excluded_streams).data()\n",
    "\n",
    "        else:\n",
    "            # No specific practice type filtering\n",
    "            if job_role in NURSE_ROLES:\n",
    "                # Only Rule 3\n",
    "                query = (\n",
    "                    base_query\n",
    "                    + \"\"\"\n",
    "                MATCH (s)-[:HAS_STREAM]->(stream:Stream)\n",
    "                WHERE stream.stream IN [\"nursing\", \"wellbeing\", \"welfare\"]\n",
    "                RETURN DISTINCT s.session_id as session_id\n",
    "                \"\"\"\n",
    "                )\n",
    "            elif job_role in VET_ROLES:\n",
    "                # Only Rule 4\n",
    "                query = (\n",
    "                    base_query\n",
    "                    + \"\"\"\n",
    "                MATCH (s)\n",
    "                WHERE NOT EXISTS {\n",
    "                    MATCH (s)-[:HAS_STREAM]->(stream:Stream)\n",
    "                    WHERE stream.stream = \"nursing\"\n",
    "                }\n",
    "                RETURN DISTINCT s.session_id as session_id\n",
    "                \"\"\"\n",
    "                )\n",
    "            else:\n",
    "                # No filtering\n",
    "                query = (\n",
    "                    base_query\n",
    "                    + \"\"\"\n",
    "                RETURN DISTINCT s.session_id as session_id\n",
    "                \"\"\"\n",
    "                )\n",
    "\n",
    "    return tx.run(query).data()\n",
    "\n",
    "\n",
    "# Function to check if a session is appropriate for a visitor's job role\n",
    "def is_session_appropriate_for_role(session_stream, job_role):\n",
    "    \"\"\"\n",
    "    Determine if a session is appropriate for a visitor's job role.\n",
    "\n",
    "    Rule 3: NURSE_ROLES can only be recommended sessions with streams: [nursing, wellbeing, welfare]\n",
    "    Rule 4: VET_ROLES cannot be recommended sessions with streams: [nursing]\n",
    "\n",
    "    Session_stream can be a semicolon-separated list.\n",
    "    \"\"\"\n",
    "    if not session_stream or not job_role:\n",
    "        return True, \"No role or session stream restrictions\"\n",
    "\n",
    "    # Split session stream into tokens\n",
    "    session_streams = [ss.strip().lower() for ss in session_stream.split(\";\")]\n",
    "\n",
    "    # Rule 3: For Nurse roles\n",
    "    if job_role in NURSE_ROLES:\n",
    "        nurse_streams = [\"nursing\", \"wellbeing\", \"welfare\"]\n",
    "        # Check if ANY session stream matches ANY nurse stream\n",
    "        has_nurse_content = any(\n",
    "            any(nurse_stream in ss for nurse_stream in nurse_streams)\n",
    "            for ss in session_streams\n",
    "        )\n",
    "        if not has_nurse_content:\n",
    "            return False, \"Not suitable for nursing roles\"\n",
    "        return True, \"Nursing content\"\n",
    "\n",
    "    # Rule 4: For Vet roles\n",
    "    elif job_role in VET_ROLES:\n",
    "        # Check if ANY session stream contains \"nursing\"\n",
    "        if any(\"nursing\" in ss for ss in session_streams):\n",
    "            return False, \"Not suitable for veterinary roles\"\n",
    "\n",
    "    return True, \"Suitable for role\"\n",
    "\n",
    "\n",
    "# Modularized function to get visitor information\n",
    "def get_visitor_info(tx, visitor_id):\n",
    "    \"\"\"Get visitor information including practice type and job role.\"\"\"\n",
    "    visitor_query = \"\"\"\n",
    "    MATCH (v:Visitor_this_year {BadgeId: $visitor_id})\n",
    "    RETURN v\n",
    "    \"\"\"\n",
    "    visitor_data = tx.run(visitor_query, visitor_id=visitor_id).single()\n",
    "    if not visitor_data:\n",
    "        return None, None, None\n",
    "\n",
    "    visitor = visitor_data[\"v\"]\n",
    "    assisted = visitor.get(\"assist_year_before\", \"0\")\n",
    "    practice_type = visitor.get(\"what_type_does_your_practice_specialise_in\", \"\")\n",
    "    job_role = visitor.get(\"job_role\", \"\")\n",
    "\n",
    "    return visitor, assisted, practice_type, job_role\n",
    "\n",
    "\n",
    "# Modularized function to get past sessions for a visitor who assisted last year\n",
    "def get_past_sessions(tx, visitor_id):\n",
    "    \"\"\"Get sessions the visitor attended last year.\"\"\"\n",
    "    query_past = \"\"\"\n",
    "    MATCH (v:Visitor_this_year {BadgeId: $visitor_id})-[:Same_Visitor]->(vp_bva:Visitor_last_year_bva)-[:attended_session]->(sp_bva:Sessions_past_year)\n",
    "    RETURN sp_bva.session_id as session_id\n",
    "    UNION\n",
    "    MATCH (v:Visitor_this_year {BadgeId: $visitor_id})-[:Same_Visitor]->(vp_lva:Visitor_last_year_lva)-[:attended_session]->(sp_lva:Sessions_past_year)\n",
    "    RETURN sp_lva.session_id as session_id\n",
    "    \"\"\"\n",
    "    return tx.run(query_past, visitor_id=visitor_id).data()\n",
    "\n",
    "\n",
    "# Modularized function to get similar visitors\n",
    "def get_similar_visitors(tx, visitor):\n",
    "    \"\"\"Find similar visitors who attended last year.\"\"\"\n",
    "    all_visitors = tx.run(\n",
    "        \"\"\"\n",
    "        MATCH (v:Visitor_this_year)\n",
    "        WHERE v.assist_year_before = '1'\n",
    "        RETURN v\n",
    "        \"\"\"\n",
    "    ).data()\n",
    "\n",
    "    # Calculate similarities\n",
    "    similarities = []\n",
    "    for vdata in all_visitors:\n",
    "        v_compare = vdata[\"v\"]\n",
    "        sim = visitor_similarity(visitor, v_compare)\n",
    "        similarities.append((v_compare[\"BadgeId\"], sim))\n",
    "\n",
    "    # Sort by similarity and get top 3 similar visitors\n",
    "    similarities.sort(key=lambda x: -x[1])\n",
    "    return [sid for sid, _ in similarities[:3]]\n",
    "\n",
    "\n",
    "# Modularized function to get sessions attended by similar visitors\n",
    "def get_similar_visitor_sessions(tx, similar_visitor_badge_ids):\n",
    "    \"\"\"Get sessions attended by similar visitors.\"\"\"\n",
    "    similar_visitor_sessions = []\n",
    "\n",
    "    for similar_vid in similar_visitor_badge_ids:\n",
    "        query_sim_past = \"\"\"\n",
    "            MATCH (v:Visitor_this_year {BadgeId: $similar_vid})-[:Same_Visitor]->(vp)-[:attended_session]->(sp:Sessions_past_year)\n",
    "            WHERE vp:Visitor_last_year_bva OR vp:Visitor_last_year_lva\n",
    "            RETURN sp.session_id AS session_id\n",
    "        \"\"\"\n",
    "        sessions = tx.run(query_sim_past, similar_vid=similar_vid).data()\n",
    "        similar_visitor_sessions.extend(sessions)\n",
    "\n",
    "    return similar_visitor_sessions\n",
    "\n",
    "\n",
    "# Modularized function to calculate session similarities\n",
    "def calculate_session_similarities(\n",
    "    past_sessions, session_embeddings, this_year_sessions, practice_type, job_role\n",
    "):\n",
    "    \"\"\"Calculate similarities between past sessions and this year's sessions.\"\"\"\n",
    "    recommendations = []\n",
    "\n",
    "    for past_sess in past_sessions:\n",
    "        # Skip if the session_id doesn't exist in embeddings\n",
    "        if past_sess[\"session_id\"] not in session_embeddings:\n",
    "            continue\n",
    "\n",
    "        past_emb = session_embeddings[past_sess[\"session_id\"]][\"embedding\"]\n",
    "\n",
    "        for sid, data in this_year_sessions.items():\n",
    "            title = data.get(\"title\", \"\")\n",
    "\n",
    "            # Apply practice type filtering\n",
    "            is_appropriate_practice, practice_reason = (\n",
    "                is_session_appropriate_for_practice_type(\n",
    "                    data[\"stream\"], practice_type, title\n",
    "                )\n",
    "            )\n",
    "            if not is_appropriate_practice:\n",
    "                continue\n",
    "\n",
    "            # Apply role-based filtering\n",
    "            is_appropriate_role, role_reason = is_session_appropriate_for_role(\n",
    "                data[\"stream\"], job_role\n",
    "            )\n",
    "            if not is_appropriate_role:\n",
    "                continue\n",
    "\n",
    "            # Calculate similarity based on embeddings (Rule 7)\n",
    "            sim = cosine_similarity([past_emb], [data[\"embedding\"]])[0][0]\n",
    "\n",
    "            # Combine reasons for recommendation\n",
    "            reason = f\"{role_reason}. {practice_reason}\"\n",
    "\n",
    "            recommendations.append(\n",
    "                {\"session_id\": sid, \"similarity\": sim, \"reason\": reason}\n",
    "            )\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# Refactored recommend_sessions function\n",
    "def recommend_sessions(tx, visitor_id, session_embeddings):\n",
    "    \"\"\"\n",
    "    Recommend sessions for a visitor based on their profile and history.\n",
    "\n",
    "    Implementation of new rules:\n",
    "    1. Filter by practice type (Equine/Mixed, Small Animal)\n",
    "    2. Filter by job role (Nurse, Vet)\n",
    "    3. Base scoring on embedding similarity\n",
    "    4. Keep track of reasons for recommendations\n",
    "    \"\"\"\n",
    "    # Get visitor information\n",
    "    visitor, assisted, practice_type, job_role = get_visitor_info(tx, visitor_id)\n",
    "    if not visitor:\n",
    "        return []\n",
    "\n",
    "    # Get filtered sessions that match the visitor's practice type and role\n",
    "    filtered_sessions_data = get_filtered_sessions_for_visitor(\n",
    "        tx, visitor_id, practice_type, job_role\n",
    "    )\n",
    "    filtered_session_ids = {s[\"session_id\"] for s in filtered_sessions_data}\n",
    "\n",
    "    # Filter this year's sessions to only include those matching our criteria\n",
    "    this_year_sessions = {\n",
    "        k: v\n",
    "        for k, v in session_embeddings.items()\n",
    "        if v[\"type\"] == \"Sessions_this_year\" and k in filtered_session_ids\n",
    "    }\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    if assisted == \"1\":\n",
    "        # Case 1: Visitor attended last year\n",
    "        past_sessions = get_past_sessions(tx, visitor_id)\n",
    "\n",
    "        # Calculate similarities with past sessions\n",
    "        for past_sess in past_sessions:\n",
    "            # Skip if the session_id doesn't exist in embeddings\n",
    "            if past_sess[\"session_id\"] not in session_embeddings:\n",
    "                continue\n",
    "\n",
    "            past_emb = session_embeddings[past_sess[\"session_id\"]][\"embedding\"]\n",
    "\n",
    "            for sid, data in this_year_sessions.items():\n",
    "                # Calculate similarity based on embeddings (Rule 7)\n",
    "                sim = cosine_similarity([past_emb], [data[\"embedding\"]])[0][0]\n",
    "\n",
    "                # Create recommendation reason\n",
    "                if job_role in NURSE_ROLES:\n",
    "                    reason = \"Nursing content based on your past attendance\"\n",
    "                elif job_role in VET_ROLES:\n",
    "                    reason = \"Veterinary content based on your past attendance\"\n",
    "                else:\n",
    "                    reason = \"Based on your past attendance\"\n",
    "\n",
    "                recommendations.append(\n",
    "                    {\"session_id\": sid, \"similarity\": sim, \"reason\": reason}\n",
    "                )\n",
    "\n",
    "    else:\n",
    "        # Case 2: New visitor - find similar visitors with history\n",
    "        similar_visitor_badge_ids = get_similar_visitors(tx, visitor)\n",
    "\n",
    "        # Get sessions attended by similar visitors\n",
    "        similar_visitor_sessions = get_similar_visitor_sessions(\n",
    "            tx, similar_visitor_badge_ids\n",
    "        )\n",
    "\n",
    "        # Calculate similarities with sessions attended by similar visitors\n",
    "        for past_sess in similar_visitor_sessions:\n",
    "            # Skip if the session_id doesn't exist in embeddings\n",
    "            if past_sess[\"session_id\"] not in session_embeddings:\n",
    "                continue\n",
    "\n",
    "            past_emb = session_embeddings[past_sess[\"session_id\"]][\"embedding\"]\n",
    "\n",
    "            for sid, data in this_year_sessions.items():\n",
    "                # Calculate similarity based on embeddings (Rule 7)\n",
    "                sim = cosine_similarity([past_emb], [data[\"embedding\"]])[0][0]\n",
    "\n",
    "                # Create recommendation reason\n",
    "                if job_role in NURSE_ROLES:\n",
    "                    reason = \"Nursing content based on similar visitors\"\n",
    "                elif job_role in VET_ROLES:\n",
    "                    reason = \"Veterinary content based on similar visitors\"\n",
    "                else:\n",
    "                    reason = \"Based on similar visitors with your profile\"\n",
    "\n",
    "                recommendations.append(\n",
    "                    {\"session_id\": sid, \"similarity\": sim, \"reason\": reason}\n",
    "                )\n",
    "\n",
    "    # Remove duplicates (keeping the highest similarity score)\n",
    "    session_to_best_rec = {}\n",
    "    for rec in recommendations:\n",
    "        sid = rec[\"session_id\"]\n",
    "        if (\n",
    "            sid not in session_to_best_rec\n",
    "            or rec[\"similarity\"] > session_to_best_rec[sid][\"similarity\"]\n",
    "        ):\n",
    "            session_to_best_rec[sid] = rec\n",
    "\n",
    "    # Convert back to list and sort by similarity\n",
    "    unique_recommendations = list(session_to_best_rec.values())\n",
    "    unique_recommendations.sort(key=lambda x: -x[\"similarity\"])\n",
    "\n",
    "    return unique_recommendations\n",
    "\n",
    "\n",
    "# Modified function to filter sessions with similarity scores\n",
    "def filter_sessions_by_visitor_stream_relationships(\n",
    "    tx, visitor_id, session_recommendations\n",
    "):\n",
    "    \"\"\"\n",
    "    Filter sessions to keep only those where the visitor has a relationship\n",
    "    (specialization_to_stream or job_to_stream) to any Stream that has a HAS_STREAM\n",
    "    relationship to the session. Preserves similarity scores.\n",
    "    \"\"\"\n",
    "    if not session_recommendations:\n",
    "        return []\n",
    "\n",
    "    # Extract just the session IDs for the query\n",
    "    session_ids = [rec[\"session_id\"] for rec in session_recommendations]\n",
    "\n",
    "    # Query to find sessions with valid stream relationships to the visitor\n",
    "    query = \"\"\"\n",
    "    MATCH (v:Visitor_this_year {BadgeId: $visitor_id})\n",
    "    MATCH (s:Sessions_this_year)\n",
    "    WHERE s.session_id IN $session_ids\n",
    "    MATCH (s)-[:HAS_STREAM]->(stream:Stream)<-[r]-(v)\n",
    "    WHERE type(r) IN ['specialization_to_stream', 'job_to_stream']\n",
    "    RETURN DISTINCT s.session_id as session_id\n",
    "    \"\"\"\n",
    "\n",
    "    results = tx.run(query, visitor_id=visitor_id, session_ids=session_ids).data()\n",
    "    valid_session_ids = set(r[\"session_id\"] for r in results)\n",
    "\n",
    "    # Filter the original recommendations to keep only valid sessions with their scores\n",
    "    filtered_recommendations = [\n",
    "        rec for rec in session_recommendations if rec[\"session_id\"] in valid_session_ids\n",
    "    ]\n",
    "\n",
    "    return filtered_recommendations\n",
    "\n",
    "\n",
    "# Modified get_session_attributes to include similarity scores\n",
    "def get_session_attributes(session_recommendations):\n",
    "    # Handle empty recommendations to prevent errors\n",
    "    if not session_recommendations:\n",
    "        return []\n",
    "\n",
    "    # Extract just the session IDs for the query\n",
    "    session_ids = [rec[\"session_id\"] for rec in session_recommendations]\n",
    "\n",
    "    # Create a mapping of session_id to similarity score for later use\n",
    "    similarity_map = {\n",
    "        rec[\"session_id\"]: {\"similarity\": rec[\"similarity\"], \"reason\": rec[\"reason\"]}\n",
    "        for rec in session_recommendations\n",
    "    }\n",
    "\n",
    "    query = \"\"\"\n",
    "    MATCH (s:Sessions_this_year)\n",
    "    WHERE s.session_id IN $session_ids\n",
    "    RETURN s {\n",
    "        .stream,\n",
    "        .session_id,\n",
    "        .title,\n",
    "        .synopsis_stripped,\n",
    "        .end_time,\n",
    "        .start_time,\n",
    "        .date,\n",
    "        .theatre__name,\n",
    "        .sponsored_by,\n",
    "        .sponsored_session\n",
    "    } AS session_details\n",
    "    \"\"\"\n",
    "\n",
    "    with driver.session() as session:\n",
    "        results = session.run(query, session_ids=session_ids)\n",
    "        session_details = []\n",
    "\n",
    "        for record in results:\n",
    "            details = record[\"session_details\"]\n",
    "            # Add similarity score and reason to the session details\n",
    "            session_id = details[\"session_id\"]\n",
    "            if session_id in similarity_map:\n",
    "                details[\"similarity_score\"] = similarity_map[session_id][\"similarity\"]\n",
    "                details[\"recommendation_reason\"] = similarity_map[session_id][\"reason\"]\n",
    "            session_details.append(details)\n",
    "\n",
    "    # Sort by similarity score (highest first)\n",
    "    session_details.sort(key=lambda x: x.get(\"similarity_score\", 0), reverse=True)\n",
    "\n",
    "    return session_details\n",
    "\n",
    "\n",
    "# Cached embeddings at the module level\n",
    "_session_embeddings = None\n",
    "\n",
    "\n",
    "def get_recommendations(visitor_id, max_recommendations=None):\n",
    "    \"\"\"\n",
    "    Main function to get recommendations for a visitor.\n",
    "    Returns two sets of session details: filtered and unfiltered, both including similarity scores.\n",
    "\n",
    "    Parameters:\n",
    "    - visitor_id: The ID of the visitor to get recommendations for\n",
    "    - max_recommendations: Maximum number of recommendations to return (None = no limit)\n",
    "\n",
    "    Returns:\n",
    "    - A tuple (filtered_recommendations, unfiltered_recommendations) with session details\n",
    "    \"\"\"\n",
    "    global _session_embeddings\n",
    "\n",
    "    # Create session embeddings once if they don't exist\n",
    "    if _session_embeddings is None:\n",
    "        with driver.session() as session:\n",
    "            _session_embeddings = session.execute_read(embed_all_sessions)\n",
    "\n",
    "    # Use the cached embeddings for recommendations\n",
    "    with driver.session() as session:\n",
    "        # Get initial recommended sessions with similarity scores\n",
    "        recommended_sessions = session.execute_read(\n",
    "            recommend_sessions,\n",
    "            visitor_id=visitor_id,\n",
    "            session_embeddings=_session_embeddings,\n",
    "        )\n",
    "\n",
    "        # Limit the number of recommendations if specified\n",
    "        if max_recommendations is not None and max_recommendations > 0:\n",
    "            recommended_sessions = recommended_sessions[:max_recommendations]\n",
    "\n",
    "        # Get the details of the unfiltered recommended sessions\n",
    "        unfiltered_recommendations = get_session_attributes(recommended_sessions)\n",
    "\n",
    "        # Apply the additional filter for stream relationships\n",
    "        filtered_sessions = session.execute_read(\n",
    "            filter_sessions_by_visitor_stream_relationships,\n",
    "            visitor_id=visitor_id,\n",
    "            session_recommendations=recommended_sessions,\n",
    "        )\n",
    "\n",
    "        # Limit the filtered recommendations if specified\n",
    "        if max_recommendations is not None and max_recommendations > 0:\n",
    "            filtered_sessions = filtered_sessions[:max_recommendations]\n",
    "\n",
    "    # Get the details of the filtered recommended sessions\n",
    "    filtered_recommendations = get_session_attributes(filtered_sessions)\n",
    "\n",
    "    return filtered_recommendations, unfiltered_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42756b8-b458-4789-a32c-7a5688a13cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"data/bva/output/df_reg_demo_this.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d172fae-f4e8-4f8e-9751-12c8feccd5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(csv_file_path)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9738030-31fe-4554-ad7a-a16890aac51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_badgeId_this = list(data[\"BadgeId\"].unique())\n",
    "len(list_badgeId_this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a3ecf-b5f0-498e-9bad-8c049b276e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_badgeId_this = list_badgeId_this[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efd9b8a-399f-4c5a-81e7-8d6899157173",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_recommendations, pa_recommendations_full = get_batch_recommendations(\n",
    "    list_badgeId_this,\n",
    "    max_recommendations=10,\n",
    "    num_workers=4,  # Adjust based on your CPU cores\n",
    ")\n",
    "\n",
    "print(f\"Total badges processed: {len(pa_recommendations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e34570-1505-418c-901b-06a2809a15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc02ac-8862-42dc-80cf-71083b3a3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in pa_recommendations.keys():\n",
    "    for ele in pa_recommendations[key]:\n",
    "        ele[\"similarity_score\"] = float(ele[\"similarity_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f47dfc-f840-4a96-a322-ba08f57df995",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for key in pa_recommendations.keys():\n",
    "    if len(pa_recommendations[key]) == 0:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16b76f-4411-4c97-9329-34f766d3c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in pa_recommendations_full.keys():\n",
    "    for ele in pa_recommendations_full[key]:\n",
    "        ele[\"similarity_score\"] = float(ele[\"similarity_score\"])\n",
    "count = 0\n",
    "for key in pa_recommendations_full.keys():\n",
    "    if len(pa_recommendations_full[key]) == 0:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd5d36-63b5-46d8-8f78-733de3a9d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/bva/bva_pa_recomendations.json\", \"w\") as f:\n",
    "    json.dump(pa_recommendations, f, indent=4)\n",
    "with open(\"data/bva/bva_pa_recomendations_full.json\", \"w\") as f:\n",
    "    json.dump(pa_recommendations_full, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ce5d8-f1ab-4de2-8511-583675b73d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_recommendations[\"YCRZ6F4\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71bbf6-f181-40da-92de-a14d2f2dc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_recommendations_to_dataframe(pa_recommendations):\n",
    "    \"\"\"\n",
    "    Transforms a dictionary of session recommendations to a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        pa_recommendations (dict): A dictionary where keys are badge IDs (strings)\n",
    "            and values are lists of session dictionaries.  Each session dictionary\n",
    "            contains information about a recommended session.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with columns 'badgeid', 'session_id', 'stream',\n",
    "            'title', 'end_time', 'synopsis_stripped', 'start_time', 'date',\n",
    "            and 'theatre__name'. Returns an empty DataFrame if the input dictionary is empty.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []  # List to hold the rows of the DataFrame\n",
    "    for badgeid, session_list in pa_recommendations.items():\n",
    "        for session in session_list:\n",
    "            row = {\n",
    "                \"badgeid\": badgeid,\n",
    "                \"session_id\": session.get(\"session_id\", None),  # Use .get() for safety\n",
    "                \"stream\": session.get(\"stream\", None),\n",
    "                \"title\": session.get(\"title\", None),\n",
    "                \"synopsis_stripped\": session.get(\"synopsis_stripped\", None),\n",
    "                \"date\": session.get(\"date\", None),\n",
    "                \"start_time\": session.get(\"start_time\", None),\n",
    "                \"end_time\": session.get(\"end_time\", None),\n",
    "                \"theatre__name\": session.get(\"theatre__name\", None),\n",
    "                \"sponsored_by\": session.get(\"sponsored_by\", None),\n",
    "                \"similarity_score\": session.get(\"similarity_score\", None),\n",
    "                \"recommendation_reason\": session.get(\"recommendation_reason\", None),\n",
    "            }\n",
    "            data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = transform_recommendations_to_dataframe(pa_recommendations)\n",
    "df_full = transform_recommendations_to_dataframe(pa_recommendations_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d32eb0f-308f-43b9-b1ed-aa4f747be1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df), len(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ec320-1c46-41c0-acba-d0e12c12b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def flag_overlapping_sessions(df):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "\n",
    "    # Convert string time columns to datetime\n",
    "    df[\"start_datetime\"] = pd.to_datetime(df[\"date\"] + \" \" + df[\"start_time\"])\n",
    "    df[\"end_datetime\"] = pd.to_datetime(df[\"date\"] + \" \" + df[\"end_time\"])\n",
    "\n",
    "    # Initialize the overlapping_sessions column\n",
    "    df[\"overlapping_sessions\"] = None\n",
    "\n",
    "    # Process each badge group\n",
    "    for badge_id, group in df.groupby(\"badgeid\"):\n",
    "        # Skip if only one session\n",
    "        if len(group) <= 1:\n",
    "            continue\n",
    "\n",
    "        # Get indices in the original dataframe\n",
    "        group_indices = group.index\n",
    "\n",
    "        # For each session in the group\n",
    "        for i, idx in enumerate(group_indices):\n",
    "            # Get current session times\n",
    "            current_start = df.loc[idx, \"start_datetime\"]\n",
    "            current_end = df.loc[idx, \"end_datetime\"]\n",
    "            current_id = df.loc[idx, \"session_id\"]\n",
    "\n",
    "            # Create mask for overlapping sessions (vectorized comparison)\n",
    "            # A session overlaps if it starts before current ends AND ends after current starts\n",
    "            mask = (\n",
    "                (group[\"start_datetime\"] < current_end)\n",
    "                & (group[\"end_datetime\"] > current_start)\n",
    "                & (group[\"session_id\"] != current_id)\n",
    "            )\n",
    "\n",
    "            # Get overlapping session IDs\n",
    "            overlapping_ids = group.loc[mask, \"session_id\"].tolist()\n",
    "\n",
    "            # Update if overlaps found\n",
    "            if overlapping_ids:\n",
    "                df.at[idx, \"overlapping_sessions\"] = \"|\".join(map(str, overlapping_ids))\n",
    "\n",
    "    # Drop the temporary datetime columns\n",
    "    df.drop([\"start_datetime\", \"end_datetime\"], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "df = flag_overlapping_sessions(df)\n",
    "df_full = flag_overlapping_sessions(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d017fac-7de3-451e-bd7e-078ae59df61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.overlapping_sessions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ccd67-3131-41a8-b730-5f6aba8176aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c0537-6754-41f1-a29b-a16fe28ce681",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.recommendation_reason.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881488c7-fb54-4434-847d-9fbd9521ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:100].to_csv(\"data/bva/bva_pa_recomendations_example.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38a0b8-05a0-4f4c-a7bd-ab64523852a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/bva/bva_pa_recomendations.csv\", index=False)\n",
    "df_full.to_csv(\"data/bva/bva_pa_recomendations_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52b28e-a840-4c69-8cf9-f769d4d81c4f",
   "metadata": {},
   "source": [
    "# ADD registration Demo Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd480ae1-baad-4e86-a8d1-37b3abf19659",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd552035-21d2-4e48-9629-32438306c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [\n",
    "    \"Email\",\n",
    "    \"Email_domain\",\n",
    "    \"Company\",\n",
    "    \"JobTitle\",\n",
    "    \"Country\",\n",
    "    \"BadgeType\",\n",
    "    \"ShowRef\",\n",
    "    \"badgeid\",\n",
    "    \"Source\",\n",
    "    \"Days_since_registration\",\n",
    "    \"assist_year_before\",\n",
    "    \"BadgeId_last_year_bva\",\n",
    "    \"BadgeId_last_year_lva\",\n",
    "    \"what_type_does_your_practice_specialise_in\",\n",
    "    \"organisation_type\",\n",
    "    \"job_role\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07523c-08bf-4d0c-9563-4841d097fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = pd.merge(df, data, on=[\"badgeid\"], how=\"left\")\n",
    "data_final_full = pd.merge(df_full, data, on=[\"badgeid\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e322e3-c8d9-4eb3-93c9-35dc2657f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df), len(data_final), len(df_full), len(data_final_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ef460-8895-4621-a208-a2f02d638807",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.to_csv(\"data/bva/bva_pa_recomendations_with_demo.csv\", index=False)\n",
    "data_final_full.to_csv(\"data/bva/bva_pa_recomendations_with_demo_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f8dbb-1682-4335-b398-88e48316131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_final_full[\"badgeid\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d91e54-7a0c-4c94-a573-1bd947b340d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_role\n",
    "data_final_full[\"job_role\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d5713-43eb-4ef2-9088-50450fb6bb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
