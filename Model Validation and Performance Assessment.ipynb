{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56357206-721e-4198-92c4-b956a45ba70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083cfcc8-2c89-4487-bc6b-6a1ab4d75be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(412)\n",
    "\n",
    "# Define date range\n",
    "start_date = datetime(2021, 1, 1)\n",
    "end_date = datetime(2025, 12, 31)\n",
    "\n",
    "# Generate date range with daily frequency\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Generate random sales data\n",
    "# Using a combination of base sales + trend + seasonality + random noise\n",
    "n_days = len(date_range)\n",
    "\n",
    "# Base sales around 1000 units\n",
    "base_sales = 1000\n",
    "\n",
    "# Add trend component (slight upward trend over time)\n",
    "trend = np.linspace(0, 200, n_days)\n",
    "\n",
    "# Add seasonal component (yearly cycle)\n",
    "seasonal = 150 * np.sin(2 * np.pi * np.arange(n_days) / 365.25)\n",
    "\n",
    "# Add weekly pattern (higher sales on weekends)\n",
    "day_of_week = pd.Series(date_range).dt.dayofweek\n",
    "weekly_pattern = np.where(day_of_week.isin([5, 6]), 100, 0)  # Weekend boost\n",
    "\n",
    "# Add random noise\n",
    "noise = np.random.normal(0, 50, n_days)\n",
    "\n",
    "# Combine all components\n",
    "sales = base_sales + trend + seasonal + weekly_pattern + noise\n",
    "\n",
    "# Ensure sales are positive\n",
    "sales = np.maximum(sales, 0)\n",
    "\n",
    "# Round to nearest integer\n",
    "sales = np.round(sales).astype(int)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date': date_range,\n",
    "    'sales': sales\n",
    "})\n",
    "\n",
    "# Display first and last few rows\n",
    "print(\"Dataset created successfully!\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nLast 10 rows:\")\n",
    "print(df.tail(10))\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "print(df['sales'].describe())\n",
    "\n",
    "# Save to CSV file\n",
    "csv_filename = 'sales_data_2023_2025_v2.csv'\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nDataset saved to '{csv_filename}'\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad83c2-0c05-40f8-b782-b4d01996224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Plot 1: Full time series\n",
    "    axes[0].plot(df['date'], df['sales'], linewidth=0.5, alpha=0.7)\n",
    "    axes[0].set_title('Daily Sales Volume (2023-2025)')\n",
    "    axes[0].set_xlabel('Date')\n",
    "    axes[0].set_ylabel('Sales Volume')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Monthly aggregated sales\n",
    "    df_monthly = df.set_index('date').resample('ME')['sales'].sum().reset_index()\n",
    "    axes[1].bar(df_monthly['date'], df_monthly['sales'], width=20, alpha=0.7)\n",
    "    axes[1].set_title('Monthly Sales Volume')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel('Total Monthly Sales')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sales_visualization_v2.png', dpi=100)\n",
    "    plt.show()\n",
    "    print(\"\\nVisualization saved to 'sales_visualization_v2.png'\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\nNote: Install matplotlib for visualization (pip install matplotlib)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd62dc-fb52-44de-b8cd-6cbd48dc277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.prophet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "experiment_name = \"/Users/j.huertas@closerstillmedia.com/prophet\"\n",
    "try:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "except:\n",
    "    print(\"experiment exists\")\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f32bc-9ddf-4d54-b146-4e60dc2c21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prophet_data(data, date_col, value_col, freq=\"D\"):\n",
    "    \"\"\"\n",
    "    Prepare data for Prophet training.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame with time series data\n",
    "        date_col: Name of date column\n",
    "        value_col: Name of value column\n",
    "        freq: Frequency of the time series\n",
    "    \"\"\"\n",
    "\n",
    "    # Prophet requires columns named 'ds' (datestamp) and 'y' (value)\n",
    "    prophet_df = data[[date_col, value_col]].copy()\n",
    "    prophet_df.columns = [\"ds\", \"y\"]\n",
    "\n",
    "    # Ensure ds is datetime\n",
    "    prophet_df[\"ds\"] = pd.to_datetime(prophet_df[\"ds\"])\n",
    "\n",
    "    # Sort by date\n",
    "    prophet_df = prophet_df.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "    # Handle missing dates if needed\n",
    "    if freq:\n",
    "        full_date_range = pd.date_range(\n",
    "            start=prophet_df[\"ds\"].min(), end=prophet_df[\"ds\"].max(), freq=freq\n",
    "        )\n",
    "\n",
    "        # Reindex to fill missing dates\n",
    "        prophet_df = prophet_df.set_index(\"ds\").reindex(full_date_range).reset_index()\n",
    "        prophet_df.columns = [\"ds\", \"y\"]\n",
    "\n",
    "        # Log data quality metrics\n",
    "        missing_dates = prophet_df[\"y\"].isna().sum()\n",
    "        print(f\"Missing dates filled: {missing_dates}\")\n",
    "\n",
    "    return prophet_df\n",
    "\n",
    "    \n",
    "df_prepared = prepare_prophet_data(df, 'date', 'sales', freq='D')\n",
    "df_prepared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f0af2-7c81-421a-a1a6-d00e9d0a944b",
   "metadata": {},
   "source": [
    "# Seasonality and Trend Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9da6b-38c7-42f4-a562-4eeee2939f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_prophet_configuration():\n",
    "    \"\"\"Demonstrate advanced Prophet configuration options.\"\"\"\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Advanced Prophet Configuration\"):\n",
    "        # Create Prophet model with advanced settings\n",
    "        model = Prophet(\n",
    "            # Trend configuration\n",
    "            growth=\"logistic\",  # or 'linear'\n",
    "            changepoints=None,  # Let Prophet auto-detect, or specify dates\n",
    "            n_changepoints=25,  # Number of potential changepoints\n",
    "            changepoint_range=0.8,  # Proportion of history for changepoints\n",
    "            changepoint_prior_scale=0.05,  # Flexibility of trend changes\n",
    "            # Seasonality configuration\n",
    "            yearly_seasonality=\"auto\",  # or True/False/number\n",
    "            weekly_seasonality=\"auto\",\n",
    "            daily_seasonality=\"auto\",\n",
    "            seasonality_mode=\"additive\",  # or 'multiplicative'\n",
    "            seasonality_prior_scale=10,\n",
    "            # Holiday configuration\n",
    "            holidays_prior_scale=10,\n",
    "            # Uncertainty configuration\n",
    "            interval_width=0.80,  # Width of uncertainty intervals\n",
    "            uncertainty_samples=1000,  # Monte Carlo samples for uncertainty\n",
    "            # Stan configuration\n",
    "            mcmc_samples=0,  # Use MAP instead of MCMC\n",
    "            stan_backend=\"CMDSTANPY\",  # Stan backend\n",
    "        )\n",
    "\n",
    "        # For logistic growth, need to specify capacity\n",
    "        if model.growth == \"logistic\":\n",
    "            df_prepared[\"cap\"] = df_prepared[\"y\"].max() * 1.2  # Set capacity 20% above max observed\n",
    "            df_prepared[\"floor\"] = 0  # Optional floor\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(df_prepared)\n",
    "\n",
    "        # Log configuration parameters\n",
    "        config_params = {\n",
    "            \"growth\": model.growth,\n",
    "            \"n_changepoints\": model.n_changepoints,\n",
    "            \"changepoint_range\": model.changepoint_range,\n",
    "            \"seasonality_mode\": model.seasonality_mode,\n",
    "            \"interval_width\": model.interval_width,\n",
    "        }\n",
    "        mlflow.log_params(config_params)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "# Usage\n",
    "advanced_model = advanced_prophet_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9eaee-b68e-4c51-a19e-3bff4050bc45",
   "metadata": {},
   "source": [
    "# Cross-Validation Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519cea5-7f16-4fe5-aa58-df002bdac61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_model_validation(model, df):\n",
    "    \"\"\"Perform comprehensive Prophet model validation.\"\"\"\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Comprehensive Model Validation\"):\n",
    "        # Multiple cross-validation configurations\n",
    "        cv_configs = [\n",
    "            {\n",
    "                \"name\": \"short_horizon\",\n",
    "                \"initial\": \"365 days\",\n",
    "                \"period\": \"90 days\",\n",
    "                \"horizon\": \"90 days\",\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"medium_horizon\",\n",
    "                \"initial\": \"730 days\",\n",
    "                \"period\": \"180 days\",\n",
    "                \"horizon\": \"180 days\",\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"long_horizon\",\n",
    "                \"initial\": \"1095 days\",\n",
    "                \"period\": \"180 days\",\n",
    "                \"horizon\": \"365 days\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        all_metrics = {}\n",
    "\n",
    "        for config in cv_configs:\n",
    "            try:\n",
    "                # Perform cross-validation\n",
    "                cv_results = cross_validation(\n",
    "                    model,\n",
    "                    initial=config[\"initial\"],\n",
    "                    period=config[\"period\"],\n",
    "                    horizon=config[\"horizon\"],\n",
    "                    parallel=\"threads\",\n",
    "                )\n",
    "\n",
    "                # Calculate metrics\n",
    "                metrics = performance_metrics(cv_results)\n",
    "                avg_metrics = metrics[[\"mse\", \"rmse\", \"mae\", \"mape\", \"coverage\"]].mean()\n",
    "\n",
    "                # Store metrics with configuration prefix\n",
    "                for metric, value in avg_metrics.items():\n",
    "                    metric_name = f\"{config['name']}_{metric}\"\n",
    "                    all_metrics[metric_name] = value\n",
    "                    mlflow.log_metric(metric_name, value)\n",
    "\n",
    "                # Log additional statistics\n",
    "                mlflow.log_metrics(\n",
    "                    {\n",
    "                        f\"{config['name']}_cv_folds\": len(cv_results),\n",
    "                        f\"{config['name']}_mape_std\": metrics[\"mape\"].std(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Cross-validation failed for {config['name']}: {e}\")\n",
    "                mlflow.log_param(f\"{config['name']}_error\", str(e))\n",
    "\n",
    "        return all_metrics\n",
    "\n",
    "\n",
    "# Usage\n",
    "validation_metrics = comprehensive_model_validation(advanced_model, df_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e16f98-97af-476a-bfad-bdd376e2c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a0fce-b41c-47b6-9a39-eadc77fb76f2",
   "metadata": {},
   "source": [
    "# Forecast Quality Assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857695df-2123-4ab9-8e2b-5f51fc2408f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! uv pip install seaborn statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6250dfe-8135-458e-8400-1e831ef71f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def analyze_forecast_quality(model, df):\n",
    "    \"\"\"Analyze forecast quality with visualizations.\"\"\"\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Forecast Quality Analysis\"):\n",
    "        # Generate forecast\n",
    "        future = model.make_future_dataframe(periods=365)\n",
    "        if model.growth == \"logistic\":\n",
    "            future[\"cap\"] = df[\"cap\"].iloc[-1]  # Use last known capacity\n",
    "            future[\"floor\"] = df[\"floor\"].iloc[-1] if \"floor\" in df.columns else 0\n",
    "\n",
    "        forecast = model.predict(future)\n",
    "\n",
    "        # Component analysis\n",
    "        fig = model.plot_components(forecast, figsize=(12, 8))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"forecast_components.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        mlflow.log_artifact(\"forecast_components.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Forecast plot\n",
    "        fig = model.plot(forecast, figsize=(12, 6))\n",
    "        plt.title(\"Prophet Forecast\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"forecast_plot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        mlflow.log_artifact(\"forecast_plot.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Residual analysis\n",
    "        # Get historical predictions\n",
    "        historical_forecast = forecast[forecast[\"ds\"] <= df[\"ds\"].max()]\n",
    "        residuals = (\n",
    "            df.set_index(\"ds\")[\"y\"] - historical_forecast.set_index(\"ds\")[\"yhat\"]\n",
    "        )\n",
    "\n",
    "        # Plot residuals\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Residuals over time\n",
    "        axes[0, 0].plot(residuals.index, residuals.values)\n",
    "        axes[0, 0].set_title(\"Residuals Over Time\")\n",
    "        axes[0, 0].set_xlabel(\"Date\")\n",
    "        axes[0, 0].set_ylabel(\"Residual\")\n",
    "\n",
    "        # Residual distribution\n",
    "        axes[0, 1].hist(residuals.values, bins=30, alpha=0.7)\n",
    "        axes[0, 1].set_title(\"Residual Distribution\")\n",
    "        axes[0, 1].set_xlabel(\"Residual\")\n",
    "        axes[0, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "        # Q-Q plot\n",
    "        from scipy import stats\n",
    "        from statsmodels.stats import diagnostic as diag\n",
    "\n",
    "        stats.probplot(residuals.values, dist=\"norm\", plot=axes[1, 0])\n",
    "        axes[1, 0].set_title(\"Q-Q Plot\")\n",
    "\n",
    "        # Residuals vs fitted\n",
    "        axes[1, 1].scatter(historical_forecast[\"yhat\"], residuals.values, alpha=0.6)\n",
    "        axes[1, 1].set_title(\"Residuals vs Fitted\")\n",
    "        axes[1, 1].set_xlabel(\"Fitted Values\")\n",
    "        axes[1, 1].set_ylabel(\"Residuals\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"residual_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        mlflow.log_artifact(\"residual_analysis.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Calculate residual statistics\n",
    "        residual_stats = {\n",
    "            \"residual_mean\": residuals.mean(),\n",
    "            \"residual_std\": residuals.std(),\n",
    "            \"residual_skewness\": stats.skew(residuals.values),\n",
    "            \"residual_kurtosis\": stats.kurtosis(residuals.values),\n",
    "            \"ljung_box_pvalue\": diag.acorr_ljungbox(\n",
    "                residuals.values, lags=10, return_df=True\n",
    "            )[\"lb_pvalue\"].iloc[-1],\n",
    "        }\n",
    "\n",
    "        mlflow.log_metrics(residual_stats)\n",
    "\n",
    "        return forecast, residual_stats\n",
    "\n",
    "\n",
    "# Usage\n",
    "forecast_analysis, residual_stats = analyze_forecast_quality(advanced_model, df_prepared)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
