{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53790fd-ba26-4471-9f74-caa54cb2a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    %pip install -q \"numpy<2.0.0\"\n",
    "\n",
    "if not Path(\"notebook_utils.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    "    )\n",
    "    open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "# Read more about telemetry collection at https://github.com/openvinotoolkit/openvino_notebooks?tab=readme-ov-file#-telemetry\n",
    "from notebook_utils import collect_telemetry\n",
    "\n",
    "collect_telemetry(\"distil-whisper-asr.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ed1a1-ea9e-47d9-88bd-d0ff068924b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "model_ids = {\n",
    "    \"Distil-Whisper\": [\n",
    "        \"distil-whisper/distil-large-v2\",\n",
    "        \"distil-whisper/distil-large-v3\",\n",
    "        \"distil-whisper/distil-medium.en\",\n",
    "        \"distil-whisper/distil-small.en\",\n",
    "    ],\n",
    "    \"Whisper\": [\n",
    "        \"openai/whisper-large-v3-turbo\",\n",
    "        \"openai/whisper-large-v3\",\n",
    "        \"openai/whisper-large-v2\",\n",
    "        \"openai/whisper-large\",\n",
    "        \"openai/whisper-medium\",\n",
    "        \"openai/whisper-small\",\n",
    "        \"openai/whisper-base\",\n",
    "        \"openai/whisper-tiny\",\n",
    "        \"openai/whisper-medium.en\",\n",
    "        \"openai/whisper-small.en\",\n",
    "        \"openai/whisper-base.en\",\n",
    "        \"openai/whisper-tiny.en\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "model_type = widgets.Dropdown(\n",
    "    options=model_ids.keys(),\n",
    "    value=\"Distil-Whisper\",\n",
    "    description=\"Model type:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7806f-347a-4bd7-8de9-923afd43f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = widgets.Dropdown(\n",
    "    options=model_ids[model_type.value],\n",
    "    value=model_ids[model_type.value][0],\n",
    "    description=\"Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147b028-04a2-4e36-ae15-5bc551ac7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c388de-e533-4edc-8e69-122e1264bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id.value)\n",
    "\n",
    "pt_model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id.value)\n",
    "pt_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad51ba2-0e8e-4015-9815-a65a2fc0ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def extract_input_features(sample):\n",
    "    input_features = processor(\n",
    "        sample[\"audio\"][\"array\"],\n",
    "        sampling_rate=sample[\"audio\"][\"sampling_rate\"],\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_features\n",
    "    return input_features\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
    "sample = dataset[0]\n",
    "input_features = extract_input_features(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911c918-82a6-4b2f-a7d2-65e6f81f7726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "predicted_ids = pt_model.generate(input_features)\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "display(ipd.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"]))\n",
    "print(f\"Reference: {sample['text']}\")\n",
    "print(f\"Result: {transcription[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ce311-9b8c-4b21-be28-053a52ccc09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoModelForSpeechSeq2Seq\n",
    "from optimum.intel.openvino import OVModelForSpeechSeq2Seq\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_id = 'distil-whisper/distil-small.en'\n",
    "#-model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)\n",
    "model = OVModelForSpeechSeq2Seq.from_pretrained(model_id, export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92853e89-dcf4-40f4-8fc7-4cedb3729073",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc77c37-7e18-4e6b-9407-de62871ffa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from optimum.intel.openvino import OVModelForSpeechSeq2Seq\n",
    "\n",
    "model_path = Path(model_id.replace(\"/\", \"_\"))\n",
    "ov_config = {\"CACHE_DIR\": \"\"}\n",
    "\n",
    "if not model_path.exists():\n",
    "    ov_model = OVModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id,\n",
    "        ov_config=ov_config,\n",
    "        export=True,\n",
    "        compile=False,\n",
    "        load_in_8bit=False,\n",
    "    )\n",
    "    ov_model.half()\n",
    "    ov_model.save_pretrained(model_path)\n",
    "else:\n",
    "    ov_model = OVModelForSpeechSeq2Seq.from_pretrained(model_path, ov_config=ov_config, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47d1be-c40e-4cb2-b50e-f218c67d8264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "device = device_widget()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d66425-71e8-4f0c-80e0-273225d1ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model.to(device.value)\n",
    "ov_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb05a81-27b1-4969-824c-47c465eb34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids = ov_model.generate(input_features)\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "display(ipd.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"]))\n",
    "print(f\"Reference: {sample['text']}\")\n",
    "print(f\"Result: {transcription[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b756c-87be-4445-a5ad-764d6d941a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def measure_perf(model, sample, n=10):\n",
    "    timers = []\n",
    "    input_features = extract_input_features(sample)\n",
    "    for _ in tqdm(range(n), desc=\"Measuring performance\"):\n",
    "        start = time.perf_counter()\n",
    "        model.generate(input_features)\n",
    "        end = time.perf_counter()\n",
    "        timers.append(end - start)\n",
    "    return np.median(timers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02321f-aa53-42b0-8653-ea9e5606308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_torch = measure_perf(pt_model, sample)\n",
    "perf_ov = measure_perf(ov_model, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304c49a-7ac9-463c-8503-987f7e3abf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean torch {model_id} generation time: {perf_torch:.3f}s\")\n",
    "print(f\"Mean openvino {model_id} generation time: {perf_ov:.3f}s\")\n",
    "print(f\"Performance {model_id} openvino speedup: {perf_torch / perf_ov:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4afaa7-0700-4058-a2a0-af600602ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "ov_model.generation_config = pt_model.generation_config\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=ov_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=15,\n",
    "    batch_size=16,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78edb3b-dbf2-40bd-987d-e5f34fda64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
    "sample_long = dataset[0]\n",
    "\n",
    "\n",
    "def format_timestamp(seconds: float):\n",
    "    \"\"\"\n",
    "    format time in srt-file expected format\n",
    "    \"\"\"\n",
    "    assert seconds >= 0, \"non-negative timestamp expected\"\n",
    "    milliseconds = round(seconds * 1000.0)\n",
    "\n",
    "    hours = milliseconds // 3_600_000\n",
    "    milliseconds -= hours * 3_600_000\n",
    "\n",
    "    minutes = milliseconds // 60_000\n",
    "    milliseconds -= minutes * 60_000\n",
    "\n",
    "    seconds = milliseconds // 1_000\n",
    "    milliseconds -= seconds * 1_000\n",
    "\n",
    "    return (f\"{hours}:\" if hours > 0 else \"00:\") + f\"{minutes:02d}:{seconds:02d},{milliseconds:03d}\"\n",
    "\n",
    "\n",
    "def prepare_srt(transcription):\n",
    "    \"\"\"\n",
    "    Format transcription into srt file format\n",
    "    \"\"\"\n",
    "    segment_lines = []\n",
    "    for idx, segment in enumerate(transcription[\"chunks\"]):\n",
    "        segment_lines.append(str(idx + 1) + \"\\n\")\n",
    "        timestamps = segment[\"timestamp\"]\n",
    "        time_start = format_timestamp(timestamps[0])\n",
    "        time_end = format_timestamp(timestamps[1])\n",
    "        time_str = f\"{time_start} --> {time_end}\\n\"\n",
    "        segment_lines.append(time_str)\n",
    "        segment_lines.append(segment[\"text\"] + \"\\n\\n\")\n",
    "    return segment_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc80438-5c26-4b43-9fb6-a4294c0584bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe(sample_long[\"audio\"].copy(), return_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45decd5-6675-4674-a0a7-00cbc4663b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "srt_lines = prepare_srt(result)\n",
    "\n",
    "display(ipd.Audio(sample_long[\"audio\"][\"array\"], rate=sample_long[\"audio\"][\"sampling_rate\"]))\n",
    "print(\"\".join(srt_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf99ef9-e8a9-42a6-9eba-6cd26e2f0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import quantization_widget\n",
    "\n",
    "to_quantize = quantization_widget()\n",
    "\n",
    "to_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc45dc-dd3b-4dcc-b6e4-f35fbdefe4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch `skip_kernel_extension` module\n",
    "import requests\n",
    "\n",
    "if not Path(\"skip_kernel_extension.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/skip_kernel_extension.py\",\n",
    "    )\n",
    "    open(\"skip_kernel_extension.py\", \"w\").write(r.text)\n",
    "\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f87f9-e8b7-4572-b2fb-051d5381826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "from itertools import islice\n",
    "from optimum.intel.openvino.quantization import InferRequestWrapper\n",
    "\n",
    "\n",
    "def collect_calibration_dataset(ov_model: OVModelForSpeechSeq2Seq, calibration_dataset_size: int):\n",
    "    # Overwrite model request properties, saving the original ones for restoring later\n",
    "    encoder_calibration_data = []\n",
    "    decoder_calibration_data = []\n",
    "    ov_model.encoder.request = InferRequestWrapper(ov_model.encoder.request, encoder_calibration_data, apply_caching=True)\n",
    "    ov_model.decoder.request = InferRequestWrapper(ov_model.decoder.request,\n",
    "                                                             decoder_calibration_data,\n",
    "                                                             apply_caching=True)\n",
    "\n",
    "    try:\n",
    "        calibration_dataset = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=\"validation\", streaming=True, trust_remote_code=True)\n",
    "        for sample in tqdm(islice(calibration_dataset, calibration_dataset_size), desc=\"Collecting calibration data\",\n",
    "                           total=calibration_dataset_size):\n",
    "            input_features = extract_input_features(sample)\n",
    "            ov_model.generate(input_features)\n",
    "    finally:\n",
    "        ov_model.encoder.request = ov_model.encoder.request.request\n",
    "        ov_model.decoder.request = ov_model.decoder.request.request\n",
    "\n",
    "    return encoder_calibration_data, decoder_calibration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b135a-ed68-43b2-9b54-56973cd5d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "import gc\n",
    "import shutil\n",
    "import nncf\n",
    "import openvino as ov\n",
    "\n",
    "CALIBRATION_DATASET_SIZE = 50\n",
    "quantized_model_path = Path(f\"{model_path}_quantized\")\n",
    "\n",
    "\n",
    "def quantize(ov_model: OVModelForSpeechSeq2Seq, calibration_dataset_size: int):\n",
    "    if not quantized_model_path.exists():\n",
    "        encoder_calibration_data, decoder_calibration_data = collect_calibration_dataset(\n",
    "            ov_model, calibration_dataset_size\n",
    "        )\n",
    "        print(\"Quantizing encoder\")\n",
    "        quantized_encoder = nncf.quantize(\n",
    "            ov_model.encoder.model,\n",
    "            nncf.Dataset(encoder_calibration_data),\n",
    "            subset_size=len(encoder_calibration_data),\n",
    "            model_type=nncf.ModelType.TRANSFORMER,\n",
    "            # Smooth Quant algorithm reduces activation quantization error; optimal alpha value was obtained through grid search\n",
    "            advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=0.50)\n",
    "        )\n",
    "        ov.save_model(quantized_encoder, quantized_model_path / \"openvino_encoder_model.xml\")\n",
    "        del quantized_encoder\n",
    "        del encoder_calibration_data\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Quantizing decoder with past\")\n",
    "        quantized_decoder = nncf.quantize(\n",
    "            ov_model.decoder.model,\n",
    "            nncf.Dataset(decoder_calibration_data),\n",
    "            subset_size=len(decoder_calibration_data),\n",
    "            model_type=nncf.ModelType.TRANSFORMER,\n",
    "            # Smooth Quant algorithm reduces activation quantization error; optimal alpha value was obtained through grid search\n",
    "            advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=0.95)\n",
    "        )\n",
    "        ov.save_model(quantized_decoder, quantized_model_path / \"openvino_decoder_model.xml\")\n",
    "        del quantized_decoder\n",
    "        del decoder_calibration_data\n",
    "        gc.collect()\n",
    "\n",
    "        # Copy the config file and the first-step-decoder manually\n",
    "        shutil.copy(model_path / \"config.json\", quantized_model_path / \"config.json\")\n",
    "\n",
    "    quantized_ov_model = OVModelForSpeechSeq2Seq.from_pretrained(quantized_model_path, ov_config=ov_config, compile=False)\n",
    "    quantized_ov_model.to(device.value)\n",
    "    quantized_ov_model.compile()\n",
    "    return quantized_ov_model\n",
    "\n",
    "\n",
    "ov_quantized_model = quantize(ov_model, CALIBRATION_DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda54f2-69e3-421c-91f4-050a302baf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\", trust_remote_code=True\n",
    ")\n",
    "sample = dataset[0]\n",
    "input_features = extract_input_features(sample)\n",
    "\n",
    "predicted_ids = ov_model.generate(input_features)\n",
    "transcription_original = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "predicted_ids = ov_quantized_model.generate(input_features)\n",
    "transcription_quantized = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "display(ipd.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"]))\n",
    "print(f\"Original : {transcription_original[0]}\")\n",
    "print(f\"Quantized: {transcription_quantized[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc9eee4-598f-450c-91de-fc5389a441ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import time\n",
    "# from contextlib import contextmanager\n",
    "# from jiwer import wer, wer_standardize\n",
    "\n",
    "\n",
    "# TEST_DATASET_SIZE = 50\n",
    "# MEASURE_TIME = False\n",
    "\n",
    "# @contextmanager\n",
    "# def time_measurement():\n",
    "#     global MEASURE_TIME\n",
    "#     try:\n",
    "#         MEASURE_TIME = True\n",
    "#         yield\n",
    "#     finally:\n",
    "#         MEASURE_TIME = False\n",
    "\n",
    "# def time_fn(obj, fn_name, time_list):\n",
    "#     original_fn = getattr(obj, fn_name)\n",
    "\n",
    "#     def wrapper(*args, **kwargs):\n",
    "#         if not MEASURE_TIME:\n",
    "#             return original_fn(\\*args, \\*\\*kwargs)\n",
    "#         start_time = time.perf_counter()\n",
    "#         result = original_fn(\\*args, \\*\\*kwargs)\n",
    "#         end_time = time.perf_counter()\n",
    "#         time_list.append(end_time - start_time)\n",
    "#         return result\n",
    "\n",
    "#     setattr(obj, fn_name, wrapper)\n",
    "\n",
    "# def calculate_transcription_time_and_accuracy(ov_model, test_samples):\n",
    "#     encoder_infer_times = []\n",
    "#     decoder_with_past_infer_times = []\n",
    "#     whole_infer_times = []\n",
    "#     time_fn(ov_model, \"generate\", whole_infer_times)\n",
    "#     time_fn(ov_model.encoder, \"forward\", encoder_infer_times)\n",
    "#     time_fn(ov_model.decoder, \"forward\", decoder_with_past_infer_times)\n",
    "\n",
    "#     ground_truths = []\n",
    "#     predictions = []\n",
    "#     for data_item in tqdm(test_samples, desc=\"Measuring performance and accuracy\"):\n",
    "#         input_features = extract_input_features(data_item)\n",
    "\n",
    "#         with time_measurement():\n",
    "#             predicted_ids = ov_model.generate(input_features)\n",
    "#         transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "#         ground_truths.append(data_item[\"text\"])\n",
    "#         predictions.append(transcription[0])\n",
    "\n",
    "#     word_accuracy = (1 - wer(ground_truths, predictions, reference_transform=wer_standardize,\n",
    "#                              hypothesis_transform=wer_standardize)) * 100\n",
    "#     mean_whole_infer_time = sum(whole_infer_times)\n",
    "#     mean_encoder_infer_time = sum(encoder_infer_times)\n",
    "#     mean_decoder_with_time_infer_time = sum(decoder_with_past_infer_times)\n",
    "#     return word_accuracy, (mean_whole_infer_time, mean_encoder_infer_time, mean_decoder_with_time_infer_time)\n",
    "\n",
    "# test_dataset = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=\"test\", streaming=True, trust_remote_code=True)\n",
    "# test_dataset = test_dataset.shuffle(seed=42).take(TEST_DATASET_SIZE)\n",
    "# test_samples = [sample for sample in test_dataset]\n",
    "\n",
    "# accuracy_original, times_original = calculate_transcription_time_and_accuracy(ov_model, test_samples)\n",
    "# accuracy_quantized, times_quantized = calculate_transcription_time_and_accuracy(ov_quantized_model, test_samples)\n",
    "# print(f\"Encoder performance speedup: {times_original[1] / times_quantized[1]:.3f}\")\n",
    "# print(f\"Decoder with past performance speedup: {times_original[2] / times_quantized[2]:.3f}\")\n",
    "# print(f\"Whole pipeline performance speedup: {times_original[0] / times_quantized[0]:.3f}\")\n",
    "# print(f\"Whisper transcription word accuracy. Original model: {accuracy_original:.2f}%. Quantized model: {accuracy_quantized:.2f}%.\")\n",
    "# print(f\"Accuracy drop: {accuracy_original - accuracy_quantized:.2f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f610481-97cc-4dbe-abfc-d8c96918d654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (whispher)",
   "language": "python",
   "name": "whispher"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
