{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8f70b75-7af4-480a-abcb-5db3b9418f01",
   "metadata": {},
   "source": [
    "# Best Practices in Feature Engineering for Tabular Data With GPU Acceleration #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8159b9b8-466f-4b68-a27e-685d8fbbcf0a",
   "metadata": {},
   "source": [
    "## Part 3: Train Models with GPUs, cuDF, and cuML ##\n",
    "Training models is the process of learning patterns from train data features to predict targets. Previously we learned how feature engineering transforms raw features into new columns to help our models recognize patterns. Now we will learn how to train models quickly allowing us to evaluate different feature engineering experiments and build the most accurate models.\n",
    "\n",
    "The secret to building the most accurate models is to evaluate as many experiments as possible and keep the features, models, ideas that perform the best. To run the most experiments we need to run everything as fast as possible with GPUs.\n",
    "\n",
    "**Table of Contents**\n",
    "<br>\n",
    "In this notebook, we will use the speed of GPUs to help us evaluate which feature engineering creates the most accurate model. Specifically we will use `RAPIDS cuDF` for fast feature engineering and `XGBoost on GPU` for fast model training, and `cuML` which accelerates `Scikit-Learn` with `GPU` acceleration for fast model training. This notebook covers the below sections: \n",
    "\n",
    "1. [XGBoost with Categorical Feature Engineering](#XGBoost-with-Categorical-Feature-Engineering)\n",
    "    * [Load Data with cuDF](#Load-Data-with-cuDF)\n",
    "    * [EDA and Find Categorical Columns](#EDA-and-Find-Categorical-Columns)\n",
    "    * [XGBoost with Built-In Enable Categorical](#XGBoost-with-Built-In-Enable-Categorical)\n",
    "    * [XGBoost with Label Encoding](#XGBoost-with-Label-Encoding)\n",
    "    * [XGBoost with Target Encoding](#XGBoost-with-Target-Encoding)\n",
    "2. [XGB Feature Importance](#XGB-Feature-Importance)\n",
    "    * [Summary of XGB Experiments](#Summary-of-XGB-Experiments)\n",
    "    * [CPU-GPU Comparison for XGB](#CPU-GPU-Comparison-for-XGB)\n",
    "3. [SVC](#SVC)\n",
    "    * [SVC with Target Encoding](#SVC-with-Target-Encoding)\n",
    "    * [SVC with TF-IDF Feature Engineering](#SVC-with-TF-IDF-Feature-Engineering)\n",
    "    * [CPU-GPU Comparison for SVC](#CPU-GPU-Comparison-for-SVC)\n",
    "4. [Summary of Experiments](#Summary-of-Experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ff1f9-f127-4f1b-847c-b8691c6146ad",
   "metadata": {},
   "source": [
    "## XGBoost with Categorical Feature Engineering\n",
    "[XGBoost](https://xgboost.readthedocs.io/en/stable/tutorials/model.html) uses gradient boosted decision trees to build accurate models quickly for tabular datasets. To optimize model acccuray, we must experiment using different feature engineering. Let's explore the effects of different categorical column encodings and demonstrate the process of improving model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a32e8",
   "metadata": {
    "papermill": {
     "duration": 0.008221,
     "end_time": "2025-01-16T17:22:02.455961",
     "exception": false,
     "start_time": "2025-01-16T17:22:02.447740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load Data with cuDF\n",
    "For this course, we are using the **Amazon product data dataset**. \n",
    "\n",
    "**Description**<br>\n",
    "This dataset contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014.\n",
    "\n",
    "This dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).\n",
    "\n",
    "**Citation**<br>\n",
    "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering\n",
    "R. He, J. McAuley\n",
    "WWW, 2016\n",
    "[pdf](http://cseweb.ucsd.edu/~jmcauley/pdfs/www16a.pdf)\n",
    "\n",
    "Image-based recommendations on styles and substitutes\n",
    "J. McAuley, C. Targett, J. Shi, A. van den Hengel\n",
    "SIGIR, 2015\n",
    "[pdf](http://cseweb.ucsd.edu/~jmcauley/pdfs/sigir15.pdf)\n",
    "\n",
    "We load our dataframes on the GPU using RAPIDS cuDF for fast processing and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba42037",
   "metadata": {
    "papermill": {
     "duration": 7.699154,
     "end_time": "2025-01-16T17:22:10.163782",
     "exception": false,
     "start_time": "2025-01-16T17:22:02.464628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import pandas as pd, numpy as np\n",
    "import cudf, cupy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LOAD DATA\n",
    "PATH = \"./data/\"\n",
    "train0 = cudf.read_parquet(f'{PATH}train.parquet').reset_index(drop=True)\n",
    "valid0 = cudf.read_parquet(f'{PATH}valid.parquet').reset_index(drop=True)\n",
    "test0 = cudf.read_parquet(f'{PATH}test.parquet').reset_index(drop=True)\n",
    "\n",
    "# FEATURES NOT USED\n",
    "not_used = ['timestamp']\n",
    "train = train0.drop(not_used,axis=1)\n",
    "test = test0.drop(not_used,axis=1)\n",
    "valid = valid0.drop(not_used,axis=1)\n",
    "\n",
    "print(\"Train data shape, valid data shape, test data shape:\")\n",
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6103f73",
   "metadata": {
    "papermill": {
     "duration": 0.008608,
     "end_time": "2025-01-16T17:22:10.181764",
     "exception": false,
     "start_time": "2025-01-16T17:22:10.173156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### EDA and Find Categorical Columns\n",
    "When training models with XGBoost, an effective type of feature engineering is encoding the categorical columns. We learned two techniques `Target Encoding` and `Count Encoding` in our previous tutorials. Popular encoding techniques are:\n",
    "* Built-In Encoding\n",
    "* One Hot Encoding\n",
    "* Label Encoding\n",
    "* Target Encoding\n",
    "* Count Encoding\n",
    "* Other Encodings (like tfidf)\n",
    "\n",
    "Let's identify the categorical columns and then experiment using some different techniques to discover which produces the most accurate for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e47e62",
   "metadata": {
    "papermill": {
     "duration": 0.014941,
     "end_time": "2025-01-16T17:22:10.205830",
     "exception": false,
     "start_time": "2025-01-16T17:22:10.190889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"ALL COLUMNS:\")\n",
    "print( list(train.columns) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c0050",
   "metadata": {
    "papermill": {
     "duration": 0.127412,
     "end_time": "2025-01-16T17:22:10.341835",
     "exception": false,
     "start_time": "2025-01-16T17:22:10.214423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"CATEGORICAL COLUMNS:\")\n",
    "CAT_COLS = []\n",
    "for c in train.columns:\n",
    "    if train[c].dtype==\"object\":\n",
    "        u = train[c].nunique()\n",
    "        n = 100 * train[c].isna().mean()\n",
    "        print(f\"{c} (is categorical) with number unique = {u}, with nan = {n:.1f}%\")\n",
    "        CAT_COLS.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab5c576",
   "metadata": {
    "papermill": {
     "duration": 0.009137,
     "end_time": "2025-01-16T17:22:10.360288",
     "exception": false,
     "start_time": "2025-01-16T17:22:10.351151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### XGBoost with Built-In Enable Categorical\n",
    "Most GBDT (gradient boosted decision trees, i.e. XGB, LGB, CAT) have built-in algorithms for dealing with categorical columns (read more [here](https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html)). We will use XGBoost's build-in categorical handling to train a model. To use XGB built-in, we convert category columns to dtype `category` and use flag `enable_categorical=True`. Additionally we can use parameter `min_child_weight` to prevent overfitting if necessary.\n",
    "\n",
    "We observe on this particular dataset, the built-in categorical handling achieves a good train AUC score but doesn't generalize well to valid and test data well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57da79c",
   "metadata": {
    "papermill": {
     "duration": 1.053193,
     "end_time": "2025-01-16T17:22:11.422673",
     "exception": false,
     "start_time": "2025-01-16T17:22:10.369480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings, xgboost as xgb\n",
    "print(\"XGBoost version\",xgb.__version__)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0510d20",
   "metadata": {
    "papermill": {
     "duration": 0.385848,
     "end_time": "2025-01-16T17:22:11.817794",
     "exception": false,
     "start_time": "2025-01-16T17:22:11.431946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MARK CATEGORICAL COLUMNS AS CATEGORY FOR XGBOOST BUILT-IN\n",
    "for c in train.columns:\n",
    "    if c in CAT_COLS:\n",
    "        train[c] = train[c].astype('category')\n",
    "        test[c] = test[c].astype('category')\n",
    "        valid[c] = valid[c].astype('category')\n",
    "    else:\n",
    "        train[c] = train[c].astype('float32')\n",
    "        test[c] = test[c].astype('float32')\n",
    "        valid[c] = valid[c].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382bdfcd",
   "metadata": {
    "papermill": {
     "duration": 0.014321,
     "end_time": "2025-01-16T17:22:11.841464",
     "exception": false,
     "start_time": "2025-01-16T17:22:11.827143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XGBOOST PARAMETERS\n",
    "params = {\n",
    "        'objective': 'binary:logistic', \n",
    "        'learning_rate':0.1,\n",
    "        'tree_method': 'hist', \n",
    "        'max_depth': 7, \n",
    "        'subsample':0.8,\n",
    "        'eval_metric': 'auc',\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight':1,\n",
    "        'device':'cuda',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038dd28",
   "metadata": {
    "papermill": {
     "duration": 4.647392,
     "end_time": "2025-01-16T17:22:16.497887",
     "exception": false,
     "start_time": "2025-01-16T17:22:11.850495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# TRAIN XGBOOST\n",
    "dtrain = xgb.DMatrix(data=train.drop('label',axis=1), label=train['label'], enable_categorical=True)\n",
    "dvalid = xgb.DMatrix(data=valid.drop('label',axis=1), label=valid['label'], enable_categorical=True)\n",
    "dtest = xgb.DMatrix(data=test.drop('label',axis=1), enable_categorical=True)\n",
    "watchlist = [(dtrain, 'train'),(dvalid, 'eval')]\n",
    "clf = xgb.train(params, dtrain=dtrain,\n",
    "                num_boost_round=2000,evals=watchlist,\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cebc41",
   "metadata": {
    "papermill": {
     "duration": 2.320632,
     "end_time": "2025-01-16T17:22:18.827861",
     "exception": false,
     "start_time": "2025-01-16T17:22:16.507229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import roc_auc_score\n",
    "from cuml.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae62638",
   "metadata": {
    "papermill": {
     "duration": 9.611447,
     "end_time": "2025-01-16T17:22:28.448692",
     "exception": false,
     "start_time": "2025-01-16T17:22:18.837245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# COMPUTE VALIDATION SCORES\n",
    "yp = clf.predict(dvalid)\n",
    "valid_auc_XE = roc_auc_score(valid['label'],yp)\n",
    "yp = clf.predict(dtest)\n",
    "test_auc_XE = roc_auc_score(test['label'],yp)\n",
    "print(f\"Valid AUC = {valid_auc_XE:.3f}, Test AUC = {test_auc_XE:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d9e32a",
   "metadata": {
    "papermill": {
     "duration": 0.008731,
     "end_time": "2025-01-16T17:22:28.467078",
     "exception": false,
     "start_time": "2025-01-16T17:22:28.458347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### XGBoost with Label Encoding\n",
    "We will now convert the categorical strings to numbers using basic `Label Encoding`. We will fill NAN with a new categorical value of `NONE`. Doing this instead of imputing an existing category will allow XGBoost to decide how to use NAN. \n",
    "\n",
    "One advantage of basic `Label Encoding` is that it overfits the train data less (than other encodings) because all the categorical values are mixed together with their random number label encoded values. We observe that using `Label Encoding` achieves similar train AUC score as `Built-In Encoding` but generalizes better to valid and achieves a better valid AUC score and test AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e64c0ea",
   "metadata": {
    "papermill": {
     "duration": 0.013598,
     "end_time": "2025-01-16T17:22:28.489549",
     "exception": false,
     "start_time": "2025-01-16T17:22:28.475951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import LabelEncoder\n",
    "from cuml.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c7ebdf",
   "metadata": {
    "papermill": {
     "duration": 0.701045,
     "end_time": "2025-01-16T17:22:29.199382",
     "exception": false,
     "start_time": "2025-01-16T17:22:28.498337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LABEL ENCODING\n",
    "print(\"Processing: \",end=\"\")\n",
    "for c in CAT_COLS:\n",
    "    print(f\"{c}, \",end=\"\")\n",
    "\n",
    "    # CONVERT CATEOGORY COLUMN BACK TO OBJECT\n",
    "    train[c] = train[c].astype('object')\n",
    "    test[c] = test[c].astype('object')\n",
    "    valid[c] = valid[c].astype('object')\n",
    "\n",
    "    # IMPUTE NAN WITH NEW CATEGORY\n",
    "    train[c] = train[c].fillna('NONE') \n",
    "    test[c] = test[c].fillna('NONE')\n",
    "    valid[c] = valid[c].fillna('NONE')\n",
    "\n",
    "    # FIT LABEL ENCODER\n",
    "    values = cudf.concat([train[c],test[c],valid[c]])\n",
    "    LE = LabelEncoder()\n",
    "    LE.fit( values )\n",
    "\n",
    "    # LE TRANSFORM TRAIN, VALID, TEST\n",
    "    train[c] = LE.transform(train[c]).astype(\"float32\")\n",
    "    test[c] = LE.transform(test[c]).astype(\"float32\")\n",
    "    valid[c] = LE.transform(valid[c]).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f4bf6",
   "metadata": {
    "papermill": {
     "duration": 8.014652,
     "end_time": "2025-01-16T17:22:37.223520",
     "exception": false,
     "start_time": "2025-01-16T17:22:29.208868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# TRAIN XGBOOST\n",
    "\n",
    "dtrain = xgb.DMatrix(data=train.drop('label',axis=1), label=train['label'])\n",
    "dvalid = xgb.DMatrix(data=valid.drop('label',axis=1), label=valid['label'])\n",
    "dtest = xgb.DMatrix(data=test.drop('label',axis=1))\n",
    "watchlist = [(dtrain, 'train'),(dvalid, 'eval')]\n",
    "clf = xgb.train(params, dtrain=dtrain,\n",
    "                num_boost_round=2000,evals=watchlist,\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748a93a",
   "metadata": {
    "papermill": {
     "duration": 0.042669,
     "end_time": "2025-01-16T17:22:37.276871",
     "exception": false,
     "start_time": "2025-01-16T17:22:37.234202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# COMPUTE VALIDATION SCORES\n",
    "yp = clf.predict(dvalid)\n",
    "valid_auc_LE = roc_auc_score(valid['label'],yp)\n",
    "yp = clf.predict(dtest)\n",
    "test_auc_LE = roc_auc_score(test['label'],yp)\n",
    "print(f\"Valid AUC = {valid_auc_LE:.3f}, Test AUC = {test_auc_LE:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103079b",
   "metadata": {
    "papermill": {
     "duration": 0.009112,
     "end_time": "2025-01-16T17:22:37.295363",
     "exception": false,
     "start_time": "2025-01-16T17:22:37.286251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### XGBoost with Target Encoding\n",
    "We will now add new columns of `Target Encoding`. We have a choice to keep the existing `Label Encoding` and add a new `Target Encoding` column, or we can replace `Label Encoding` columns with `Target Encoding` columns. In this notebook, we will add new columns. We `Target Encode` with `smoothing=10` and `kfold=5`. We will encode the target's `mean` but we can compute other statistics too like `median`, `max`, `min`, etc. We wrote code in our previous tutorial but for conveinence, we will use code from `cuML` implementation of `Target Encoding`.\n",
    "\n",
    "We observe that `Target Encoding` improves the valid AUC score and test AUC score. Woohoo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3a708",
   "metadata": {
    "papermill": {
     "duration": 0.014164,
     "end_time": "2025-01-16T17:22:37.318919",
     "exception": false,
     "start_time": "2025-01-16T17:22:37.304755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import TargetEncoder\n",
    "from cuml.preprocessing import TargetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7039209",
   "metadata": {
    "papermill": {
     "duration": 2.578404,
     "end_time": "2025-01-16T17:22:39.906594",
     "exception": false,
     "start_time": "2025-01-16T17:22:37.328190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TARGET ENCODE WITH SMOOTHING=10, KFOLD=5, STAT=MEAN\n",
    "print(\"Processing: \",end=\"\")\n",
    "for c in CAT_COLS:\n",
    "    print(f\"{c}, \",end=\"\")\n",
    "\n",
    "    # ADD TARGET ENCODE FEATURES\n",
    "    TE = TargetEncoder(smooth=10, n_folds=5, stat=\"mean\")\n",
    "    train[f\"TE_{c}\"] = TE.fit_transform(train[c],train['label']).astype(\"float32\")\n",
    "    test[f\"TE_{c}\"] = TE.transform(test[c]).astype(\"float32\")\n",
    "    valid[f\"TE_{c}\"] = TE.transform(valid[c]).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579b6c4",
   "metadata": {
    "papermill": {
     "duration": 4.442311,
     "end_time": "2025-01-16T17:22:44.360335",
     "exception": false,
     "start_time": "2025-01-16T17:22:39.918024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# TRAIN XGBOOST GPU\n",
    "dtrain = xgb.DMatrix(data=train.drop('label',axis=1), label=train['label'])\n",
    "dvalid = xgb.DMatrix(data=valid.drop('label',axis=1), label=valid['label'])\n",
    "dtest = xgb.DMatrix(data=test.drop('label',axis=1))\n",
    "watchlist = [(dtrain, 'train'),(dvalid, 'eval')]\n",
    "clf = xgb.train(params, dtrain=dtrain,\n",
    "                num_boost_round=2000,evals=watchlist,\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0662725",
   "metadata": {
    "papermill": {
     "duration": 0.035779,
     "end_time": "2025-01-16T17:22:44.407739",
     "exception": false,
     "start_time": "2025-01-16T17:22:44.371960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# COMPUTE VALIDATION SCORES\n",
    "yp_te_v = clf.predict(dvalid)\n",
    "valid_auc_TE = roc_auc_score(valid['label'],yp_te_v)\n",
    "yp_te_t = clf.predict(dtest)\n",
    "test_auc_TE = roc_auc_score(test['label'],yp_te_t)\n",
    "print(f\"Valid AUC = {valid_auc_TE:.3f}, Test AUC = {test_auc_TE:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113d789",
   "metadata": {
    "papermill": {
     "duration": 0.009744,
     "end_time": "2025-01-16T17:22:44.427482",
     "exception": false,
     "start_time": "2025-01-16T17:22:44.417738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## XGB Feature Importance\n",
    "We can use XGBoost feature importance to see which features are the most helpful. We see that the most helpful features are our `Target Encode` features. In particular `TE_userID` and `TE_productID` are the two strongest features. The model learns that some users and some products are more likely to have positive targets. And our model uses this knowledge to predict future users and products accurately. Note the `Label Encoded` columns of `userID` and `productID` are not as helpful as the `Target Encoded` versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e83cd1",
   "metadata": {
    "papermill": {
     "duration": 0.414905,
     "end_time": "2025-01-16T17:22:44.852204",
     "exception": false,
     "start_time": "2025-01-16T17:22:44.437299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLOT TOP 25 FEATURES BY IMPORTANCE\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 5))  # Adjust the figure size if needed\n",
    "xgb.plot_importance(\n",
    "    clf,\n",
    "    ax=ax,\n",
    "    max_num_features=25,  # Display only the top 25 features\n",
    "    importance_type=\"weight\",  # Options: 'weight', 'gain', 'cover', 'total_gain', 'total_cover'\n",
    ")\n",
    "plt.title(\"XGB Top 25 Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe14af",
   "metadata": {
    "papermill": {
     "duration": 0.010953,
     "end_time": "2025-01-16T17:22:44.874777",
     "exception": false,
     "start_time": "2025-01-16T17:22:44.863824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Summary of XGB Experiments\n",
    "Let's pause and compare our three XGB experiments side by side. We see that using `Label Encoding` and `Target Encoding` worked best for this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1438b14",
   "metadata": {
    "papermill": {
     "duration": 0.223826,
     "end_time": "2025-01-16T17:22:45.109707",
     "exception": false,
     "start_time": "2025-01-16T17:22:44.885881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3), dpi=180)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "x = ['valid_auc_XE', 'test_auc_XE',\n",
    "     'valid_auc_LE', 'test_auc_LE',\n",
    "     'valid_auc_TE', 'test_auc_TE']\n",
    "\n",
    "y = [valid_auc_XE, test_auc_XE,\n",
    "     valid_auc_LE, test_auc_LE,\n",
    "     valid_auc_TE, test_auc_TE]\n",
    "\n",
    "ax.bar(x, y, color=['g', 'g',  # XE pair\n",
    "                    'b', 'b',  # LE pair\n",
    "                    'r', 'r'])  # TE pair\n",
    "\n",
    "ax.set_title('AUC higher is better')\n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3a024",
   "metadata": {
    "papermill": {
     "duration": 0.012628,
     "end_time": "2025-01-16T17:22:45.134725",
     "exception": false,
     "start_time": "2025-01-16T17:22:45.122097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CPU-GPU Comparison for XGB\n",
    "Let's compare the runtime between CPU XGBoost and GPU XGBoost. To toggle between CPU and GPU, we change the parameter `device` from `cude` to `cpu`. We observe that GPU XGBoost is roughly 10x faster than CPU XGBoost. Above GPU took about 1 second and below CPU takes about 10 seconds for this small dataset. This means that we can explore 10x more feature engineering in the same amount of time using GPU versus CPU. (And when training models with bigger datasets, the ratio of speed up will be even larger because GPUs are most efficient when doing lots of work and processing lots of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2b7a3",
   "metadata": {
    "papermill": {
     "duration": 0.018361,
     "end_time": "2025-01-16T17:22:45.165547",
     "exception": false,
     "start_time": "2025-01-16T17:22:45.147186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "params['device'] = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca75a98",
   "metadata": {
    "papermill": {
     "duration": 31.819195,
     "end_time": "2025-01-16T17:23:16.997096",
     "exception": false,
     "start_time": "2025-01-16T17:22:45.177901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# TRAIN XGBOOST CPU\n",
    "dtrain = xgb.DMatrix(data=train.drop('label',axis=1), label=train['label'])\n",
    "dvalid = xgb.DMatrix(data=valid.drop('label',axis=1), label=valid['label'])\n",
    "dtest = xgb.DMatrix(data=test.drop('label',axis=1))\n",
    "watchlist = [(dtrain, 'train'),(dvalid, 'eval')]\n",
    "clf = xgb.train(params, dtrain=dtrain,\n",
    "                num_boost_round=2000,evals=watchlist,\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c993267-deb1-4dd7-8953-110e1b994cce",
   "metadata": {},
   "source": [
    "## SVC\n",
    "We will now explore the performance of a different model. We will investigate Support Vector Machines (SVM). SVM will find a decision boundary to separate the two classes of target. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff7ab8",
   "metadata": {
    "papermill": {
     "duration": 0.011887,
     "end_time": "2025-01-16T17:23:17.021221",
     "exception": false,
     "start_time": "2025-01-16T17:23:17.009334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SVC with Target Encoding\n",
    "Unlike GBDT, SVM do not have built-in categorical support. Therefore we need to transform all categorical features with an encoding. Let's try using `Target Encoding`. Also SVM prefer numerical inputs to have `mean=0` and `std=1` per column, so lets use `StandardScaler` to normalize all numeric columns.\n",
    "\n",
    "In order to train SVC in a resonable amount of time, we will need to use GPU, specifically **RAPIDS cuML Support Vector Machine Classifier** [(SVC)](https://docs.rapids.ai/api/cuml/stable/api/). Inspired by Scikit-learn’s implementation, the SVC classifier in cuML is designed to be a drop-in replacement for scikit-learn’s SVC module. Also we will train SVC with a subset of the train data which allows us to perform more experiments quickly.\n",
    "\n",
    "\n",
    "\n",
    "Image source: https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e6737",
   "metadata": {
    "papermill": {
     "duration": 0.017066,
     "end_time": "2025-01-16T17:23:17.050416",
     "exception": false,
     "start_time": "2025-01-16T17:23:17.033350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from sklearn.svm import SVC\n",
    "from cuml.svm import SVC\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from cuml.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd471fd",
   "metadata": {
    "papermill": {
     "duration": 6.674543,
     "end_time": "2025-01-16T17:23:23.737240",
     "exception": false,
     "start_time": "2025-01-16T17:23:17.062697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Processing: \",end=\"\")\n",
    "for c in train.columns:\n",
    "    if c==\"label\": continue\n",
    "    print(f\"{c}, \",end=\"\")\n",
    "\n",
    "    # STANDARD SCALER FIT TRANSFORM\n",
    "    SS = StandardScaler().fit(train[[c]])\n",
    "    train[c] = SS.transform( train[[c]] )\n",
    "    test[c] = SS.transform( test[[c]] )\n",
    "    valid[c] = SS.transform( valid[[c]] )\n",
    "\n",
    "    # IMPUTE NAN WITH MEAN\n",
    "    train[c] = train[c].fillna(0).astype('float32')\n",
    "    test[c] = test[c].fillna(0).astype('float32')\n",
    "    valid[c] = valid[c].fillna(0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10a50d",
   "metadata": {
    "papermill": {
     "duration": 3.697622,
     "end_time": "2025-01-16T17:23:27.448281",
     "exception": false,
     "start_time": "2025-01-16T17:23:23.750659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SUBSAMPLE TRAIN\n",
    "import numpy as np\n",
    "np.random.seed(42) \n",
    "idx = np.random.randint(0,len(train),15*1024)\n",
    "\n",
    "# TRAIN SVC\n",
    "clf = SVC(C=1,probability=True,kernel=\"rbf\",cache_size=8*1024)\n",
    "_ = clf.fit( train.drop([\"label\"],axis=1).iloc[idx], train[\"label\"].iloc[idx] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bcc401",
   "metadata": {
    "papermill": {
     "duration": 1.585557,
     "end_time": "2025-01-16T17:23:29.048414",
     "exception": false,
     "start_time": "2025-01-16T17:23:27.462857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# COMPUTE VALIDATION SCORES\n",
    "yp_svc = clf.predict_proba(valid.drop([\"label\"],axis=1))\n",
    "valid_auc_SVC = roc_auc_score(valid['label'],yp_svc[1].to_numpy())\n",
    "yp = clf.predict_proba(test.drop([\"label\"],axis=1))\n",
    "test_auc_SVC = roc_auc_score(test['label'],yp[1].to_numpy())\n",
    "print(f\"Valid AUC {valid_auc_SVC:.3f}, Test AUC {test_auc_SVC:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0998d0",
   "metadata": {
    "papermill": {
     "duration": 0.013932,
     "end_time": "2025-01-16T17:23:29.075985",
     "exception": false,
     "start_time": "2025-01-16T17:23:29.062053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SVC with TF-IDF Feature Engineering\n",
    "We will now create new columns of feature engineering. We will combine the columns `['brand','cat_0','cat_1','cat_2','cat3']` into a single product string. Then we will use `TF-IDF` to transform this string into features for our model. We use [cuML TfidfVectorizer](https://github.com/rapidsai/cuml/blob/branch-25.04/python/cuml/cuml/feature_extraction/_tfidf_vectorizer.py) method, largely based on scikit-learn's TfIdfVectorizer code. In order to speed up experiments, we fit transform `TF-IDF` with GPU specifically `RAPIDS cuML TF-IDF`. These new features imporove our valid AUC score and test AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22269237",
   "metadata": {
    "papermill": {
     "duration": 0.017532,
     "end_time": "2025-01-16T17:23:29.106267",
     "exception": false,
     "start_time": "2025-01-16T17:23:29.088735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from cuml.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f04905e",
   "metadata": {
    "papermill": {
     "duration": 0.113036,
     "end_time": "2025-01-16T17:23:29.231826",
     "exception": false,
     "start_time": "2025-01-16T17:23:29.118790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FEATURE ENGINEER - COMBINE COLUMNS\n",
    "train_product = (train0[\"brand\"].fillna(\"\") + \" \" + train0[\"cat_0\"] + \" \" + \n",
    "                         train0[\"cat_1\"] + \" \" + train0[\"cat_2\"] + \" \" + train0[\"cat_3\"])\n",
    "valid_product = (valid0[\"brand\"].fillna(\"\") + \" \" + valid0[\"cat_0\"] + \" \" + \n",
    "                         valid0[\"cat_1\"] + \" \" + valid0[\"cat_2\"] + \" \" + valid0[\"cat_3\"])\n",
    "test_product = (test0[\"brand\"].fillna(\"\") + \" \" + test0[\"cat_0\"] + \" \" + \n",
    "                         test0[\"cat_1\"] + \" \" + test0[\"cat_2\"] + \" \" + test0[\"cat_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654cee26",
   "metadata": {
    "papermill": {
     "duration": 8.231653,
     "end_time": "2025-01-16T17:23:37.476372",
     "exception": false,
     "start_time": "2025-01-16T17:23:29.244719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FEATURE ENGINEER WITH TFIDF\n",
    "from cupy import hstack\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=10) \n",
    "tfidf.fit(train_product)\n",
    "Xtrain1 = train.drop([\"label\"],axis=1).iloc[idx].values\n",
    "Xtrain2 = tfidf.transform(train_product.iloc[idx]).todense()\n",
    "Xtrain = hstack([Xtrain1,Xtrain2])\n",
    "print(\"Train data shape:\")\n",
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63327b4c",
   "metadata": {
    "papermill": {
     "duration": 14.592751,
     "end_time": "2025-01-16T17:23:52.082653",
     "exception": false,
     "start_time": "2025-01-16T17:23:37.489902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# TRAIN SVC GPU\n",
    "clf = SVC(C=1,probability=True,kernel=\"rbf\",cache_size=8*1024)\n",
    "_ = clf.fit( Xtrain, train[\"label\"].iloc[idx].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0248d0a3",
   "metadata": {
    "papermill": {
     "duration": 16.928064,
     "end_time": "2025-01-16T17:24:09.024233",
     "exception": false,
     "start_time": "2025-01-16T17:23:52.096169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# COMPUTE VALIDATION SCORES\n",
    "Xvalid1 = valid.drop([\"label\"],axis=1).values\n",
    "Xvalid2 = tfidf.transform(valid_product).todense()\n",
    "Xvalid = hstack([Xvalid1,Xvalid2])\n",
    "yp = clf.predict_proba(Xvalid)\n",
    "valid_auc_SVC_tfidf = roc_auc_score(valid['label'],yp[:,1].get())\n",
    "\n",
    "Xtest1 = test.drop([\"label\"],axis=1).values\n",
    "Xtest2 = tfidf.transform(test_product).todense()\n",
    "Xtest = hstack([Xtest1,Xtest2])\n",
    "yp = clf.predict_proba(Xtest)\n",
    "test_auc_SVC_tfidf = roc_auc_score(test['label'],yp[:,1].get())\n",
    "\n",
    "print(f\"Valid AUC {valid_auc_SVC_tfidf:.3f}, Test AUC {test_auc_SVC_tfidf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dfff44",
   "metadata": {
    "papermill": {
     "duration": 0.013578,
     "end_time": "2025-01-16T17:24:09.355285",
     "exception": false,
     "start_time": "2025-01-16T17:24:09.341707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CPU-GPU Comparison for SVC\n",
    "Using GPU versus CPU is 100x or more times faster. The SVC above on GPU trained in a few seconds. The SVC on CPU below takes 1+ hours to train. Wow! \n",
    "The cell is commented out, we will not run it now, but you can test it yourself later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797a4ef",
   "metadata": {
    "papermill": {
     "duration": 0.018703,
     "end_time": "2025-01-16T17:24:09.388083",
     "exception": false,
     "start_time": "2025-01-16T17:24:09.369380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SVC_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0ea12",
   "metadata": {
    "papermill": {
     "duration": 0.018274,
     "end_time": "2025-01-16T17:24:09.420265",
     "exception": false,
     "start_time": "2025-01-16T17:24:09.401991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# TRAIN SVC CPU\n",
    "clf = SVC_cpu(C=1,probability=True,kernel=\"rbf\",cache_size=8*1024)\n",
    "_ = clf.fit( Xtrain.get(), train[\"label\"].iloc[idx].values.get() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d3738",
   "metadata": {
    "papermill": {
     "duration": 0.012734,
     "end_time": "2025-01-16T17:24:09.050495",
     "exception": false,
     "start_time": "2025-01-16T17:24:09.037761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary of Experiments\n",
    "In this notebook we observed the typical model building strategy. We try different models and we try different feature engineering ideas and keep what works best for our particular dataset. We can also try ensembling our different models and we can try doing more feature engineering. Performing fast experiments is key to finding the most accurate model quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab6b9c7",
   "metadata": {
    "papermill": {
     "duration": 0.264061,
     "end_time": "2025-01-16T17:24:09.327193",
     "exception": false,
     "start_time": "2025-01-16T17:24:09.063132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3), dpi=180)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "x = ['valid_auc_XE', 'test_auc_XE',\n",
    "     'valid_auc_LE', 'test_auc_LE',\n",
    "     'valid_auc_TE', 'test_auc_TE',\n",
    "     'valid_auc_SVC_TE', 'test_auc_SVC_TE',\n",
    "     'valid_auc_SVC_tfidf', 'test_auc_SVC_tfidf',]\n",
    "\n",
    "y = [valid_auc_XE, test_auc_XE,\n",
    "     valid_auc_LE, test_auc_LE,\n",
    "     valid_auc_TE, test_auc_TE,\n",
    "     valid_auc_SVC, test_auc_SVC,\n",
    "     valid_auc_SVC_tfidf, test_auc_SVC_tfidf]\n",
    "\n",
    "ax.bar(x, y, color=['g','g','b','b','r','r',\n",
    "                    'y','y','m','m'])\n",
    "\n",
    "ax.set_title('AUC higher is better')\n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7afb661-87bc-463b-b2dd-b832a4d8376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6332640,
     "sourceId": 10240283,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 132.827137,
   "end_time": "2025-01-16T17:24:12.670435",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-16T17:21:59.843298",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
