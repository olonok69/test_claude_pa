{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ljvDxQGWqll"
      },
      "source": [
        "# EMBEDDINGS\n",
        "```\n",
        "clustering_config:\n",
        "  # Huggingface access token\n",
        "  access_token: \"hb-token\"\n",
        "  # Huggingface Model Hub repo id:\n",
        "  model_name: \"mistralai/Mistral-7B-v0.1\" #\"meta-llama/Meta-Llama-3-8B\"\n",
        "  # \"sentence-transformers/stsb-bert-large\" #\"mistralai/Mistral-7B-v0.1\" #\"mistralai/Mixtral-8x7B-v0.1\" # \"mistralai/Mistral-7B-v0.1\"\n",
        "  # options: llama.cpp, gptq, transformers\n",
        "  backend_type: \"transformers\" # transformers, ctransformers\n",
        "\n",
        "  session_data_path: \"/home/samtukra/LLMU/saved_jsons/new_recommendation_db/show_ref_agg/sessions/speaker_aggregated_info.json\"\n",
        "  # \"/home/samtukra/LLMU/saved_jsons/new_claire_db/badge_id_aggregated_results.json\"\n",
        "  #\"/home/samtukra/LLMU/saved_jsons/badge_id_all_data_aggregated_iter_1000.json\"\n",
        "\n",
        "  clustering_algorithm: \"kmeans\" # kmeans, agglomerative, dbscan\n",
        "  nomenclature_file: \"/home/samtukra/LLMU/configs/clustering/cluster_numeculature.json\" # None or path to nomenclature file\n",
        "  nomenclature_embeddings_path: \"/home/samtukra/LLMU/embeddings/old/cluster_numeculature_embeddings_Mistral-7B-v0.1.json\"\n",
        "  \n",
        "  # output file dir\n",
        "  embeddings_root: \"/home/samtukra/LLMU/embeddings/new_recommendation_db\"\n",
        "  \n",
        "  default_output: \"json\" # csv, json\n",
        "  additional_output: \"csv\" # csv, json\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LArcujZnCODC",
        "outputId": "736561cd-ad43-48aa-87ca-ff86fb60725d"
      },
      "outputs": [],
      "source": [
        "! pip install -U bitsandbytes Faker -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHvpd1WOBY08"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import json\n",
        "import gc\n",
        "import datetime\n",
        "from faker import Faker\n",
        "\n",
        "fake = Faker()\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import json\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23waq3YwWzFl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "class BadgeDataset(Dataset):\n",
        "    def __init__(self, csv_file, json_file, split, new_db=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (str): Path to the CSV file with badge and cluster info.\n",
        "            json_file (str): Path to the JSON file with badge embeddings.\n",
        "        \"\"\"\n",
        "        initial_data = pd.read_csv(csv_file)\n",
        "\n",
        "        with open(json_file, \"r\") as f:\n",
        "            self.embeddings = json.load(f)\n",
        "\n",
        "        # Mapping cluster names to labels\n",
        "        if new_db != None:\n",
        "            # New db doesn't have the 6th class.\n",
        "            self.cluster_to_label = {\n",
        "                \"Networking\": 0,\n",
        "                \"Learning\": 1,\n",
        "                \"Searching\": 2,\n",
        "                \"Sourcing: Early\": 3,\n",
        "                \"Sourcing: In Process\": 4,\n",
        "            }\n",
        "\n",
        "            print(\"using the new cluster to label dict, with ':'\")\n",
        "        else:\n",
        "            self.cluster_to_label = {\n",
        "                \"Networking\": 0,\n",
        "                \"Learning\": 1,\n",
        "                \"Searching\": 2,\n",
        "                \"Sourcing – Early\": 3,\n",
        "                \"Sourcing – In Process\": 4,\n",
        "                \"Sourcing – Deciding\": 5,\n",
        "            }\n",
        "            print(\"using the original cluster to label dict, with '-'\")\n",
        "\n",
        "        # Check that all BadgeIds in CSV have corresponding embeddings in the JSON\n",
        "        self.data = initial_data[\n",
        "            initial_data[\"BadgeId\"].apply(lambda x: f\"BadgeId_{x}\" in self.embeddings)\n",
        "        ]\n",
        "\n",
        "        if split == \"train\":\n",
        "            pass\n",
        "        elif split == \"val\":\n",
        "            pass\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        badge_id = self.data.iloc[idx][\"BadgeId\"]\n",
        "        cluster_name = self.data.iloc[idx][\"ClusterId\"]\n",
        "\n",
        "        # Convert the embedding list (the first element of the list under each BadgeId) to a tensor\n",
        "        embedding = torch.tensor(\n",
        "            self.embeddings[\"BadgeId_{}\".format(badge_id)][0], dtype=torch.float32\n",
        "        )\n",
        "        # Get the label for the cluster\n",
        "        label = self.cluster_to_label[cluster_name]\n",
        "\n",
        "        # Convert label to tensor\n",
        "        label = torch.tensor(label, dtype=torch.int64)\n",
        "\n",
        "        return embedding, label\n",
        "\n",
        "    def split_data(self, split):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnKoPMGUj9Ge"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        ")\n",
        "\n",
        "\n",
        "# Custom Loss function\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n",
        "        \"\"\"\n",
        "        Initializes the focal loss function.\n",
        "\n",
        "        Parameters:\n",
        "            alpha (float): Balancing factor, default is 0.25.\n",
        "            gamma (float): Focusing parameter, default is 2.0.\n",
        "            reduction (str): Specifies the reduction to apply to the output: 'none', 'mean', 'sum'.\n",
        "        \"\"\"\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        Forward pass for the focal loss calculation.\n",
        "\n",
        "        Parameters:\n",
        "            inputs (tensor): Logits as predicted by the model.\n",
        "            targets (tensor): True labels.\n",
        "        \"\"\"\n",
        "        BCE_loss = F.cross_entropy(inputs, targets, reduction=\"none\")\n",
        "        pt = torch.exp(-BCE_loss)  # Prevents nans when probability is 0\n",
        "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduction == \"mean\":\n",
        "            return torch.mean(F_loss)\n",
        "        elif self.reduction == \"sum\":\n",
        "            return torch.sum(F_loss)\n",
        "        else:\n",
        "            return F_loss\n",
        "\n",
        "\n",
        "# Model\n",
        "class BadgeNet(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(BadgeNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 64)\n",
        "        self.fc4 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def save_model(model, path):\n",
        "    torch.save(model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtakG_4sMKZG",
        "outputId": "75f8ce94-d5a3-4fe5-eef6-b5b0115056e0"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5-yj4eOQLSX"
      },
      "outputs": [],
      "source": [
        "session_data_path = \"/content/drive/MyDrive/data/CSM/speaker_aggregated_info.json\"\n",
        "nomenclature_embeddings_path = (\n",
        "    \"/content/drive/MyDrive/data/CSM/cluster_numeculature.json\"\n",
        ")\n",
        "output_path = \"/content/drive/MyDrive/data/CSM/embeddings_test_mistral.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S4eCMxKXVU9"
      },
      "outputs": [],
      "source": [
        "with open(session_data_path) as f:\n",
        "    session_data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lxU47SJXa0W",
        "outputId": "35b0de60-166a-4091-af3a-3d82eed0e770"
      },
      "outputs": [],
      "source": [
        "session_data[\"SessionInfo\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib6FQR8Sc1Cp",
        "outputId": "1c0f3ead-8a99-434e-bbce-b3c0aa274729"
      },
      "outputs": [],
      "source": [
        "session_data[\"SessionInfo\"][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lMIFvjIXj7x",
        "outputId": "dbad21bc-2bcd-4358-913f-4baefe2c0bb7"
      },
      "outputs": [],
      "source": [
        "len(session_data[\"SessionInfo\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqtL136YYCZk"
      },
      "outputs": [],
      "source": [
        "embeddings_mistral_path = \"/content/drive/MyDrive/data/CSM/embeddings_test_mistral.json\"\n",
        "\n",
        "with open(embeddings_mistral_path) as f:\n",
        "    embeddings_mistral = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMzLKWphLkFo",
        "outputId": "51581258-5cc2-41db-842b-f7c19d048b57"
      },
      "outputs": [],
      "source": [
        "np.squeeze(embeddings_mistral[\"SessionInfo_37780\"]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmVD2dqxL9oS"
      },
      "outputs": [],
      "source": [
        "embeddings_nomic_path = \"/content/drive/MyDrive/data/CSM/embeddings_test_nomic.json\"\n",
        "\n",
        "with open(embeddings_nomic_path) as f:\n",
        "    embeddings_nomic = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtQfqbJ6ME_E",
        "outputId": "eb1ddb14-536a-4ed6-8063-43e0e56a628a"
      },
      "outputs": [],
      "source": [
        "np.squeeze(embeddings_nomic[\"SessionInfo_37780\"]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFcZbrAZBw_-"
      },
      "outputs": [],
      "source": [
        "csv_path = \"/content/drive/MyDrive/data/CSM/new_claire_db_badge_cluster_data_with_aggregated_info_GIO_GT_LABELS.csv\"\n",
        "csv_path = \"/content/drive/MyDrive/data/CSM/20240512_new_labels_WITH_AGGINFO.csv\"\n",
        "# Update with actual path\n",
        "json_path = \"/content/drive/MyDrive/data/CSM/session_embeddings_stsb-bert-large.json\"  # Update with actual path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RBE_FnfBCDvd",
        "outputId": "4fc6f2e2-6023-4065-9803-e3f560acf682"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(csv_path)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WjnhTfT-M1V"
      },
      "outputs": [],
      "source": [
        "with open(json_path) as f:\n",
        "    embeddings = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZchndRgcIra",
        "outputId": "76bc7947-1490-4be2-f1b7-7c32cb4dcd23"
      },
      "outputs": [],
      "source": [
        "len(embeddings.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX99AgG3b699",
        "outputId": "29c90ad8-e24d-4f28-d95b-d16011cdd074"
      },
      "outputs": [],
      "source": [
        "len(embeddings[\"BadgeId_Z9ZXS8W\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3B4wtVmdBRP",
        "outputId": "d9887f91-7b26-4efe-bd0f-11d16ec0fecd"
      },
      "outputs": [],
      "source": [
        "badge_dataset = BadgeDataset(csv_path, json_path, split=\"train\")\n",
        "print(\"dataset size: {}\".format(len(badge_dataset)))\n",
        "# Create the DataLoader\n",
        "badge_loader = DataLoader(badge_dataset, batch_size=10, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln3aBVUadIK0",
        "outputId": "14df5193-7678-4560-a528-5791ac517d26"
      },
      "outputs": [],
      "source": [
        "# initialise dataset:\n",
        "train_db = BadgeDataset(csv_path, json_path, split=\"train\", new_db=True)\n",
        "val_db = BadgeDataset(csv_path, json_path, split=\"val\", new_db=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvCAAsSCtWpF"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "num_workers = 2\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS9g31f0MOyt"
      },
      "outputs": [],
      "source": [
        "trainloader = DataLoader(\n",
        "    train_db,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=False,\n",
        ")\n",
        "valloader = DataLoader(\n",
        "    val_db,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PurWraJVd77s",
        "outputId": "cced75c4-4192-4604-a702-522890e1793a"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"total training samples #: {}, total val samples #: {}\".format(\n",
        "        len(train_db), len(val_db)\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hol5wtrDeAUp"
      },
      "outputs": [],
      "source": [
        "# initialise model:\n",
        "input_size = 1024  # Change this to the size of your embeddings\n",
        "num_classes = 5\n",
        "model = BadgeNet(input_size, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGM5iM3POsfO"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "class Args(BaseModel):\n",
        "    ckpt_dir: str\n",
        "    model_name: str\n",
        "    epochs: int\n",
        "    loss_function: str = \"cross_entropy\"\n",
        "\n",
        "\n",
        "args = Args(\n",
        "    ckpt_dir=\"/content/drive/MyDrive/data/CSM/cpkts\", model_name=\"llama3\", epochs=400\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "komjmmL7OzvF"
      },
      "outputs": [],
      "source": [
        "# initialise optimiser and loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "best_accuracy = 0.0\n",
        "model = model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BFplHfs0fWk"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByzLUmT6HWiu",
        "outputId": "458823f6-7f11-41df-cdd3-23603d699eea"
      },
      "outputs": [],
      "source": [
        "for epoch in tqdm(range(args.epochs)):\n",
        "    running_loss = 0.0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    for inputs, labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(trainloader.dataset)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
        "    recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{args.epochs}\")\n",
        "    print(f\"Loss: {epoch_loss:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\n",
        "        classification_report(\n",
        "            all_labels,\n",
        "            all_preds,\n",
        "            target_names=[\n",
        "                \"Networking\",\n",
        "                \"Learning\",\n",
        "                \"Searching\",\n",
        "                \"Sourcing: Early\",\n",
        "                \"Sourcing: In Process\",\n",
        "            ],\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        model_path = \"{}/{}/loss_{}_data_with_fluff_acc_{}_epoch_{}.pth\".format(\n",
        "            args.ckpt_dir, args.model_name, args.loss_function, accuracy, epoch\n",
        "        )\n",
        "        save_model(model, model_path)\n",
        "        print(f\"Model improved to {accuracy:.4f} accuracy. Saving model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "k8t8gRZdGdjK",
        "outputId": "3b883c65-8b12-4d2d-8354-07dcd66d7321"
      },
      "outputs": [],
      "source": [
        "model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vngfMc-69LTz"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnIo7RBAILuU"
      },
      "outputs": [],
      "source": [
        "def predict_classes(model, embeddings_json, output_csv):\n",
        "    # Load the embeddings from JSON file\n",
        "    with open(embeddings_json, \"r\") as file:\n",
        "        embeddings = json.load(file)\n",
        "\n",
        "    # Prepare for predictions\n",
        "    class_labels = [\n",
        "        \"Networking\",\n",
        "        \"Learning\",\n",
        "        \"Searching\",\n",
        "        \"Sourcing: Early\",\n",
        "        \"Sourcing: In Process\",\n",
        "    ]\n",
        "    predictions = []\n",
        "    class_counts = defaultdict(int)  # Dictionary to count class occurrences\n",
        "\n",
        "    # Predict each embedding\n",
        "    for badge_id, embedding in tqdm(embeddings.items()):\n",
        "        embedding_tensor = torch.tensor(embedding[0], dtype=torch.float32)\n",
        "        embedding_tensor = embedding_tensor.unsqueeze(0)  # Add batch dimension\n",
        "        with torch.no_grad():\n",
        "            output = model(embedding_tensor)\n",
        "            predicted_class = torch.argmax(output, dim=1)\n",
        "            badge_id = badge_id.split(\"_\")[1]  # Extract part after '_'\n",
        "            predictions.append((badge_id, class_labels[predicted_class.item()]))\n",
        "            class_counts[\n",
        "                class_labels[predicted_class.item()]\n",
        "            ] += 1  # Increment count for the predicted class\n",
        "\n",
        "    # Write predictions to a CSV file\n",
        "    with open(output_csv, \"w\", newline=\"\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"BadgeId\", \"ClusterId\"])\n",
        "        writer.writerows(predictions)\n",
        "\n",
        "    # Print class counts\n",
        "    print(\"Total number of samples predicted for each class:\")\n",
        "    for class_label, count in class_counts.items():\n",
        "        print(f\"{class_label}: {count}\")\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def load_model(path, input_size, num_classes):\n",
        "    model = BadgeNet(input_size, num_classes)\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFIjcH0OIc5X",
        "outputId": "cb3190b9-9e69-4586-9914-535b826e66ef"
      },
      "outputs": [],
      "source": [
        "input_size = 1024  # The size of your embeddings\n",
        "num_classes = 5\n",
        "model = load_model(model_path, input_size, num_classes)\n",
        "\n",
        "print(f\"Model loaded successfully. from path: {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKZJnWlRIoDN",
        "outputId": "50f2f683-4465-4e54-8a0e-f17c0f6d22b5"
      },
      "outputs": [],
      "source": [
        "# Predict and write the class labels to CSV\n",
        "\n",
        "embeddings_json = \"/home/samtukra/LLMU/embeddings/new_claire_db/fluff/session_embeddings_Meta-Llama-3-8B.json\"  # Update this path\n",
        "# EMbeddings SBERT\n",
        "embeddings_json = (\n",
        "    \"/content/drive/MyDrive/data/CSM/session_embeddings_stsb-bert-large.json\"\n",
        ")\n",
        "output_csv = \"/content/drive/MyDrive/data/CSM/predictions/new_20250124_sbert_loss_cross_entropy.csv\"  # Specify your output CSV file path\n",
        "predict_classes(model, embeddings_json, output_csv)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPPw5FjFubOQ2HYB+cjmzPI",
      "include_colab_link": true,
      "machine_shape": "hm",
      "mount_file_id": "1ViWE5EPMPIXsjaNARNKeJhlnnURlWq1s",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
