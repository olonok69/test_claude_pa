{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94cbf18e-ca39-4e56-a676-7fa4bd787f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/3039328626124250', creation_time=1755624146266, experiment_id='3039328626124250', last_update_time=1755874967812, lifecycle_stage='active', name='/Users/j.huertas@closerstillmedia.com/prophet', tags={'mlflow.databricks.filesystem.experiment_permissions_check': 'test',\n",
       " 'mlflow.experiment.sourceName': '/Users/j.huertas@closerstillmedia.com/prophet',\n",
       " 'mlflow.experimentKind': 'custom_model_development',\n",
       " 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n",
       " 'mlflow.ownerEmail': 'b.relf@closerstillmedia.com',\n",
       " 'mlflow.ownerId': '7931383772120950'}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.prophet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "experiment_name = \"/Users/j.huertas@closerstillmedia.com/prophet\"\n",
    "try:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "except:\n",
    "    print(\"experiment exists\")\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59661626-c8bb-4258-b8b2-f5ac16474a60",
   "metadata": {},
   "source": [
    "# Forecast Accuracy Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7328887d-034b-4333-be9a-501abe6b547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals_df = pd.read_csv(\"wp_log_peyton_manning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e2c5abb-dbf2-428f-ba0d-35d6887ce8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-01-20'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actuals_df[\"ds\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ca8c88-6ec1-4085-a1d2-fda57a29282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals_df[\"ds\"]=pd.to_datetime(actuals_df[\"ds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf17812c-5c2f-4d21-95a1-4a1bf719dcda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ds', 'y'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actuals_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c88ae30b-e250-4fbc-86a7-d5bf33d7e720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f393d48fde04034b2be6fa7179694c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2854bb4186184685893729f72e78914f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:05:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:17 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run Forecast Accuracy Monitoring at: https://adb-3191716161457605.5.azuredatabricks.net/ml/experiments/3039328626124250/runs/779ed5196b034107b11c9feb71c2e020\n",
      "ðŸ§ª View experiment at: https://adb-3191716161457605.5.azuredatabricks.net/ml/experiments/3039328626124250\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "run_id = \"1655d93c38a4421cb6aa50a49c28e127\"\n",
    "model_uri = f\"runs:/{run_id}/best_model\"\n",
    "\n",
    "def monitor_forecast_accuracy(model_uri, actuals_df, prediction_horizon_days=30):\n",
    "    \"\"\"Monitor Prophet model accuracy against actual values.\"\"\"\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Forecast Accuracy Monitoring\"):\n",
    "        # Load model\n",
    "        model = mlflow.prophet.load_model(model_uri)\n",
    "\n",
    "        # Generate historical predictions for comparison\n",
    "        cutoff_date = actuals_df[\"ds\"].max() - pd.Timedelta(\n",
    "            days=prediction_horizon_days\n",
    "        )\n",
    "        historical_data = actuals_df[actuals_df[\"ds\"] <= cutoff_date]\n",
    "\n",
    "        # Refit model on historical data\n",
    "        temp_model = Prophet()\n",
    "        temp_model.fit(historical_data)\n",
    "\n",
    "        # Generate predictions for the monitoring period\n",
    "        future = temp_model.make_future_dataframe(periods=prediction_horizon_days)\n",
    "        if temp_model.growth == \"logistic\":\n",
    "            future[\"cap\"] = (\n",
    "                historical_data[\"cap\"].iloc[-1]\n",
    "                if \"cap\" in historical_data.columns\n",
    "                else 10000\n",
    "            )\n",
    "\n",
    "        forecast = temp_model.predict(future)\n",
    "\n",
    "        # Get actual values for the prediction period\n",
    "        actual_values = actuals_df[actuals_df[\"ds\"] > cutoff_date]\n",
    "        forecast_values = forecast[forecast[\"ds\"] > cutoff_date]\n",
    "\n",
    "        # Align dates\n",
    "        merged = actual_values.merge(\n",
    "            forecast_values[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]], on=\"ds\"\n",
    "        )\n",
    "\n",
    "        if len(merged) > 0:\n",
    "            # Calculate accuracy metrics\n",
    "            mae = np.mean(np.abs(merged[\"y\"] - merged[\"yhat\"]))\n",
    "            mape = np.mean(np.abs((merged[\"y\"] - merged[\"yhat\"]) / merged[\"y\"])) * 100\n",
    "            rmse = np.sqrt(np.mean((merged[\"y\"] - merged[\"yhat\"]) ** 2))\n",
    "\n",
    "            # Coverage (percentage of actuals within prediction intervals)\n",
    "            coverage = (\n",
    "                np.mean(\n",
    "                    (merged[\"y\"] >= merged[\"yhat_lower\"])\n",
    "                    & (merged[\"y\"] <= merged[\"yhat_upper\"])\n",
    "                )\n",
    "                * 100\n",
    "            )\n",
    "\n",
    "            # Log metrics\n",
    "            accuracy_metrics = {\n",
    "                \"monitoring_mae\": mae,\n",
    "                \"monitoring_mape\": mape,\n",
    "                \"monitoring_rmse\": rmse,\n",
    "                \"prediction_coverage\": coverage,\n",
    "            }\n",
    "            mlflow.log_metrics(accuracy_metrics)\n",
    "\n",
    "            # Create accuracy visualization\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(merged[\"ds\"], merged[\"y\"], label=\"Actual\", marker=\"o\")\n",
    "            plt.plot(merged[\"ds\"], merged[\"yhat\"], label=\"Predicted\", marker=\"s\")\n",
    "            plt.fill_between(\n",
    "                merged[\"ds\"],\n",
    "                merged[\"yhat_lower\"],\n",
    "                merged[\"yhat_upper\"],\n",
    "                alpha=0.3,\n",
    "                label=\"Prediction Interval\",\n",
    "            )\n",
    "            plt.title(f\"Forecast Accuracy Monitoring (MAPE: {mape:.2f}%)\")\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            plt.legend()\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"accuracy_monitoring.png\", dpi=300, bbox_inches=\"tight\")\n",
    "            mlflow.log_artifact(\"accuracy_monitoring.png\")\n",
    "            plt.close()\n",
    "\n",
    "            return accuracy_metrics\n",
    "        else:\n",
    "            print(\"No overlapping dates found for accuracy assessment\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Usage\n",
    "accuracy_metrics = monitor_forecast_accuracy(model_uri, actuals_df, prediction_horizon_days=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ebe404-ca95-43f8-9b3b-5a687a497ece",
   "metadata": {},
   "source": [
    "# Automated Model Retraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794f264a-b32c-45dc-a655-559532c38f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_prophet_retraining(\n",
    "    model_uri, new_data, performance_threshold_mape=10.0, min_data_points=100\n",
    "):\n",
    "    \"\"\"Automated Prophet model retraining pipeline.\"\"\"\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Automated Prophet Retraining\"):\n",
    "        # Load current production model\n",
    "        current_model_uri = model_uri\n",
    "\n",
    "        try:\n",
    "            current_model = mlflow.prophet.load_model(current_model_uri)\n",
    "            mlflow.log_param(\"current_model_loaded\", True)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load current model: {e}\")\n",
    "            current_model = None\n",
    "            mlflow.log_param(\"current_model_loaded\", False)\n",
    "\n",
    "        # Data quality checks\n",
    "        data_quality_passed = True\n",
    "        quality_issues = []\n",
    "\n",
    "        # Check data quantity\n",
    "        if len(new_data) < min_data_points:\n",
    "            data_quality_passed = False\n",
    "            quality_issues.append(\n",
    "                f\"Insufficient data: {len(new_data)} < {min_data_points}\"\n",
    "            )\n",
    "\n",
    "        # Check for missing values\n",
    "        missing_values = new_data[[\"ds\", \"y\"]].isnull().sum().sum()\n",
    "        if missing_values > 0:\n",
    "            quality_issues.append(f\"Missing values found: {missing_values}\")\n",
    "\n",
    "        # Check date continuity\n",
    "        new_data = new_data.sort_values(\"ds\")\n",
    "        date_gaps = pd.to_datetime(new_data[\"ds\"]).diff().dt.days\n",
    "        large_gaps = (date_gaps > 7).sum()  # Gaps larger than 7 days\n",
    "        if large_gaps > 0:\n",
    "            quality_issues.append(f\"Large date gaps found: {large_gaps}\")\n",
    "\n",
    "        mlflow.log_params(\n",
    "            {\n",
    "                \"data_quality_passed\": data_quality_passed,\n",
    "                \"data_points\": len(new_data),\n",
    "                \"quality_issues\": \"; \".join(quality_issues),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if not data_quality_passed:\n",
    "            print(\"Data quality checks failed. Skipping retraining.\")\n",
    "            return None\n",
    "\n",
    "        # Train new model\n",
    "        new_model = Prophet(\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False,\n",
    "            changepoint_prior_scale=0.05,\n",
    "        )\n",
    "\n",
    "        new_model.fit(new_data)\n",
    "\n",
    "        # Evaluate new model performance\n",
    "        cv_results = cross_validation(\n",
    "            new_model,\n",
    "            initial=\"365 days\",\n",
    "            period=\"90 days\",\n",
    "            horizon=\"30 days\",\n",
    "            parallel=\"threads\",\n",
    "        )\n",
    "\n",
    "        metrics = performance_metrics(cv_results)\n",
    "        new_mape = metrics[\"mape\"].mean()\n",
    "\n",
    "        mlflow.log_metric(\"new_model_mape\", new_mape)\n",
    "\n",
    "        # Compare with current model if available\n",
    "        should_deploy = True\n",
    "        if current_model is not None:\n",
    "            try:\n",
    "                # Test current model on new data\n",
    "                current_cv = cross_validation(\n",
    "                    current_model,\n",
    "                    initial=\"365 days\",\n",
    "                    period=\"90 days\",\n",
    "                    horizon=\"30 days\",\n",
    "                )\n",
    "                current_metrics = performance_metrics(current_cv)\n",
    "                current_mape = current_metrics[\"mape\"].mean()\n",
    "\n",
    "                mlflow.log_metric(\"current_model_mape\", current_mape)\n",
    "\n",
    "                # Deploy if new model is significantly better\n",
    "                improvement = (current_mape - new_mape) / current_mape * 100\n",
    "                mlflow.log_metric(\"performance_improvement_percent\", improvement)\n",
    "\n",
    "                should_deploy = improvement > 5.0  # Deploy if >5% improvement\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Could not evaluate current model: {e}\")\n",
    "                should_deploy = new_mape < performance_threshold_mape\n",
    "        else:\n",
    "            should_deploy = new_mape < performance_threshold_mape\n",
    "\n",
    "        mlflow.log_params(\n",
    "            {\n",
    "                \"should_deploy\": should_deploy,\n",
    "                \"performance_threshold\": performance_threshold_mape,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Log and potentially deploy new model\n",
    "        model_info = mlflow.prophet.log_model(\n",
    "            pr_model=new_model,\n",
    "            name=\"retrained_model\",\n",
    "            registered_model_name=current_model_name if should_deploy else None,\n",
    "        )\n",
    "\n",
    "        if should_deploy:\n",
    "            # Transition to production\n",
    "            client = mlflow.MlflowClient()\n",
    "            latest_version = client.get_latest_versions(\n",
    "                current_model_name, stages=[\"None\"]\n",
    "            )[0]\n",
    "\n",
    "            client.transition_model_version_stage(\n",
    "                name=current_model_name,\n",
    "                version=latest_version.version,\n",
    "                stage=\"Production\",\n",
    "            )\n",
    "\n",
    "            print(f\"New model deployed to production with MAPE: {new_mape:.2f}%\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"New model not deployed. MAPE: {new_mape:.2f}% did not meet criteria.\"\n",
    "            )\n",
    "\n",
    "        return new_model, should_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81ed4cb2-dc20-4230-930c-0d1a59469d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"1655d93c38a4421cb6aa50a49c28e127\"\n",
    "model_uri = f\"runs:/{run_id}/best_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b70a608-44e1-4913-a815-476e864474b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prophet_data(data, date_col, value_col, freq=\"D\"):\n",
    "    \"\"\"\n",
    "    Prepare data for Prophet training.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame with time series data\n",
    "        date_col: Name of date column\n",
    "        value_col: Name of value column\n",
    "        freq: Frequency of the time series\n",
    "    \"\"\"\n",
    "\n",
    "    # Prophet requires columns named 'ds' (datestamp) and 'y' (value)\n",
    "    prophet_df = data[[date_col, value_col]].copy()\n",
    "    prophet_df.columns = [\"ds\", \"y\"]\n",
    "\n",
    "    # Ensure ds is datetime\n",
    "    prophet_df[\"ds\"] = pd.to_datetime(prophet_df[\"ds\"])\n",
    "\n",
    "    # Sort by date\n",
    "    prophet_df = prophet_df.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "    # Handle missing dates if needed\n",
    "    if freq:\n",
    "        full_date_range = pd.date_range(\n",
    "            start=prophet_df[\"ds\"].min(), end=prophet_df[\"ds\"].max(), freq=freq\n",
    "        )\n",
    "\n",
    "        # Reindex to fill missing dates\n",
    "        prophet_df = prophet_df.set_index(\"ds\").reindex(full_date_range).reset_index()\n",
    "        prophet_df.columns = [\"ds\", \"y\"]\n",
    "\n",
    "        # Log data quality metrics\n",
    "        missing_dates = prophet_df[\"y\"].isna().sum()\n",
    "        print(f\"Missing dates filled: {missing_dates}\")\n",
    "\n",
    "    return prophet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87e36442-bd6d-41d6-8303-d3a940913194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing dates filled: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>1157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-03</td>\n",
       "      <td>1149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>1071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ds     y\n",
       "0 2021-01-01   941\n",
       "1 2021-01-02  1157\n",
       "2 2021-01-03  1149\n",
       "3 2021-01-04   944\n",
       "4 2021-01-05  1071"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data =  pd.read_csv(\"sales_data_2023_2025_v3.csv\")\n",
    "new_data = prepare_prophet_data(raw_data, 'date', 'sales', freq='D')\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cb23b51-4522-4d18-8bae-4faf0ab99274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5eb0e3e66b64c2b94c0b028e4a41503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ced9dd0fc184aa08dbdd1b53bb8aa9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:05:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "Seasonality has period of 365.25 days which is larger than initial window. Consider increasing initial.\n",
      "16:05:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "Seasonality has period of 365.25 days which is larger than initial window. Consider increasing initial.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9424f94fdac04785a4551fcb27a3baf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:05:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "2025/08/22 16:06:02 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "2025/08/22 16:06:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model not deployed. MAPE: 0.04% did not meet criteria.\n",
      "ðŸƒ View run Automated Prophet Retraining at: https://adb-3191716161457605.5.azuredatabricks.net/ml/experiments/3039328626124250/runs/a8fa80d3063945ccb6550b1d31fe9473\n",
      "ðŸ§ª View experiment at: https://adb-3191716161457605.5.azuredatabricks.net/ml/experiments/3039328626124250\n"
     ]
    }
   ],
   "source": [
    "new_model, should_deploy = automated_prophet_retraining( model_uri, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c9dcb1b-cc35-48ef-b587-706b3318f15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "should_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d13ea-33b8-4c57-bb60-9f1dd568fa8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
